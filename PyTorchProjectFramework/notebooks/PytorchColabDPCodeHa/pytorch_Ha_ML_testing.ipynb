{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "M5I78oVUX7wt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torchvision.transforms as transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOjoLEdqYFI2",
        "outputId": "c63cb2f6-6256-4a4c-8175-907b0dcedff4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(2.)\n",
            "tensor(1.)\n",
            "tensor(1.)\n",
            "tensor(4.)\n",
            "tensor(2.)\n",
            "tensor(2.)\n",
            "tensor(6.)\n",
            "tensor(3.)\n",
            "tensor(3.)\n"
          ]
        }
      ],
      "source": [
        "x = torch.tensor(1., requires_grad=True)\n",
        "w = torch.tensor(2., requires_grad=True)\n",
        "b = torch.tensor(3., requires_grad=True)\n",
        "\n",
        "# Build a computational graph.\n",
        "y = w * x + b    # y = 2 * x + 3\n",
        "\n",
        "# Compute gradients.\n",
        "y.backward(retain_graph=True)\n",
        "\n",
        "# Print out the gradients.\n",
        "print(x.grad)    # x.grad = 2\n",
        "print(w.grad)    # w.grad = 1\n",
        "print(b.grad)    # b.grad = 1\n",
        "\n",
        "# Compute gradients.\n",
        "\n",
        "y.backward(retain_graph=True)\n",
        "\n",
        "# Print out the gradients.\n",
        "print(x.grad)    # x.grad = 2\n",
        "print(w.grad)    # w.grad = 1\n",
        "print(b.grad)    # b.grad = 1\n",
        "\n",
        "y.backward(retain_graph=True)\n",
        "\n",
        "# Print out the gradients.\n",
        "print(x.grad)    # x.grad = 2\n",
        "print(w.grad)    # w.grad = 1\n",
        "print(b.grad)    # b.grad = 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpCd-mkj0ngZ",
        "outputId": "dfd47b2b-4a51-4850-842f-b9e840f6963f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "w:  Parameter containing:\n",
            "tensor([[-0.3083, -0.3908, -0.0916],\n",
            "        [-0.2473,  0.4523,  0.2336]], requires_grad=True)\n",
            "b:  Parameter containing:\n",
            "tensor([ 0.0698, -0.4536], requires_grad=True)\n",
            "loss:  11.13102912902832\n"
          ]
        }
      ],
      "source": [
        "x = torch.randn(10, 3)\n",
        "y = torch.randn(10, 2)\n",
        "\n",
        "# Build a fully connected layer.\n",
        "linear = nn.Linear(3, 2)\n",
        "print ('w: ', linear.weight)\n",
        "print ('b: ', linear.bias)\n",
        "\n",
        "# Build loss function and optimizer.\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(linear.parameters(), lr=0.01)\n",
        "\n",
        "# Forward pass.\n",
        "pred = linear(x)\n",
        "\n",
        "# Compute loss.\n",
        "loss = criterion(pred, y) +  3*criterion(2*pred, y)\n",
        "print('loss: ', loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrgFCZTb0gkd",
        "outputId": "7c6f859e-7716-47f0-dbc1-e9e5ec90848a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dL/dw:  tensor([[-10.2175,  -2.5126,  -7.2176],\n",
            "        [ -3.4455,   3.9356,   6.1196]])\n",
            "dL/db:  tensor([-1.8300, -8.6937])\n",
            "dL/dw:  tensor([[-10.2175,  -2.5126,  -7.2176],\n",
            "        [ -3.4455,   3.9356,   6.1196]])\n",
            "dL/db:  tensor([-1.8300, -8.6937])\n",
            "loss after 1 step optimization:  1.3294565677642822\n"
          ]
        }
      ],
      "source": [
        "# Backward pass.\n",
        "loss.backward(retain_graph=True)\n",
        "\n",
        "# Print out the gradients.\n",
        "print ('dL/dw: ', linear.weight.grad)\n",
        "print ('dL/db: ', linear.bias.grad)\n",
        "\n",
        "optimizer.zero_grad()\n",
        "\n",
        "loss.backward(retain_graph=True)\n",
        "\n",
        "# Print out the gradients.\n",
        "print ('dL/dw: ', linear.weight.grad)\n",
        "print ('dL/db: ', linear.bias.grad)\n",
        "\n",
        "# 1-step gradient descent.\n",
        "optimizer.step()\n",
        "\n",
        "# You can also perform gradient descent at the low level.\n",
        "# linear.weight.data.sub_(0.01 * linear.weight.grad.data)\n",
        "# linear.bias.data.sub_(0.01 * linear.bias.grad.data)\n",
        "\n",
        "# Print out the loss after 1-step gradient descent.\n",
        "pred = linear(x)\n",
        "loss = criterion(pred, y)\n",
        "print('loss after 1 step optimization: ', loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "-hOttvGcoIlF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "# Hyper-parameters\n",
        "input_size = 28 * 28    # 784\n",
        "num_classes = 10\n",
        "num_epochs = 5\n",
        "batch_size = 2\n",
        "learning_rate = 0.001\n",
        "\n",
        "# MNIST dataset (images and labels)\n",
        "train_dataset = torchvision.datasets.MNIST(root='../../data',\n",
        "                                           train=True,\n",
        "                                           transform=transforms.ToTensor(),\n",
        "                                           download=True)\n",
        "\n",
        "test_dataset = torchvision.datasets.MNIST(root='../../data',\n",
        "                                          train=False,\n",
        "                                          transform=transforms.ToTensor())\n",
        "\n",
        "# Data loader (input pipeline)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False)\n",
        "\n",
        "# Logistic regression model\n",
        "model = nn.Linear(input_size, num_classes)\n",
        "\n",
        "# Loss and optimizer\n",
        "# nn.CrossEntropyLoss() computes softmax internally\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdvCLxZFohP0",
        "outputId": "fb7f9e2c-165d-4215-ed4e-b2613808f05c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          ...,\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
            "\n",
            "\n",
            "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          ...,\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.]]]])\n",
            "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
            "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          ...,\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
            "\n",
            "\n",
            "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          ...,\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.]]]])\n",
            "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]])\n"
          ]
        }
      ],
      "source": [
        "for i, (images, labels) in enumerate(train_loader):\n",
        "        # Move tensors to the configured device\n",
        "        if(i <= 1):\n",
        "          print(images)\n",
        "          images = images.reshape(-1, 28*28)\n",
        "          labels = labels\n",
        "          print(images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RTDQqA1Mw2fa",
        "outputId": "00b6d0d1-0509-4da5-bba9-de7cc1fcd9cb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1, 2, 3, 4, 5, 6, 7, 8])"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a = np.array([[1, 2, 3, 4], [5, 6, 7, 8]])\n",
        "a.reshape(-1,2)\n",
        "a.reshape(-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ZASr0XB_3smc"
      },
      "outputs": [],
      "source": [
        "#https://medium.com/@mustafa.kamalzhd/writing-a-custom-loss-in-pytorch-part-1-7dd857934f48\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class CustomMSELoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CustomMSELoss, self).__init__()\n",
        "\n",
        "    def forward(self, predictions, targets):\n",
        "        return torch.mean((predictions - targets) ** 2)\n",
        "\n",
        "loss_fn = CustomMSELoss()\n",
        "# loss = loss_fn(predictions, targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ZIhGmGjuQxiv"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'output' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[17], line 8\u001b[0m\n\u001b[0;32m      4\u001b[0m   loss \u001b[38;5;241m=\u001b[39m T\u001b[38;5;241m.\u001b[39mmean(T\u001b[38;5;241m.\u001b[39mabs(output \u001b[38;5;241m-\u001b[39m target))\n\u001b[0;32m      5\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m loss\n\u001b[1;32m----> 8\u001b[0m loss_val \u001b[38;5;241m=\u001b[39m my_L1Loss(\u001b[43moutput\u001b[49m, y)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mweighted_L1Loss\u001b[39m(output, target):\n\u001b[0;32m     11\u001b[0m   \u001b[38;5;66;03m# weight poverty twice as much as price\u001b[39;00m\n\u001b[0;32m     12\u001b[0m   wts \u001b[38;5;241m=\u001b[39m T\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m1\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mT\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;66;03m# by cols\u001b[39;00m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'output' is not defined"
          ]
        }
      ],
      "source": [
        "#https://jamesmccaffrey.wordpress.com/2022/11/28/custom-loss-functions-for-pytorch/\n",
        "\n",
        "def my_L1Loss(output, target):\n",
        "  loss = torch.mean(torch.abs(output - target))\n",
        "  return loss\n",
        "\n",
        "output = torch.tensor([2,1], dtype=torch.float32)\n",
        "loss_val = my_L1Loss(output, y)\n",
        "\n",
        "def weighted_L1Loss(output, target, device):\n",
        "  # weight poverty twice as much as price\n",
        "  wts = torch.tensor([2,1], dtype=torch.float32).to(device) # by cols\n",
        "  weighted_outputs = torch.mul(output, wts)\n",
        "  weighted_targets = torch.mul(target, wts)\n",
        "  loss = torch.mean(torch.abs(weighted_outputs - weighted_targets))\n",
        "  return loss\n",
        "\n",
        "\n",
        "loss_val = weighted_L1Loss(output, y, \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Oiui2DTwLf9p"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "###########\n",
            "tensor([[78., 14., 87., 43., 48.],\n",
            "        [37.,  8., 28., 52., 83.],\n",
            "        [26., 48., 35., 40., 69.],\n",
            "        [87., 97., 80.,  7., 20.],\n",
            "        [16.,  7., 74., 42.,  9.],\n",
            "        [97.,  7., 73., 56., 82.],\n",
            "        [ 7., 91., 85., 74., 27.],\n",
            "        [59., 79., 58.,  9., 47.],\n",
            "        [12., 96., 75., 33., 73.],\n",
            "        [68., 81., 48., 42., 66.],\n",
            "        [75., 39., 94., 60., 36.],\n",
            "        [12., 36., 86., 96., 51.]])\n",
            "tensor([[1.],\n",
            "        [2.],\n",
            "        [2.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [2.],\n",
            "        [1.],\n",
            "        [2.],\n",
            "        [2.],\n",
            "        [2.],\n",
            "        [1.],\n",
            "        [1.]])\n",
            "###########\n",
            "tensor([[ 72.,   4.,  22.,  32.,  23.],\n",
            "        [ 72.,  21.,  89.,  25.,  31.],\n",
            "        [ 64.,  29.,  78.,  91.,  53.],\n",
            "        [ 13.,  18.,  37.,  70.,   1.],\n",
            "        [ 96.,  74.,  33.,  38.,  66.],\n",
            "        [ 90.,  25.,  37.,  17.,  50.],\n",
            "        [ 67.,  52.,  87.,  23.,  52.],\n",
            "        [ 88.,  86.,  24.,  41.,  55.],\n",
            "        [ 78.,  44.,  30.,  10.,  32.],\n",
            "        [ 69.,   9.,  19.,  34., 100.],\n",
            "        [  8.,  30.,  12.,  18.,  21.],\n",
            "        [  7.,  87.,  52.,  80.,  99.]])\n",
            "tensor([[2.],\n",
            "        [2.],\n",
            "        [2.],\n",
            "        [2.],\n",
            "        [2.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [2.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [2.]])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\nntoa\\AppData\\Local\\Temp\\ipykernel_2572\\1373304879.py:20: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:277.)\n",
            "  tensor_x = torch.Tensor(my_x) # transform to torch tensor\n"
          ]
        }
      ],
      "source": [
        "##### CREATE A DATASET AND DATALOADER FROM arrays\n",
        "#https://stackoverflow.com/questions/44429199/how-to-load-a-list-of-numpy-arrays-to-pytorch-dataset-loader\n",
        "\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "\n",
        "my_x = []\n",
        "my_y = []\n",
        "\n",
        "batch_size = 12\n",
        "\n",
        "for i in range(24):\n",
        "  my_x.append(np.random.randint(1,101,5))\n",
        "  my_y.append(np.random.randint(1,3,1))\n",
        "\n",
        "\n",
        "tensor_x = torch.Tensor(my_x) # transform to torch tensor\n",
        "tensor_y = torch.Tensor(my_y)\n",
        "\n",
        "my_dataset = TensorDataset(tensor_x,tensor_y) # create your datset\n",
        "my_dataloader = DataLoader(my_dataset,batch_size) # create your dataloader\n",
        "\n",
        "for input,label in my_dataloader:\n",
        "  print(\"###########\")\n",
        "  print(input)\n",
        "  print(label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "vyWyeBqDO8D-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "###########\n",
            "0\n",
            "tensor([[78., 14., 87., 43., 48.],\n",
            "        [37.,  8., 28., 52., 83.],\n",
            "        [26., 48., 35., 40., 69.],\n",
            "        [87., 97., 80.,  7., 20.],\n",
            "        [16.,  7., 74., 42.,  9.],\n",
            "        [97.,  7., 73., 56., 82.],\n",
            "        [ 7., 91., 85., 74., 27.],\n",
            "        [59., 79., 58.,  9., 47.],\n",
            "        [12., 96., 75., 33., 73.],\n",
            "        [68., 81., 48., 42., 66.],\n",
            "        [75., 39., 94., 60., 36.],\n",
            "        [12., 36., 86., 96., 51.]])\n",
            "tensor([[1.],\n",
            "        [2.],\n",
            "        [2.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [2.],\n",
            "        [1.],\n",
            "        [2.],\n",
            "        [2.],\n",
            "        [2.],\n",
            "        [1.],\n",
            "        [1.]])\n",
            "*******\n",
            "0\n",
            "tensor([[78., 14., 87., 43., 48.],\n",
            "        [37.,  8., 28., 52., 83.],\n",
            "        [26., 48., 35., 40., 69.]])\n",
            "tensor([[1.],\n",
            "        [2.],\n",
            "        [2.]])\n",
            "*******\n",
            "1\n",
            "tensor([[87., 97., 80.,  7., 20.],\n",
            "        [16.,  7., 74., 42.,  9.],\n",
            "        [97.,  7., 73., 56., 82.]])\n",
            "tensor([[1.],\n",
            "        [1.],\n",
            "        [2.]])\n",
            "*******\n",
            "2\n",
            "tensor([[ 7., 91., 85., 74., 27.],\n",
            "        [59., 79., 58.,  9., 47.],\n",
            "        [12., 96., 75., 33., 73.]])\n",
            "tensor([[1.],\n",
            "        [2.],\n",
            "        [2.]])\n",
            "*******\n",
            "3\n",
            "tensor([[68., 81., 48., 42., 66.],\n",
            "        [75., 39., 94., 60., 36.],\n",
            "        [12., 36., 86., 96., 51.]])\n",
            "tensor([[2.],\n",
            "        [1.],\n",
            "        [1.]])\n",
            "###########\n",
            "1\n",
            "tensor([[ 72.,   4.,  22.,  32.,  23.],\n",
            "        [ 72.,  21.,  89.,  25.,  31.],\n",
            "        [ 64.,  29.,  78.,  91.,  53.],\n",
            "        [ 13.,  18.,  37.,  70.,   1.],\n",
            "        [ 96.,  74.,  33.,  38.,  66.],\n",
            "        [ 90.,  25.,  37.,  17.,  50.],\n",
            "        [ 67.,  52.,  87.,  23.,  52.],\n",
            "        [ 88.,  86.,  24.,  41.,  55.],\n",
            "        [ 78.,  44.,  30.,  10.,  32.],\n",
            "        [ 69.,   9.,  19.,  34., 100.],\n",
            "        [  8.,  30.,  12.,  18.,  21.],\n",
            "        [  7.,  87.,  52.,  80.,  99.]])\n",
            "tensor([[2.],\n",
            "        [2.],\n",
            "        [2.],\n",
            "        [2.],\n",
            "        [2.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [2.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [2.]])\n",
            "*******\n",
            "0\n",
            "tensor([[72.,  4., 22., 32., 23.],\n",
            "        [72., 21., 89., 25., 31.],\n",
            "        [64., 29., 78., 91., 53.]])\n",
            "tensor([[2.],\n",
            "        [2.],\n",
            "        [2.]])\n",
            "*******\n",
            "1\n",
            "tensor([[13., 18., 37., 70.,  1.],\n",
            "        [96., 74., 33., 38., 66.],\n",
            "        [90., 25., 37., 17., 50.]])\n",
            "tensor([[2.],\n",
            "        [2.],\n",
            "        [1.]])\n",
            "*******\n",
            "2\n",
            "tensor([[67., 52., 87., 23., 52.],\n",
            "        [88., 86., 24., 41., 55.],\n",
            "        [78., 44., 30., 10., 32.]])\n",
            "tensor([[1.],\n",
            "        [1.],\n",
            "        [2.]])\n",
            "*******\n",
            "3\n",
            "tensor([[ 69.,   9.,  19.,  34., 100.],\n",
            "        [  8.,  30.,  12.,  18.,  21.],\n",
            "        [  7.,  87.,  52.,  80.,  99.]])\n",
            "tensor([[1.],\n",
            "        [1.],\n",
            "        [2.]])\n"
          ]
        }
      ],
      "source": [
        "### Create sub-dataloader from a given dataloader\n",
        "## We may need it for our DP research\n",
        "\n",
        "for i,(input,label) in enumerate(my_dataloader):\n",
        "  print(\"###########\")\n",
        "  print(i)\n",
        "  print(input)\n",
        "  print(label)\n",
        "\n",
        "  mini_batch_size = 3\n",
        "  mini_dataset = TensorDataset(input,label)\n",
        "  mini_dataloader = DataLoader(mini_dataset,mini_batch_size)\n",
        "\n",
        "  for j,(inputx,labelx) in enumerate(mini_dataloader):\n",
        "    print(\"*******\")\n",
        "    print(j)\n",
        "    print(inputx)\n",
        "    print(labelx)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEE48MBEUuzB",
        "outputId": "ae9f19f6-4acf-4aa1-a12b-b449867c3978"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "param: {'params': [Parameter containing:\n",
            "tensor([[ 0.4352,  0.5631, -0.2654],\n",
            "        [-0.0384, -0.0940, -0.4564]], requires_grad=True), Parameter containing:\n",
            "tensor([-0.2266, -0.5307], requires_grad=True), Parameter containing:\n",
            "tensor([[ 0.6050, -0.3218]], requires_grad=True), Parameter containing:\n",
            "tensor([0.5907], requires_grad=True)], 'lr': 10, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False, 'foreach': None, 'differentiable': False, 'fused': None}\n",
            "lr: {'params': [Parameter containing:\n",
            "tensor([[ 0.4352,  0.5631, -0.2654],\n",
            "        [-0.0384, -0.0940, -0.4564]], requires_grad=True), Parameter containing:\n",
            "tensor([-0.2266, -0.5307], requires_grad=True), Parameter containing:\n",
            "tensor([[ 0.6050, -0.3218]], requires_grad=True), Parameter containing:\n",
            "tensor([0.5907], requires_grad=True)], 'lr': 10, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False, 'foreach': None, 'differentiable': False, 'fused': None}\n"
          ]
        }
      ],
      "source": [
        "#https://stackoverflow.com/questions/73629330/what-exactly-is-meant-by-param-groups-in-pytorch\n",
        "\n",
        "#we show changning in our defined param_groups also leads to the change in model.param_groups\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "class LinearModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layer1 = nn.Linear(3, 2)\n",
        "        self.layer2 = nn.Linear(2, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layer2(self.layer1(x))\n",
        "\n",
        "\n",
        "model = LinearModel()\n",
        "# optimizer requires default learning rate even if its overridden by all param groups\n",
        "optimizer = optim.SGD(model.parameters(), lr=10)\n",
        "\n",
        "for param in optimizer.param_groups:\n",
        "  print(f\"param: {param}\")\n",
        "\n",
        "print(f\"lr: {optimizer.param_groups[0]}\")\n",
        "\n",
        "\n",
        "\n",
        "# for i in range(2):\n",
        "#     output = model(torch.zeros(1, 3))\n",
        "#     loss = output.sum()\n",
        "#     optimizer.zero_grad()\n",
        "#     loss.backward()\n",
        "\n",
        "#     # we can change the learning rate whenever we want for each param group\n",
        "#     print(f'step {i} learning rates')\n",
        "#     for name, param_group in zip(param_group_names, optimizer.param_groups):\n",
        "#         param_group['lr'] = learning_rates[name] / (i + 1)\n",
        "#         print(f'    {name}: {param_group[\"lr\"]}')\n",
        "#         print(f'    {name}: {param_group[\"params\"]}')\n",
        "\n",
        "#     print(\"xxxxxxxx\")\n",
        "#     for name, parameter in model.named_parameters():\n",
        "#         print(name)\n",
        "#         print(parameter)\n",
        "\n",
        "#     print(\"yyyyyyy\")\n",
        "\n",
        "#     optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BY2MjxY7FwZR",
        "outputId": "21146cd4-5f7b-497d-f2d5-9f27bb0c8d00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 0 learning rates\n",
            "    layer1.weight: 0.01\n",
            "    layer1.weight: [Parameter containing:\n",
            "tensor([[ 0.5279,  0.0503, -0.2136],\n",
            "        [ 0.3738,  0.4496, -0.4380]], requires_grad=True)]\n",
            "    layer1.bias: 0.1\n",
            "    layer1.bias: [Parameter containing:\n",
            "tensor([-0.0525, -0.0419], requires_grad=True)]\n",
            "    layer2.weight: 0.001\n",
            "    layer2.weight: [Parameter containing:\n",
            "tensor([[-0.2364,  0.2687]], requires_grad=True)]\n",
            "    layer2.bias: 1.0\n",
            "    layer2.bias: [Parameter containing:\n",
            "tensor([0.3600], requires_grad=True)]\n",
            "xxxxxxxx\n",
            "layer1.weight\n",
            "Parameter containing:\n",
            "tensor([[ 0.5279,  0.0503, -0.2136],\n",
            "        [ 0.3738,  0.4496, -0.4380]], requires_grad=True)\n",
            "layer1.bias\n",
            "Parameter containing:\n",
            "tensor([-0.0525, -0.0419], requires_grad=True)\n",
            "layer2.weight\n",
            "Parameter containing:\n",
            "tensor([[-0.2364,  0.2687]], requires_grad=True)\n",
            "layer2.bias\n",
            "Parameter containing:\n",
            "tensor([0.3600], requires_grad=True)\n",
            "yyyyyyy\n",
            "step 1 learning rates\n",
            "    layer1.weight: 0.005\n",
            "    layer1.weight: [Parameter containing:\n",
            "tensor([[ 0.5279,  0.0503, -0.2136],\n",
            "        [ 0.3738,  0.4496, -0.4380]], requires_grad=True)]\n",
            "    layer1.bias: 0.05\n",
            "    layer1.bias: [Parameter containing:\n",
            "tensor([-0.0289, -0.0688], requires_grad=True)]\n",
            "    layer2.weight: 0.0005\n",
            "    layer2.weight: [Parameter containing:\n",
            "tensor([[-0.2363,  0.2687]], requires_grad=True)]\n",
            "    layer2.bias: 0.5\n",
            "    layer2.bias: [Parameter containing:\n",
            "tensor([-0.6400], requires_grad=True)]\n",
            "xxxxxxxx\n",
            "layer1.weight\n",
            "Parameter containing:\n",
            "tensor([[ 0.5279,  0.0503, -0.2136],\n",
            "        [ 0.3738,  0.4496, -0.4380]], requires_grad=True)\n",
            "layer1.bias\n",
            "Parameter containing:\n",
            "tensor([-0.0289, -0.0688], requires_grad=True)\n",
            "layer2.weight\n",
            "Parameter containing:\n",
            "tensor([[-0.2363,  0.2687]], requires_grad=True)\n",
            "layer2.bias\n",
            "Parameter containing:\n",
            "tensor([-0.6400], requires_grad=True)\n",
            "yyyyyyy\n"
          ]
        }
      ],
      "source": [
        "#https://stackoverflow.com/questions/73629330/what-exactly-is-meant-by-param-groups-in-pytorch\n",
        "\n",
        "#we show changning in our defined param_groups also leads to the change in model.param_groups\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "class LinearModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layer1 = nn.Linear(3, 2)\n",
        "        self.layer2 = nn.Linear(2, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layer2(self.layer1(x))\n",
        "\n",
        "\n",
        "model = LinearModel()\n",
        "\n",
        "learning_rates = {\n",
        "    'layer1.weight': 0.01,\n",
        "    'layer1.bias': 0.1,\n",
        "    'layer2.weight': 0.001,\n",
        "    'layer2.bias': 1.0}\n",
        "\n",
        "# Build param_group where each group consists of a single parameter.\n",
        "# `param_group_names` is created so we can keep track of which param_group\n",
        "# corresponds to which parameter.\n",
        "param_groups = []\n",
        "param_group_names = []\n",
        "for name, parameter in model.named_parameters():\n",
        "    param_groups.append({'params': [parameter], 'lr': learning_rates[name]})\n",
        "    param_group_names.append(name)\n",
        "\n",
        "# optimizer requires default learning rate even if its overridden by all param groups\n",
        "optimizer = optim.SGD(param_groups, lr=10)\n",
        "\n",
        "for i in range(2):\n",
        "    output = model(torch.zeros(1, 3))\n",
        "    loss = output.sum()\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    # we can change the learning rate whenever we want for each param group\n",
        "    print(f'step {i} learning rates')\n",
        "    for name, param_group in zip(param_group_names, optimizer.param_groups):\n",
        "        param_group['lr'] = learning_rates[name] / (i + 1)\n",
        "        print(f'    {name}: {param_group[\"lr\"]}')\n",
        "        print(f'    {name}: {param_group[\"params\"]}')\n",
        "\n",
        "    print(\"xxxxxxxx\")\n",
        "    for name, parameter in model.named_parameters():\n",
        "        print(name)\n",
        "        print(parameter)\n",
        "\n",
        "    print(\"yyyyyyy\")\n",
        "\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "hZQ-PD5LtfLP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 0 learning rates\n",
            "    layer1.weight: 0.01\n",
            "    layer1.weight: [Parameter containing:\n",
            "tensor([[ 0.0801,  0.4628,  0.1492],\n",
            "        [ 0.4205, -0.1821,  0.4633]], requires_grad=True)]\n",
            "    layer1.bias: 0.1\n",
            "    layer1.bias: [Parameter containing:\n",
            "tensor([ 0.5570, -0.3291], requires_grad=True)]\n",
            "    layer2.weight: 0.001\n",
            "    layer2.weight: [Parameter containing:\n",
            "tensor([[-0.5903, -0.2192]], requires_grad=True)]\n",
            "    layer2.bias: 1.0\n",
            "    layer2.bias: [Parameter containing:\n",
            "tensor([0.0053], requires_grad=True)]\n",
            "xxxxxxxx\n",
            "layer1.weight\n",
            "Parameter containing:\n",
            "tensor([[ 0.0801,  0.4628,  0.1492],\n",
            "        [ 0.4205, -0.1821,  0.4633]], requires_grad=True)\n",
            "layer1.bias\n",
            "Parameter containing:\n",
            "tensor([ 0.5570, -0.3291], requires_grad=True)\n",
            "layer2.weight\n",
            "Parameter containing:\n",
            "tensor([[-0.5903, -0.2192]], requires_grad=True)\n",
            "layer2.bias\n",
            "Parameter containing:\n",
            "tensor([0.0053], requires_grad=True)\n",
            "yyyyyyy\n",
            "manipulate the parameters using parameter.data\n",
            "layer1.weight\n",
            "before:  Parameter containing:\n",
            "tensor([[ 0.0801,  0.4628,  0.1492],\n",
            "        [ 0.4205, -0.1821,  0.4633]], requires_grad=True)\n",
            "after:  Parameter containing:\n",
            "tensor([[ 0.1602,  0.9255,  0.2984],\n",
            "        [ 0.8410, -0.3643,  0.9265]], requires_grad=True)\n",
            "layer1.bias\n",
            "before:  Parameter containing:\n",
            "tensor([ 0.5570, -0.3291], requires_grad=True)\n",
            "after:  Parameter containing:\n",
            "tensor([ 1.1141, -0.6582], requires_grad=True)\n",
            "layer2.weight\n",
            "before:  Parameter containing:\n",
            "tensor([[-0.5903, -0.2192]], requires_grad=True)\n",
            "after:  Parameter containing:\n",
            "tensor([[-1.1806, -0.4384]], requires_grad=True)\n",
            "layer2.bias\n",
            "before:  Parameter containing:\n",
            "tensor([0.0053], requires_grad=True)\n",
            "after:  Parameter containing:\n",
            "tensor([0.0105], requires_grad=True)\n",
            "zzzz\n",
            "step 1 learning rates\n",
            "    layer1.weight: 0.005\n",
            "    layer1.weight: [Parameter containing:\n",
            "tensor([[ 0.1602,  0.9255,  0.2984],\n",
            "        [ 0.8410, -0.3643,  0.9265]], requires_grad=True)]\n",
            "    layer1.bias: 0.05\n",
            "    layer1.bias: [Parameter containing:\n",
            "tensor([ 1.1731, -0.6363], requires_grad=True)]\n",
            "    layer2.weight: 0.0005\n",
            "    layer2.weight: [Parameter containing:\n",
            "tensor([[-1.1811, -0.4380]], requires_grad=True)]\n",
            "    layer2.bias: 0.5\n",
            "    layer2.bias: [Parameter containing:\n",
            "tensor([-0.9895], requires_grad=True)]\n",
            "xxxxxxxx\n",
            "layer1.weight\n",
            "Parameter containing:\n",
            "tensor([[ 0.1602,  0.9255,  0.2984],\n",
            "        [ 0.8410, -0.3643,  0.9265]], requires_grad=True)\n",
            "layer1.bias\n",
            "Parameter containing:\n",
            "tensor([ 1.1731, -0.6363], requires_grad=True)\n",
            "layer2.weight\n",
            "Parameter containing:\n",
            "tensor([[-1.1811, -0.4380]], requires_grad=True)\n",
            "layer2.bias\n",
            "Parameter containing:\n",
            "tensor([-0.9895], requires_grad=True)\n",
            "yyyyyyy\n",
            "manipulate the parameters using parameter.data\n",
            "layer1.weight\n",
            "before:  Parameter containing:\n",
            "tensor([[ 0.1602,  0.9255,  0.2984],\n",
            "        [ 0.8410, -0.3643,  0.9265]], requires_grad=True)\n",
            "after:  Parameter containing:\n",
            "tensor([[ 0.3204,  1.8510,  0.5969],\n",
            "        [ 1.6820, -0.7285,  1.8531]], requires_grad=True)\n",
            "layer1.bias\n",
            "before:  Parameter containing:\n",
            "tensor([ 1.1731, -0.6363], requires_grad=True)\n",
            "after:  Parameter containing:\n",
            "tensor([ 2.3462, -1.2726], requires_grad=True)\n",
            "layer2.weight\n",
            "before:  Parameter containing:\n",
            "tensor([[-1.1811, -0.4380]], requires_grad=True)\n",
            "after:  Parameter containing:\n",
            "tensor([[-2.3622, -0.8760]], requires_grad=True)\n",
            "layer2.bias\n",
            "before:  Parameter containing:\n",
            "tensor([-0.9895], requires_grad=True)\n",
            "after:  Parameter containing:\n",
            "tensor([-1.9789], requires_grad=True)\n",
            "zzzz\n"
          ]
        }
      ],
      "source": [
        "#https://stackoverflow.com/questions/73629330/what-exactly-is-meant-by-param-groups-in-pytorch\n",
        "\n",
        "#we show changning in our defined param_groups also leads to the change in model.param_groups\n",
        "#we show changing in our defined param_groups and model.param_groups can be done by param.data\n",
        "#https://discuss.pytorch.org/t/manually-change-assign-weights-of-a-neural-network/115444\n",
        "    # def set_to_one(self, model):\n",
        "    #     for name, param in model.named_parameters():\n",
        "    #         values = torch.ones(param.shape)\n",
        "    #         param.data = values\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "class LinearModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layer1 = nn.Linear(3, 2)\n",
        "        self.layer2 = nn.Linear(2, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layer2(self.layer1(x))\n",
        "\n",
        "\n",
        "model = LinearModel()\n",
        "\n",
        "learning_rates = {\n",
        "    'layer1.weight': 0.01,\n",
        "    'layer1.bias': 0.1,\n",
        "    'layer2.weight': 0.001,\n",
        "    'layer2.bias': 1.0}\n",
        "\n",
        "# Build param_group where each group consists of a single parameter.\n",
        "# `param_group_names` is created so we can keep track of which param_group\n",
        "# corresponds to which parameter.\n",
        "param_groups = []\n",
        "param_group_names = []\n",
        "for name, parameter in model.named_parameters():\n",
        "    param_groups.append({'params': [parameter], 'lr': learning_rates[name]})\n",
        "    param_group_names.append(name)\n",
        "\n",
        "# optimizer requires default learning rate even if its overridden by all param groups\n",
        "optimizer = optim.SGD(param_groups, lr=10)\n",
        "\n",
        "for i in range(2):\n",
        "    output = model(torch.zeros(1, 3))\n",
        "    loss = output.sum()\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    # we can change the learning rate whenever we want for each param group\n",
        "    print(f'step {i} learning rates')\n",
        "    for name, param_group in zip(param_group_names, optimizer.param_groups):\n",
        "        param_group['lr'] = learning_rates[name] / (i + 1)\n",
        "        print(f'    {name}: {param_group[\"lr\"]}')\n",
        "        print(f'    {name}: {param_group[\"params\"]}')\n",
        "\n",
        "    print(\"xxxxxxxx\")\n",
        "    for name, parameter in model.named_parameters():\n",
        "        print(name)\n",
        "        print(parameter)\n",
        "\n",
        "    print(\"yyyyyyy\")\n",
        "    print(\"manipulate the parameters using parameter.data\")\n",
        "    for name, parameter in model.named_parameters():\n",
        "        print(name)\n",
        "        print(\"before: \",parameter)\n",
        "        parameter.data = parameter.data * 2\n",
        "        print(\"after: \",parameter)\n",
        "\n",
        "    print(\"zzzz\")\n",
        "\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UeT2aa0FHhNi",
        "outputId": "0da8bf46-991d-48a3-b6a7-8489e57bdf61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vvvvvvv\n",
            "param.data:  tensor([[-0.4033,  0.4925, -0.0318],\n",
            "        [ 0.0829, -0.1622,  0.4725]])\n",
            "param.grad:  tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]])\n",
            "data + grad:  tensor([[-0.4033,  0.4925, -0.0318],\n",
            "        [ 0.0829, -0.1622,  0.4725]])\n",
            "norm of grad:  tensor(0.)\n",
            "clipped param.grad:  tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]])\n",
            "eeeee\n",
            "vvvvvvv\n",
            "param.data:  tensor([0.5173, 0.5733])\n",
            "param.grad:  tensor([0.0949, 0.4139])\n",
            "data + grad:  tensor([0.6122, 0.9872])\n",
            "norm of grad:  tensor(0.4247)\n",
            "clipped param.grad:  tensor([9.4911e-05, 4.1391e-04])\n",
            "eeeee\n",
            "vvvvvvv\n",
            "param.data:  tensor([[0.0949, 0.4139]])\n",
            "param.grad:  tensor([[0.5173, 0.5733]])\n",
            "data + grad:  tensor([[0.6122, 0.9872]])\n",
            "norm of grad:  tensor(0.7721)\n",
            "clipped param.grad:  tensor([[0.0005, 0.0006]])\n",
            "eeeee\n",
            "vvvvvvv\n",
            "param.data:  tensor([0.5941])\n",
            "param.grad:  tensor([1.])\n",
            "data + grad:  tensor([1.5941])\n",
            "norm of grad:  tensor(1.)\n",
            "clipped param.grad:  tensor([0.0010])\n",
            "eeeee\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\nntoa\\AppData\\Local\\Temp\\ipykernel_2572\\2338325451.py:63: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  torch.nn.utils.clip_grad_norm(param, max_norm=0.001*norml2, norm_type=2.0)\n"
          ]
        }
      ],
      "source": [
        "#https://stackoverflow.com/questions/73629330/what-exactly-is-meant-by-param-groups-in-pytorch\n",
        "\n",
        "#we show changning in our defined param_groups also leads to the change in model.param_groups\n",
        "#we show changing in our defined param_groups and model.param_groups can be done by param.data\n",
        "#https://discuss.pytorch.org/t/manually-change-assign-weights-of-a-neural-network/115444\n",
        "    # def set_to_one(self, model):\n",
        "    #     for name, param in model.named_parameters():\n",
        "    #         values = torch.ones(param.shape)\n",
        "    #         param.data = values\n",
        "\n",
        "#https://www.programcreek.com/python/example/113970/torch.nn.utils.clip_grad_norm_\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "class LinearModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layer1 = nn.Linear(3, 2)\n",
        "        self.layer2 = nn.Linear(2, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layer2(self.layer1(x))\n",
        "\n",
        "\n",
        "model = LinearModel()\n",
        "optimizer = optim.SGD(model.parameters(), lr=10)\n",
        "output = model(torch.zeros(1, 3))\n",
        "loss = output.sum()\n",
        "optimizer.zero_grad()\n",
        "loss.backward()\n",
        "\n",
        "# for name, parameter in model.named_parameters():\n",
        "#     print(name)\n",
        "#     print(\"before: \",parameter)\n",
        "#     parameter.data = parameter.data * 2\n",
        "#     print(\"after: \",parameter)\n",
        "#     print(\"xxxxxx\")\n",
        "#     print(\"grad\")\n",
        "#     print(\"grad before: \", parameter.grad)\n",
        "#     parameter.grad = parameter.grad*2\n",
        "#     print(\"grad after: \", parameter.grad)\n",
        "#     print(\"zzzzzz\")\n",
        "\n",
        "# params = []\n",
        "# for param in model.parameters():\n",
        "#     params.append(param.view(-1)) #merge the weights and bias\n",
        "#     print(param)\n",
        "#     print(\"dddddd\")\n",
        "#     print(param.view(-1))\n",
        "\n",
        "for param in model.parameters():\n",
        "    print(\"vvvvvvv\")\n",
        "    print(\"param.data: \", param.data)\n",
        "    print(\"param.grad: \", param.grad)\n",
        "    param.data = param.data + param.grad\n",
        "    print(\"data + grad: \", param.data)\n",
        "    norml2 = torch.norm(param.grad)\n",
        "    print(\"norm of grad: \", norml2)\n",
        "\n",
        "    torch.nn.utils.clip_grad_norm(param, max_norm=0.001*norml2, norm_type=2.0)\n",
        "    print(\"clipped param.grad: \", param.grad)\n",
        "    print(\"eeeee\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUSNrvZtzNk3",
        "outputId": "56b81beb-f2b7-44f9-f7d6-4841372175d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "norml2:  tensor(3.7417)\n",
            "normTotal:  tensor(14.0000)\n",
            "norml2:  tensor(1.)\n",
            "normTotal:  tensor(15.0000)\n"
          ]
        }
      ],
      "source": [
        "#https://stackoverflow.com/questions/73629330/what-exactly-is-meant-by-param-groups-in-pytorch\n",
        "\n",
        "#we show changning in our defined param_groups also leads to the change in model.param_groups\n",
        "#we show changing in our defined param_groups and model.param_groups can be done by param.data\n",
        "#https://discuss.pytorch.org/t/manually-change-assign-weights-of-a-neural-network/115444\n",
        "    # def set_to_one(self, model):\n",
        "    #     for name, param in model.named_parameters():\n",
        "    #         values = torch.ones(param.shape)\n",
        "    #         param.data = values\n",
        "\n",
        "#https://www.programcreek.com/python/example/113970/torch.nn.utils.clip_grad_norm_\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "class LinearModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layer1 = nn.Linear(3, 1)\n",
        "        #self.layer2 = nn.Linear(2, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layer1(x)\n",
        "        #return self.layer2(self.layer1(x))\n",
        "\n",
        "\n",
        "model = LinearModel()\n",
        "optimizer = optim.SGD(model.parameters(), lr=10)\n",
        "#output = model(torch.zeros(1, 3))\n",
        "#print(torch.zeros(1, 3))\n",
        "output = model(torch.tensor([[1., 2., 3.]]))\n",
        "loss = output.sum()\n",
        "optimizer.zero_grad()\n",
        "loss.backward()\n",
        "\n",
        "\n",
        "# for param in model.parameters():\n",
        "#     print(\"vvvvvvv\")\n",
        "#     print(\"param.data: \", param.data)\n",
        "#     print(\"param.grad: \", param.grad)\n",
        "#     param.data = param.data + param.grad\n",
        "#     print(\"data + grad: \", param.data)\n",
        "#     norml2 = torch.norm(param.grad)\n",
        "#     print(\"norm of grad: \", norml2)\n",
        "\n",
        "#     torch.nn.utils.clip_grad_norm(param, max_norm= 0.1*norml2, norm_type=2.0)\n",
        "#     print(\"clipped param.grad: \", param.grad)\n",
        "#     print(\"clipped grad shape: \", param.grad.size())\n",
        "#     param.grad = param.grad + (0.1**0.5)*torch.randn(param.grad.size())\n",
        "#     print(\"noisy grad: \", param.grad)\n",
        "\n",
        "#     print(\"eeeee\")\n",
        "\n",
        "# params = []\n",
        "# for param in model.parameters():\n",
        "#     params.append(param.view(-1)) #merge the weights and bias\n",
        "#     print(param)\n",
        "#     print(\"dddddd\")\n",
        "#     print(param.view(-1))\n",
        "\n",
        "#compute the norm of all grad vectors:\n",
        "normTotal = 0\n",
        "for param in model.parameters():\n",
        "    norml2 = torch.norm(param.grad)\n",
        "    normTotal = normTotal + norml2**2\n",
        "    print(\"norml2: \", norml2)\n",
        "    print(\"normTotal: \", normTotal)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "QCTk4Ly38RRX"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'ionosphere.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[25], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# load dataset, split into input (X) and output (y) variables\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m dataframe \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mionosphere.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m dataset \u001b[38;5;241m=\u001b[39m dataframe\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m     17\u001b[0m X \u001b[38;5;241m=\u001b[39m dataset[:,\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m34\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mfloat\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\nntoa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\nntoa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
            "File \u001b[1;32mc:\\Users\\nntoa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\nntoa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
            "File \u001b[1;32mc:\\Users\\nntoa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ionosphere.csv'"
          ]
        }
      ],
      "source": [
        "# scheduler https://machinelearningmastery.com/using-learning-rate-schedule-in-pytorch-training/\n",
        "# https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
        "\n",
        "#example 1\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# load dataset, split into input (X) and output (y) variables\n",
        "dataframe = pd.read_csv(\"ionosphere.csv\", header=None)\n",
        "dataset = dataframe.values\n",
        "X = dataset[:,0:34].astype(float)\n",
        "y = dataset[:,34]\n",
        "\n",
        "# encode class values as integers\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(y)\n",
        "y = encoder.transform(y)\n",
        "\n",
        "# convert into PyTorch tensors\n",
        "X = torch.tensor(X, dtype=torch.float32)\n",
        "y = torch.tensor(y, dtype=torch.float32).reshape(-1, 1)\n",
        "\n",
        "# train-test split for evaluation of the model\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, shuffle=True)\n",
        "\n",
        "# create model\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(34, 34),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(34, 1),\n",
        "    nn.Sigmoid()\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "n_epochs = 50\n",
        "batch_size = 24\n",
        "batch_start = torch.arange(0, len(X_train), batch_size)\n",
        "lr = 0.1\n",
        "loss_fn = nn.BCELoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "scheduler = lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.5, total_iters=30)\n",
        "model.train()\n",
        "for epoch in range(n_epochs):\n",
        "    for start in batch_start:\n",
        "        X_batch = X_train[start:start+batch_size]\n",
        "        y_batch = y_train[start:start+batch_size]\n",
        "        y_pred = model(X_batch)\n",
        "        loss = loss_fn(y_pred, y_batch)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    before_lr = optimizer.param_groups[0][\"lr\"]\n",
        "    scheduler.step()\n",
        "    after_lr = optimizer.param_groups[0][\"lr\"]\n",
        "    print(\"Epoch %d: SGD lr %.4f -> %.4f\" % (epoch, before_lr, after_lr))\n",
        "\n",
        "# evaluate accuracy after training\n",
        "model.eval()\n",
        "y_pred = model(X_test)\n",
        "acc = (y_pred.round() == y_test).float().mean()\n",
        "acc = float(acc)\n",
        "print(\"Model accuracy: %.2f%%\" % (acc*100))\n",
        "\n",
        "\n",
        "#example 2\n",
        "\n",
        "def lr_lambda(epoch):\n",
        "    # LR to be 0.1 * (1/1+0.01*epoch)\n",
        "    base_lr = 0.1\n",
        "    factor = 0.01\n",
        "    return base_lr/(1+factor*epoch)\n",
        "\n",
        "scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# load dataset, split into input (X) and output (y) variables\n",
        "dataframe = pd.read_csv(\"ionosphere.csv\", header=None)\n",
        "dataset = dataframe.values\n",
        "X = dataset[:,0:34].astype(float)\n",
        "y = dataset[:,34]\n",
        "\n",
        "# encode class values as integers\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(y)\n",
        "y = encoder.transform(y)\n",
        "\n",
        "# convert into PyTorch tensors\n",
        "X = torch.tensor(X, dtype=torch.float32)\n",
        "y = torch.tensor(y, dtype=torch.float32).reshape(-1, 1)\n",
        "\n",
        "# train-test split for evaluation of the model\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, shuffle=True)\n",
        "\n",
        "# create model\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(34, 34),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(34, 1),\n",
        "    nn.Sigmoid()\n",
        ")\n",
        "\n",
        "def lr_lambda(epoch):\n",
        "    # LR to be 0.1 * (1/1+0.01*epoch)\n",
        "    base_lr = 0.1\n",
        "    factor = 0.01\n",
        "    return base_lr/(1+factor*epoch)\n",
        "\n",
        "# Train the model\n",
        "n_epochs = 50\n",
        "batch_size = 24\n",
        "batch_start = torch.arange(0, len(X_train), batch_size)\n",
        "lr = 0.1\n",
        "loss_fn = nn.BCELoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "model.train()\n",
        "for epoch in range(n_epochs):\n",
        "    for start in batch_start:\n",
        "        X_batch = X_train[start:start+batch_size]\n",
        "        y_batch = y_train[start:start+batch_size]\n",
        "        y_pred = model(X_batch)\n",
        "        loss = loss_fn(y_pred, y_batch)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    before_lr = optimizer.param_groups[0][\"lr\"]\n",
        "    scheduler.step()\n",
        "    after_lr = optimizer.param_groups[0][\"lr\"]\n",
        "    print(\"Epoch %d: SGD lr %.4f -> %.4f\" % (epoch, before_lr, after_lr))\n",
        "\n",
        "# evaluate accuracy after training\n",
        "model.eval()\n",
        "y_pred = model(X_test)\n",
        "acc = (y_pred.round() == y_test).float().mean()\n",
        "acc = float(acc)\n",
        "print(\"Model accuracy: %.2f%%\" % (acc*100))\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RI-8Q3wi8B7v"
      },
      "outputs": [],
      "source": [
        "#https://pytorch.org/tutorials/intermediate/per_sample_grads.html?utm_source=whats_new_tutorials&utm_medium=per_sample_grads\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJfNP5KVP611",
        "outputId": "1059b1c1-6389-4671-b409-b65df5fafb06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 2, 3])\n",
            "tensor([[[0.9755, 2.1539, 3.0024],\n",
            "         [4.2981, 4.7473, 6.6748]],\n",
            "\n",
            "        [[1.2218, 1.9470, 2.9126],\n",
            "         [4.1704, 5.3141, 5.8889]]])\n"
          ]
        }
      ],
      "source": [
        "#https://stackoverflow.com/questions/59090533/how-do-i-add-some-gaussian-noise-to-a-tensor-in-pytorch\n",
        "\n",
        "\n",
        "import torch\n",
        "\n",
        "x = torch.tensor([[[1, 2, 3], [4, 5, 6]],[[1, 2, 3], [4, 5, 6]]])\n",
        "print(x.size())\n",
        "\n",
        "x = x + (0.1**0.5)*torch.randn(x.size())\n",
        "\n",
        "print(x)\n",
        "\n",
        "# x = torch.zeros(5, 10, 20, dtype=torch.float64)\n",
        "# print(x)\n",
        "# x = x + (0.1**0.5)*torch.randn(5, 10, 20)\n",
        "# print(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KG2XwgEnfg6W"
      },
      "outputs": [],
      "source": [
        "#multiple inputs\n",
        "#https://saturncloud.io/blog/how-to-construct-a-network-with-two-inputs-in-pytorch/\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(2, 10)\n",
        "        self.fc2 = nn.Linear(10, 1)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x = torch.cat((x1, x2), dim=1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "net = Net()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
        "\n",
        "for epoch in range(100):\n",
        "    running_loss = 0.0\n",
        "    for i in range(n):\n",
        "        inputs, labels = get_next_batch() # Get the next batch of data\n",
        "        optimizer.zero_grad() # Zero the gradients\n",
        "        outputs = net(inputs[:,0], inputs[:,1]) # Forward pass\n",
        "        loss = criterion(outputs, labels) # Compute the loss\n",
        "        loss.backward() # Backward pass\n",
        "        optimizer.step() # Update the weights\n",
        "        running_loss += loss.item()\n",
        "    print('Epoch %d loss: %.3f' % (epoch + 1, running_loss / n))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9sepeIB9Flj",
        "outputId": "8e1b20dd-fe49-41f2-bf93-43ace8901217"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<module 'torchvision.datasets' from '/usr/local/lib/python3.10/dist-packages/torchvision/datasets/__init__.py'>\n"
          ]
        }
      ],
      "source": [
        "print(torchvision.datasets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWOnukLg-1l6"
      },
      "outputs": [],
      "source": [
        "#https://viblo.asia/p/pytorch-fundamentals-YWOZrAmrKQ0\n",
        "#https://phamdinhkhanh.github.io/2019/08/10/PytorchTurtorial1.html\n",
        "\n",
        "x = torch.randn(10,10)\n",
        "print(x.shape)\n",
        "# torch.size(10,10)\n",
        "z1 = x.unsqueeze(0)\n",
        "print(z1.shape)\n",
        "# torch.size(1,10,10)\n",
        "# The same can be achieved using [None] indexing\n",
        "# Adding None will auto create a fake dim\n",
        "# at the specified axis\n",
        "x = torch.randn(10,10)\n",
        "z2, z3, z4 = x[None], x[:,None], x[:,:,None]\n",
        "print(z2.shape, z3.shape, z4.shape)\n",
        "# torch.Size([1, 10, 10])\n",
        "# torch.Size([10, 1, 10])\n",
        "# torch.Size([10, 10, 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2BtJm1QIvrHp"
      },
      "outputs": [],
      "source": [
        "x = torch.randn(2,10)\n",
        "print(x)\n",
        "x = x.view(-1)\n",
        "print(x)\n",
        "\n",
        "print(x.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhtmOrpHTZG2",
        "outputId": "3a718c3b-58a4-4c72-8939-110311ed48a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (fc1): Linear(in_features=6272, out_features=128, bias=True)\n",
            "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "#https://github.com/nttuan8/Pytorch_tutorial/blob/main/L3/CNN_MNIST.ipynb\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # input 1 channel, output 6 channel, kernel size 3*3\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 32, 3, padding=1)\n",
        "\n",
        "        self.fc1 = nn.Linear(14*14*32, 128)  # 14*14 from image dimension\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        # Max pooling over a (2, 2) window\n",
        "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
        "        # Flatten to vector to input the neural network\n",
        "        x = x.view(-1, self.num_flat_features(x))\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "    def num_flat_features(self, x):\n",
        "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
        "        num_features = 1\n",
        "        for s in size:\n",
        "            num_features *= s\n",
        "        return num_features\n",
        "\n",
        "net = Net()\n",
        "print(net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "neuuv_MPB8ie",
        "outputId": "06de00b3-3da6-4278-80e7-b2ede31903c3"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-6ac4ae27b7da>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#input = input[None,:]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mout_call\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mout_forward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-6d21d51fa32d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# Flatten to vector to input the neural network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_flat_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (32x196 and 6272x128)"
          ]
        }
      ],
      "source": [
        "params = list(net.parameters())\n",
        "\n",
        "#input = torch.randn(2, 1, 28, 28)\n",
        "input = torch.randn(1, 28, 28)\n",
        "#input = input[None,:]\n",
        "out_call = net(input)\n",
        "out_forward = net.forward(input)\n",
        "\n",
        "out_call == out_forward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezl28-jxmL3A",
        "outputId": "408e3d6f-49c9-4a9f-81e5-dcff2cbf1121"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inside Conv2d forward\n",
            "\n",
            "input:  <class 'tuple'> , len:  1\n",
            "input[0]:  <class 'torch.Tensor'> , shape:  torch.Size([2, 32, 28, 28])\n",
            "output:  <class 'torch.Tensor'> , len:  2 torch.Size([2, 32, 28, 28])\n"
          ]
        }
      ],
      "source": [
        "net = Net()\n",
        "def print_info(self, input, output):\n",
        "    # input is a tuple of packed inputs\n",
        "    # output is a Tensor. output.data is the Tensor we are interested\n",
        "    print('Inside ' + self.__class__.__name__ + ' forward')\n",
        "\n",
        "    print('')\n",
        "    print('input: ', type(input), ', len: ', len(input))\n",
        "    print('input[0]: ', type(input[0]), ', shape: ', input[0].shape)\n",
        "    print('output: ', type(output), ', len: ', len(output), output.data.shape)\n",
        "\n",
        "\n",
        "net.conv2.register_forward_hook(print_info)\n",
        "\n",
        "out = net(input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iw_NnKx9werc",
        "outputId": "20c8b936-1775-4d8e-c9a3-d512bcb00a05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inside Conv2d backward\n",
            "grad_input:  <class 'tuple'> , len:  3\n",
            "grad_output:  <class 'tuple'> , len:  1\n",
            "grad_output[0]:  <class 'torch.Tensor'> , size:  torch.Size([2, 32, 28, 28])\n"
          ]
        }
      ],
      "source": [
        "#target = torch.tensor([[2],[3]])\n",
        "target = torch.tensor([2,3])\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "net = Net()\n",
        "def print_backward_info(self, grad_input, grad_output):\n",
        "    print('Inside ' + self.__class__.__name__ + ' backward')\n",
        "    print('grad_input: ', type(grad_input), ', len: ', len(grad_input))\n",
        "    print('grad_output: ', type(grad_output), ', len: ', len(grad_output))\n",
        "    print('grad_output[0]: ', type(grad_output[0]), ', size: ', grad_output[0].shape)\n",
        "    #print('grad_output[0] value: ', grad_output[0])\n",
        "\n",
        "\n",
        "net.conv1.register_backward_hook(print_backward_info)\n",
        "\n",
        "out = net(input)\n",
        "err = loss_fn(out, target)\n",
        "err.backward()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKyYJjKaKicF"
      },
      "outputs": [],
      "source": [
        "#https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial1/Lisa_Cluster.html [LISA project]\n",
        "# https://medium.com/dair-ai/a-simple-neural-network-from-scratch-with-pytorch-and-google-colab-c7f3830618e0\n",
        "\n",
        "\n",
        "#GOOD TUTORIAL ABOUT THE SEREVER FOR MACHINE LEARNING\n",
        "https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial1/Lisa_Cluster.html\n",
        "https://winscp.net/eng/index.php\n",
        "https://filezilla-project.org/\n",
        "\n",
        "https://slurm.schedmd.com/\n",
        "\n",
        "https://kb.ndsu.edu/page.php?id=107849\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MqaTm1kuYoue"
      },
      "outputs": [],
      "source": [
        "#https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html.  [Training with validation setup]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2D_Rr2NYm29j",
        "outputId": "fed08f7c-362b-4fde-ffbf-a1499b3347ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "state1-key: fc1.weight, value: tensor([[ 0.2942,  0.1368,  0.1802,  0.2937, -0.3037,  0.2959,  0.2691, -0.1562,\n",
            "          0.1362,  0.2291],\n",
            "        [-0.3132,  0.0211,  0.2937, -0.1027, -0.2754,  0.1530, -0.2383,  0.2560,\n",
            "         -0.2284,  0.1777],\n",
            "        [ 0.1441,  0.0385,  0.1329, -0.2058,  0.0093,  0.1870,  0.2973,  0.2103,\n",
            "         -0.1391, -0.2611],\n",
            "        [ 0.0026,  0.1237, -0.0555,  0.2653,  0.0924,  0.0348,  0.1522, -0.1670,\n",
            "         -0.2538,  0.0745],\n",
            "        [ 0.2783,  0.2584, -0.1085, -0.0836,  0.2771,  0.1885,  0.2261, -0.0456,\n",
            "          0.3038, -0.2364]])\n",
            "state1-key: fc1.bias, value: tensor([-0.0671,  0.1637, -0.1931,  0.2826, -0.2874])\n",
            "state1-key: fc2.weight, value: tensor([[-0.1178,  0.2662,  0.2803,  0.1707,  0.0194]])\n",
            "state1-key: fc2.bias, value: tensor([0.2299])\n",
            "state2\n",
            "fc1.weight\n",
            "tensor([[ 0.8825,  0.4105,  0.5405,  0.8810, -0.9110,  0.8877,  0.8073, -0.4685,\n",
            "          0.4087,  0.6872],\n",
            "        [-0.9397,  0.0632,  0.8812, -0.3082, -0.8263,  0.4590, -0.7150,  0.7679,\n",
            "         -0.6852,  0.5332],\n",
            "        [ 0.4324,  0.1156,  0.3986, -0.6173,  0.0279,  0.5610,  0.8919,  0.6309,\n",
            "         -0.4173, -0.7833],\n",
            "        [ 0.0079,  0.3712, -0.1664,  0.7960,  0.2773,  0.1045,  0.4567, -0.5011,\n",
            "         -0.7613,  0.2234],\n",
            "        [ 0.8349,  0.7753, -0.3254, -0.2507,  0.8314,  0.5656,  0.6782, -0.1368,\n",
            "          0.9115, -0.7092]])\n",
            "fc1.bias\n",
            "tensor([-0.2012,  0.4910, -0.5792,  0.8478, -0.8621])\n",
            "fc2.weight\n",
            "tensor([[-0.3535,  0.7985,  0.8409,  0.5121,  0.0581]])\n",
            "fc2.bias\n",
            "tensor([0.6897])\n",
            "state2\n",
            "fc1.weight\n",
            "tensor([[ 0.8825,  0.4105,  0.5405,  0.8810, -0.9110,  0.8877,  0.8073, -0.4685,\n",
            "          0.4087,  0.6872],\n",
            "        [-0.9397,  0.0632,  0.8812, -0.3082, -0.8263,  0.4590, -0.7150,  0.7679,\n",
            "         -0.6852,  0.5332],\n",
            "        [ 0.4324,  0.1156,  0.3986, -0.6173,  0.0279,  0.5610,  0.8919,  0.6309,\n",
            "         -0.4173, -0.7833],\n",
            "        [ 0.0079,  0.3712, -0.1664,  0.7960,  0.2773,  0.1045,  0.4567, -0.5011,\n",
            "         -0.7613,  0.2234],\n",
            "        [ 0.8349,  0.7753, -0.3254, -0.2507,  0.8314,  0.5656,  0.6782, -0.1368,\n",
            "          0.9115, -0.7092]])\n",
            "fc1.bias\n",
            "tensor([-0.2012,  0.4910, -0.5792,  0.8478, -0.8621])\n",
            "fc2.weight\n",
            "tensor([[-0.3535,  0.7985,  0.8409,  0.5121,  0.0581]])\n",
            "fc2.bias\n",
            "tensor([0.6897])\n",
            "state3\n",
            "fc1.weight\n",
            "tensor([[ 0.5884,  0.2737,  0.3603,  0.5874, -0.6073,  0.5918,  0.5382, -0.3123,\n",
            "          0.2725,  0.4581],\n",
            "        [-0.6265,  0.0421,  0.5875, -0.2054, -0.5509,  0.3060, -0.4767,  0.5119,\n",
            "         -0.4568,  0.3554],\n",
            "        [ 0.2883,  0.0771,  0.2657, -0.4116,  0.0186,  0.3740,  0.5946,  0.4206,\n",
            "         -0.2782, -0.5222],\n",
            "        [ 0.0053,  0.2475, -0.1110,  0.5307,  0.1849,  0.0697,  0.3044, -0.3341,\n",
            "         -0.5075,  0.1489],\n",
            "        [ 0.5566,  0.5169, -0.2170, -0.1671,  0.5543,  0.3771,  0.4522, -0.0912,\n",
            "          0.6077, -0.4728]])\n",
            "fc1.bias\n",
            "tensor([-0.1342,  0.3273, -0.3861,  0.5652, -0.5748])\n",
            "fc2.weight\n",
            "tensor([[-0.2357,  0.5323,  0.5606,  0.3414,  0.0388]])\n",
            "fc2.bias\n",
            "tensor([0.4598])\n",
            "load new state\n",
            "Parameter containing:\n",
            "tensor([[ 0.5884,  0.2737,  0.3603,  0.5874, -0.6073,  0.5918,  0.5382, -0.3123,\n",
            "          0.2725,  0.4581],\n",
            "        [-0.6265,  0.0421,  0.5875, -0.2054, -0.5509,  0.3060, -0.4767,  0.5119,\n",
            "         -0.4568,  0.3554],\n",
            "        [ 0.2883,  0.0771,  0.2657, -0.4116,  0.0186,  0.3740,  0.5946,  0.4206,\n",
            "         -0.2782, -0.5222],\n",
            "        [ 0.0053,  0.2475, -0.1110,  0.5307,  0.1849,  0.0697,  0.3044, -0.3341,\n",
            "         -0.5075,  0.1489],\n",
            "        [ 0.5566,  0.5169, -0.2170, -0.1671,  0.5543,  0.3771,  0.4522, -0.0912,\n",
            "          0.6077, -0.4728]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.1342,  0.3273, -0.3861,  0.5652, -0.5748], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[-0.2357,  0.5323,  0.5606,  0.3414,  0.0388]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.4598], requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Net1(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net1, self).__init__()\n",
        "        self.fc1 = nn.Linear(10, 5)  # 14*14 from image dimension\n",
        "        self.fc2 = nn.Linear(5, 1)  # 14*14 from image dimension\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "net = Net1()\n",
        "# print(net)\n",
        "# print(net.parameters)\n",
        "# for param in net.parameters():\n",
        "#   print(param)\n",
        "\n",
        "state1 = net.state_dict()\n",
        "\n",
        "# print(state1)\n",
        "\n",
        "# print(state1.keys)\n",
        "\n",
        "for statekey in state1.keys():\n",
        "  print(f\"state1-key: {statekey}, value: {state1[statekey]}\")\n",
        "\n",
        "#state2 = state1*2\n",
        "\n",
        "# print(state1)\n",
        "# print(state2)\n",
        "\n",
        "from collections import OrderedDict\n",
        "state2 = OrderedDict()\n",
        "\n",
        "for statekey in state1.keys():\n",
        "  # print(statekey)\n",
        "  # print(state1[statekey])\n",
        "  # print(\"$$$$$$$$$$$$$$$\")\n",
        "  state2[statekey] = state1[statekey] * 3\n",
        "\n",
        "print(\"state2\")\n",
        "for statekey in state2.keys():\n",
        "  print(statekey)\n",
        "  print(state2[statekey])\n",
        "\n",
        "\n",
        "print(\"state2\")\n",
        "for statekey in state2.keys():\n",
        "  print(statekey)\n",
        "  print(state2[statekey])\n",
        "\n",
        "state3 = OrderedDict()\n",
        "for statekey in state1.keys():\n",
        "  state3[statekey] = state2[statekey] - state1[statekey]\n",
        "\n",
        "print(\"state3\")\n",
        "for statekey in state3.keys():\n",
        "  print(statekey)\n",
        "  print(state3[statekey])\n",
        "\n",
        "net.load_state_dict(state3)\n",
        "print(\"load new state\")\n",
        "for param in net.parameters():\n",
        "  print(param)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "msFuzoRlyD_u"
      },
      "outputs": [],
      "source": [
        "# high CIFAR10 test accuracy\n",
        "\n",
        "#https://github.com/HaNguyenPhuong/CIFAR10-image-classification/blob/main/CIFAR10_image_classification.ipynb\n",
        "#https://medium.com/mlearning-ai/cifar10-image-classification-in-pytorch-e5185176fbef#id_token=eyJhbGciOiJSUzI1NiIsImtpZCI6IjZmNzI1NDEwMWY1NmU0MWNmMzVjOTkyNmRlODRhMmQ1NTJiNGM2ZjEiLCJ0eXAiOiJKV1QifQ.eyJpc3MiOiJodHRwczovL2FjY291bnRzLmdvb2dsZS5jb20iLCJhenAiOiIyMTYyOTYwMzU4MzQtazFrNnFlMDYwczJ0cDJhMmphbTRsamRjbXMwMHN0dGcuYXBwcy5nb29nbGV1c2VyY29udGVudC5jb20iLCJhdWQiOiIyMTYyOTYwMzU4MzQtazFrNnFlMDYwczJ0cDJhMmphbTRsamRjbXMwMHN0dGcuYXBwcy5nb29nbGV1c2VyY29udGVudC5jb20iLCJzdWIiOiIxMTgyNDg3NTYyNTY0NjM0NzQ0NTQiLCJlbWFpbCI6ImtoYW5obGluaHRyYW4yNDEyODdAZ21haWwuY29tIiwiZW1haWxfdmVyaWZpZWQiOnRydWUsIm5iZiI6MTY5NTg1MzYxNywibmFtZSI6IktoYW5oIExpbmggVHJhbiIsInBpY3R1cmUiOiJodHRwczovL2xoMy5nb29nbGV1c2VyY29udGVudC5jb20vYS9BQ2c4b2NLR1JtZlZDRWQ4clRLa0wxWVNQd1hIQ2FKWE5MT3p5VlZLNHVKWnNsQ3FHQT1zOTYtYyIsImdpdmVuX25hbWUiOiJLaGFuaCBMaW5oIiwiZmFtaWx5X25hbWUiOiJUcmFuIiwibG9jYWxlIjoiZW4iLCJpYXQiOjE2OTU4NTM5MTcsImV4cCI6MTY5NTg1NzUxNywianRpIjoiZTAxMzQ3ZDg1NDUyMjgyNzA2MzJlZTA2NGI2MDExNDJlNmI4YTE4MiJ9.JAlT5PtJJ4GyCPHfs2pXe1NNpDfnDVjId3yhJ91bKp2-7DudyX2HwG1T6Nn6mDKiGz6cfqq8XIsXh-v9YnjCGEzD3bsSN1UXWlkuN4vP6MgAEnZiKPz_11RYO-u99-u8WMqU2F0NUY0Q_lpc-dX4k6Llpdk69BSr_-EybDY9Fcnngzbdf05L3ElvwYJxPDKutGZTotV06_rM-2mJTMFyg53wz25UVisDgS38vHuK6Hh5oqBFfcG9csCWPX_51iGPvk4nA46rRIEP34Vb3jREnCSLYubq2jcwit70Clyxs1Qo89uQ6JUMfNLvo7N9agKltiwk1FMFZHsa-aNJEMRx3A\n",
        "#https://github.com/mtrencseni/pytorch-playground/blob/master/05-cifar-10/CIFAR-10.ipynb\n",
        "#https://github.com/soapisnotfat/pytorch-cifar10/blob/master/main.py\n",
        "#https://lightning.ai/docs/pytorch/stable/notebooks/lightning_examples/cifar10-baseline.html\n",
        "#https://github.com/kuangliu/pytorch-cifar\n",
        "#https://www.kaggle.com/code/vikasbhadoria/cifar10-high-accuracy-model-build-on-pytorch\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8AwBTCfvfZ7B"
      },
      "outputs": [],
      "source": [
        "#DPSGD\n",
        "#https://medium.com/pytorch/differential-privacy-series-part-1-dp-sgd-algorithm-explained-12512c3959a3\n",
        "#https://github.com/ailabstw/blurnn/blob/master/blurnn/_util.py\n",
        "#https://github.com/ebagdasa/pytorch-privacy/blob/master/training.py\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVxMjn1faYjC",
        "outputId": "4c1725a4-306a-482c-aec4-c4425b579f3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x:  tensor([[2.],\n",
            "        [3.]])\n",
            "y:  tensor([[5.],\n",
            "        [7.]])\n",
            "w:  Parameter containing:\n",
            "tensor([[3.]], requires_grad=True)\n",
            "b:  Parameter containing:\n",
            "tensor([5.], requires_grad=True)\n",
            "tensor([[11.],\n",
            "        [14.]], grad_fn=<AddmmBackward0>)\n",
            "loss:  42.5\n",
            "name:  weight\n",
            "data:  tensor([[3.]])\n",
            "grad:  tensor([[33.]])\n",
            "data after adding grad:  tensor([[36.]])\n",
            "clipped param.grad:  tensor([[14.]])\n",
            "noise:  tensor([[0.0637]])\n",
            "data after adding grad and noise:  tensor([[36.0637]])\n",
            "eeeee\n",
            "name:  bias\n",
            "data:  tensor([5.])\n",
            "grad:  tensor([13.])\n",
            "data after adding grad:  tensor([18.])\n",
            "clipped param.grad:  tensor([13.])\n",
            "noise:  tensor([-0.0771])\n",
            "data after adding grad and noise:  tensor([17.9229])\n",
            "eeeee\n",
            "weight\n",
            "tensor([[36.0637]])\n",
            "torch.float32\n",
            "bias\n",
            "tensor([17.9229])\n",
            "torch.float32\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-5-b3897bf391f6>:95: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  torch.nn.utils.clip_grad_norm(parameter, max_norm=norml2, norm_type=2.0)\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "We define the weight of a network\n",
        "We want to define the input\n",
        "We want to compute the derive for each data point\n",
        "We want to compute the derive for many data points\n",
        "We generate noise\n",
        "We clip the gradient\n",
        "'''\n",
        "\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torchvision.transforms as transforms\n",
        "from collections import OrderedDict\n",
        "\n",
        "#y = 2x + 1\n",
        "# x = torch.tensor([1,3],dtype=torch.float32).view(-1, 1)\n",
        "# y = torch.tensor([3,7],dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "\n",
        "# x = torch.tensor([2],dtype=torch.float32).view(-1, 1)\n",
        "# y = torch.tensor([5],dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "# x = torch.tensor([3],dtype=torch.float32).view(-1, 1)\n",
        "# y = torch.tensor([7],dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "x = torch.tensor([2,3],dtype=torch.float32).view(-1, 1)\n",
        "y = torch.tensor([5,7],dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "print(\"x: \", x)\n",
        "print(\"y: \", y)\n",
        "\n",
        "\n",
        "# Build a fully connected layer.\n",
        "linear = nn.Linear(1, 1)\n",
        "# print ('w: ', linear.weight)\n",
        "# print ('b: ', linear.bias)\n",
        "\n",
        "\n",
        "state1 = linear.state_dict()\n",
        "\n",
        "# for statekey in state1.keys():\n",
        "#   print(statekey)\n",
        "#   print(state1[statekey])\n",
        "#   print(state1[statekey].dtype)\n",
        "\n",
        "state3 = OrderedDict()\n",
        "state3['weight'] = torch.tensor([3],dtype=torch.float32).view(-1, 1)\n",
        "state3['bias'] = torch.tensor([5],dtype=torch.float32)\n",
        "linear.load_state_dict(state3)\n",
        "\n",
        "print ('w: ', linear.weight)\n",
        "print ('b: ', linear.bias)\n",
        "\n",
        "\n",
        "# Build loss function and optimizer.\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(linear.parameters(), lr=0.01)\n",
        "optimizer.zero_grad()\n",
        "\n",
        "# Forward pass.\n",
        "pred = linear(x)\n",
        "\n",
        "print(pred)\n",
        "\n",
        "# Compute loss.\n",
        "loss = criterion(pred, y)\n",
        "print('loss: ', loss.item())\n",
        "\n",
        "loss.backward()\n",
        "\n",
        "'''\n",
        "1. y = wx + b // w -- weight and b -- bias\n",
        "2. (x=2/3/(2,3), y*= 5/7/(5,7))\n",
        "3. w = 3, b = 5\n",
        "4. L = (y*-y)^2\n",
        "5. dL/dw = dL/dy*dy/dw = 2(y*-y)*dy/dw = 2(y*-y)*x // 24/42/(24+42)/2=33\n",
        "6. dL/db = dL/dy*dy/db = 2(y*-y)*dy/db = 2(y*-y)*1 // 12/14/(12+14)/2=13\n",
        "'''\n",
        "\n",
        "norml2 = 14\n",
        "mean = 0.0\n",
        "std = 0.1\n",
        "\n",
        "for name, parameter in linear.named_parameters():\n",
        "  print(\"name: \", name)\n",
        "  print(\"data: \", parameter.data)\n",
        "  print(\"grad: \", parameter.grad)\n",
        "  parameter.data = parameter.data + parameter.grad\n",
        "  print(\"data after adding grad: \", parameter.data)\n",
        "  torch.nn.utils.clip_grad_norm(parameter, max_norm=norml2, norm_type=2.0)\n",
        "  print(\"clipped param.grad: \", parameter.grad)\n",
        "  noise = torch.normal(mean=mean, std=std, size=parameter.shape)\n",
        "  print(\"noise: \", noise)\n",
        "  parameter.data = parameter.data + noise\n",
        "  print(\"data after adding grad and noise: \", parameter.data)\n",
        "  print(\"eeeee\")\n",
        "\n",
        "state1 = linear.state_dict()\n",
        "\n",
        "for statekey in state1.keys():\n",
        "  print(statekey)\n",
        "  print(state1[statekey])\n",
        "  print(state1[statekey].dtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_PilUTqFpcpP",
        "outputId": "8c0b14d0-01cd-4027-fa01-f040f9683159"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x:  tensor([[2.],\n",
            "        [3.]])\n",
            "y:  tensor([[5.],\n",
            "        [7.]])\n",
            "w:  Parameter containing:\n",
            "tensor([[3.]], requires_grad=True)\n",
            "b:  Parameter containing:\n",
            "tensor([5.], requires_grad=True)\n",
            "grad value:  (tensor([[24.]]), tensor([12.]))\n",
            "double check:  tensor([[3.]])  ,  None\n",
            "double check:  tensor([5.])  ,  None\n",
            "grad value:  (tensor([[42.]]), tensor([14.]))\n",
            "double check:  tensor([[3.]])  ,  None\n",
            "double check:  tensor([5.])  ,  None\n",
            "sample_grads 1:  [(tensor([[24.]]), tensor([12.])), (tensor([[42.]]), tensor([14.]))]\n",
            "sample_grads 2:  <zip object at 0x7f76393592c0>\n",
            "sample_grads 3:  [tensor([[[24.]],\n",
            "\n",
            "        [[42.]]]), tensor([[12.],\n",
            "        [14.]])]\n",
            "[tensor([[66.]]), tensor([26.])]\n",
            "second time #################################\n",
            "second time #################################\n",
            "grad value:  (tensor([[24.]]), tensor([12.]))\n",
            "double check:  tensor([[3.]])  ,  None\n",
            "double check:  tensor([5.])  ,  None\n",
            "grad value:  (tensor([[42.]]), tensor([14.]))\n",
            "double check:  tensor([[3.]])  ,  None\n",
            "double check:  tensor([5.])  ,  None\n",
            "sample_grads 1:  [(tensor([[24.]]), tensor([12.])), (tensor([[42.]]), tensor([14.]))]\n",
            "sample_grads 2:  <zip object at 0x7f770c116f80>\n",
            "sample_grads 3:  [tensor([[[24.]],\n",
            "\n",
            "        [[42.]]]), tensor([[12.],\n",
            "        [14.]])]\n",
            "[tensor([[66.]]), tensor([26.])]\n",
            "param:  tensor([[3.]])\n",
            "param:  tensor([5.])\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "We define the weight of a network\n",
        "We want to define the input\n",
        "We want to compute the derive for each data point\n",
        "We want to compute the derive for many data points\n",
        "We generate noise\n",
        "We clip the gradient\n",
        "'''\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torchvision.transforms as transforms\n",
        "from collections import OrderedDict\n",
        "\n",
        "'''\n",
        "0. data true generation: y = 2x + 1\n",
        "1. y = wx + b // w -- weight and b -- bias\n",
        "2. (x=2/3/(2,3), y*= 5/7/(5,7))\n",
        "3. w = 3, b = 5\n",
        "4. L = (y*-y)^2\n",
        "5. dL/dw = dL/dy*dy/dw = 2(y*-y)*dy/dw = 2(y*-y)*x // 24/42/(24+42)/2=33\n",
        "6. dL/db = dL/dy*dy/db = 2(y*-y)*dy/db = 2(y*-y)*1 // 12/14/(12+14)/2=13\n",
        "'''\n",
        "\n",
        "x = torch.tensor([2,3],dtype=torch.float32).view(-1, 1)\n",
        "y = torch.tensor([5,7],dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "print(\"x: \", x)\n",
        "print(\"y: \", y)\n",
        "\n",
        "# Build a fully connected layer.\n",
        "linear = nn.Linear(1, 1)\n",
        "\n",
        "state3 = OrderedDict()\n",
        "state3['weight'] = torch.tensor([3],dtype=torch.float32).view(-1, 1)\n",
        "state3['bias'] = torch.tensor([5],dtype=torch.float32)\n",
        "linear.load_state_dict(state3)\n",
        "\n",
        "print ('w: ', linear.weight)\n",
        "print ('b: ', linear.bias)\n",
        "\n",
        "# Build loss function and optimizer.\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "def compute_grad(sample, target,model,loss_fn):\n",
        "    sample = sample.unsqueeze(0)  # prepend batch dimension for processing\n",
        "    target = target.unsqueeze(0)\n",
        "\n",
        "    prediction = model(sample)\n",
        "    loss = loss_fn(prediction, target)\n",
        "\n",
        "    grad = torch.autograd.grad(loss, list(model.parameters()))\n",
        "    print(\"grad value: \", grad)\n",
        "\n",
        "    for param in model.parameters():\n",
        "      print(\"double check: \", param.data, \" , \", param.grad)\n",
        "\n",
        "\n",
        "    return grad\n",
        "\n",
        "\n",
        "def compute_sample_grads(data, targets, model, loss_fn, batch_size):\n",
        "    \"\"\" manually process each sample with per sample gradient \"\"\"\n",
        "    sample_grads = [compute_grad(data[i], targets[i],model,loss_fn) for i in range(batch_size)]\n",
        "    print(\"sample_grads 1: \", sample_grads)\n",
        "    sample_grads = zip(*sample_grads)\n",
        "    print(\"sample_grads 2: \", sample_grads)\n",
        "    sample_grads = [torch.stack(shards) for shards in sample_grads]\n",
        "    print(\"sample_grads 3: \", sample_grads)\n",
        "    return sample_grads\n",
        "\n",
        "data = x\n",
        "targets = y\n",
        "model = linear\n",
        "loss_fn = criterion\n",
        "batch_size = len(x)\n",
        "\n",
        "per_sample_grads = compute_sample_grads(data, targets, model, loss_fn, batch_size)\n",
        "\n",
        "sum_grad = []\n",
        "for grad_layer in per_sample_grads:\n",
        "  sum_grad.append(sum(grad_layer))\n",
        "\n",
        "print(sum_grad)\n",
        "\n",
        "\n",
        "# for model_par, grad in zip(linear.parameters(),sum_grad):\n",
        "#     print(\"model_par: \", model_par.data)\n",
        "#     print(\"grad: \", grad)\n",
        "#     model_par.data = model_par.data + grad\n",
        "\n",
        "# for param in linear.parameters():\n",
        "#   print(\"param: \", param.data)\n",
        "\n",
        "print(\"second time #################################\")\n",
        "print(\"second time #################################\")\n",
        "'''\n",
        "The same results. It means torch.autograd.grad(loss, list(model.parameters())) cleans the models.grad\n",
        "This is why if we print param.grad, then we see the value NONE !!!!!!!\n",
        "'''\n",
        "\n",
        "per_sample_grads = compute_sample_grads(data, targets, model, loss_fn, batch_size)\n",
        "\n",
        "sum_grad = []\n",
        "for grad_layer in per_sample_grads:\n",
        "  sum_grad.append(sum(grad_layer))\n",
        "\n",
        "print(sum_grad)\n",
        "\n",
        "\n",
        "# for model_par, grad in zip(linear.parameters(),sum_grad):\n",
        "#     print(\"model_par: \", model_par.data)\n",
        "#     print(\"grad: \", grad)\n",
        "#     model_par.data = model_par.data + grad\n",
        "\n",
        "for param in linear.parameters():\n",
        "  print(\"param: \", param.data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZU2ypaQIxPQ",
        "outputId": "1a9a4c78-2b62-41c8-aa73-e389db3305f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x:  tensor([[2.],\n",
            "        [3.]])\n",
            "y:  tensor([[5.],\n",
            "        [7.]])\n",
            "w:  Parameter containing:\n",
            "tensor([[3.]], requires_grad=True)\n",
            "b:  Parameter containing:\n",
            "tensor([5.], requires_grad=True)\n",
            "before clipping:  tensor([[24.]])\n",
            "after clipping:  tensor([[14.]])\n",
            "before clipping:  tensor([12.])\n",
            "after clipping:  tensor([12.])\n",
            "before clipping:  tensor([[42.]])\n",
            "after clipping:  tensor([[14.]])\n",
            "before clipping:  tensor([14.])\n",
            "after clipping:  tensor([14.0000])\n",
            "sample_grads 1:  [[tensor([[14.]]), tensor([12.])], [tensor([[14.]]), tensor([14.0000])]]\n",
            "sample_grads 2:  <zip object at 0x7becd1c00800>\n",
            "sample_grads 3:  [tensor([[[14.]],\n",
            "\n",
            "        [[14.]]]), tensor([[12.0000],\n",
            "        [14.0000]])]\n",
            "[tensor([[28.]]), tensor([26.])]\n",
            "model_par:  tensor([[3.]])\n",
            "grad:  tensor([[28.]])\n",
            "model_par:  tensor([5.])\n",
            "grad:  tensor([26.])\n",
            "param:  tensor([[31.]])\n",
            "param:  tensor([31.])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-26-670806839942>:64: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  torch.nn.utils.clip_grad_norm(param, max_norm=14, norm_type=2.0)\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "We define the weight of a network\n",
        "We want to define the input\n",
        "We want to compute the derive for each data point\n",
        "We want to compute the derive for many data points\n",
        "We generate noise\n",
        "We clip the gradient\n",
        "'''\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torchvision.transforms as transforms\n",
        "from collections import OrderedDict\n",
        "\n",
        "'''\n",
        "0. data true generation: y = 2x + 1\n",
        "1. y = wx + b // w -- weight and b -- bias\n",
        "2. (x=2/3/(2,3), y*= 5/7/(5,7))\n",
        "3. w = 3, b = 5\n",
        "4. L = (y*-y)^2\n",
        "5. dL/dw = dL/dy*dy/dw = 2(y*-y)*dy/dw = 2(y*-y)*x // 24/42/(24+42)/2=33\n",
        "6. dL/db = dL/dy*dy/db = 2(y*-y)*dy/db = 2(y*-y)*1 // 12/14/(12+14)/2=13\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "x = torch.tensor([2,3],dtype=torch.float32).view(-1, 1)\n",
        "y = torch.tensor([5,7],dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "print(\"x: \", x)\n",
        "print(\"y: \", y)\n",
        "\n",
        "# Build a fully connected layer.\n",
        "linear = nn.Linear(1, 1)\n",
        "optimizer = torch.optim.SGD(linear.parameters(), lr=0.01)\n",
        "\n",
        "\n",
        "\n",
        "state3 = OrderedDict()\n",
        "state3['weight'] = torch.tensor([3],dtype=torch.float32).view(-1, 1)\n",
        "state3['bias'] = torch.tensor([5],dtype=torch.float32)\n",
        "linear.load_state_dict(state3)\n",
        "\n",
        "print ('w: ', linear.weight)\n",
        "print ('b: ', linear.bias)\n",
        "\n",
        "# Build loss function and optimizer.\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "def compute_grad(sample, target,model,loss_fn):\n",
        "    sample = sample.unsqueeze(0)  # prepend batch dimension for processing\n",
        "    target = target.unsqueeze(0)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    prediction = model(sample)\n",
        "    loss = loss_fn(prediction, target)\n",
        "    loss.backward()\n",
        "\n",
        "    grad = []\n",
        "    for param in model.parameters():#layer-wise clipping\n",
        "        print(\"before clipping: \", param.grad)\n",
        "        torch.nn.utils.clip_grad_norm(param, max_norm=14, norm_type=2.0)\n",
        "        print(\"after clipping: \", param.grad)\n",
        "        grad.append(param.grad)\n",
        "\n",
        "    # grad = torch.autograd.grad(loss, list(model.parameters()))\n",
        "    # print(\"grad value: \", grad)\n",
        "\n",
        "    return grad\n",
        "\n",
        "\n",
        "def compute_sample_grads(data, targets, model, loss_fn, batch_size):\n",
        "    \"\"\" manually process each sample with per sample gradient \"\"\"\n",
        "    sample_grads = [compute_grad(data[i], targets[i],model,loss_fn) for i in range(batch_size)]\n",
        "    print(\"sample_grads 1: \", sample_grads)\n",
        "    sample_grads = zip(*sample_grads)\n",
        "    print(\"sample_grads 2: \", sample_grads)\n",
        "    sample_grads = [torch.stack(shards) for shards in sample_grads]\n",
        "    print(\"sample_grads 3: \", sample_grads)\n",
        "    return sample_grads\n",
        "\n",
        "data = x\n",
        "targets = y\n",
        "model = linear\n",
        "loss_fn = criterion\n",
        "batch_size = len(x)\n",
        "\n",
        "per_sample_grads = compute_sample_grads(data, targets, model, loss_fn, batch_size)\n",
        "\n",
        "sum_grad = []\n",
        "for grad_layer in per_sample_grads:\n",
        "  sum_grad.append(sum(grad_layer))\n",
        "\n",
        "print(sum_grad)\n",
        "\n",
        "\n",
        "for model_par, grad in zip(linear.parameters(),sum_grad):\n",
        "    print(\"model_par: \", model_par.data)\n",
        "    print(\"grad: \", grad)\n",
        "    model_par.data = model_par.data + grad\n",
        "\n",
        "for param in linear.parameters():\n",
        "  print(\"param: \", param.data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_g0kOD2QOKN",
        "outputId": "1bbe6175-3bc5-48c7-fe28-7f87a806fc2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x:  tensor([[2.],\n",
            "        [3.]])\n",
            "y:  tensor([[5.],\n",
            "        [7.]])\n",
            "w:  Parameter containing:\n",
            "tensor([[3.]], requires_grad=True)\n",
            "b:  Parameter containing:\n",
            "tensor([5.], requires_grad=True)\n",
            "grad value before clipping:  (tensor([[24.]]), tensor([12.]))\n",
            "device:  cpu\n",
            "grad and uu:  tensor(24.) tensor([[24.]])\n",
            "grad and uu:  tensor(12.) tensor([12.])\n",
            "vv:  tensor([24., 12.])\n",
            "total_norm:  tensor(26.8328)\n",
            "clip_coef:  tensor(0.0373)\n",
            "clip_coef_clamped:  tensor(0.0373)\n",
            "grad value before clipping:  (tensor([[42.]]), tensor([14.]))\n",
            "device:  cpu\n",
            "grad and uu:  tensor(42.) tensor([[42.]])\n",
            "grad and uu:  tensor(14.) tensor([14.])\n",
            "vv:  tensor([42., 14.])\n",
            "total_norm:  tensor(44.2719)\n",
            "clip_coef:  tensor(0.0226)\n",
            "clip_coef_clamped:  tensor(0.0226)\n",
            "sample_grads 1:  [(tensor([[0.8944]]), tensor([0.4472])), (tensor([[0.9487]]), tensor([0.3162]))]\n",
            "sample_grads 2:  <zip object at 0x7becd1e91040>\n",
            "sample_grads 3:  [tensor([[[0.8944]],\n",
            "\n",
            "        [[0.9487]]]), tensor([[0.4472],\n",
            "        [0.3162]])]\n",
            "[tensor([[1.8431]]), tensor([0.7634])]\n",
            "model_par:  tensor([[3.]])\n",
            "grad:  tensor([[1.8431]])\n",
            "model_par:  tensor([5.])\n",
            "grad:  tensor([0.7634])\n",
            "param:  tensor([[4.8431]])\n",
            "param:  tensor([5.7634])\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "We define the weight of a network\n",
        "We want to define the input\n",
        "We want to compute the derive for each data point\n",
        "We want to compute the derive for many data points\n",
        "We generate noise\n",
        "We clip the gradient\n",
        "'''\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torchvision.transforms as transforms\n",
        "from collections import OrderedDict\n",
        "\n",
        "'''\n",
        "0. data true generation: y = 2x + 1\n",
        "1. y = wx + b // w -- weight and b -- bias\n",
        "2. (x=2/3/(2,3), y*= 5/7/(5,7))\n",
        "3. w = 3, b = 5\n",
        "4. L = (y*-y)^2\n",
        "5. dL/dw = dL/dy*dy/dw = 2(y*-y)*dy/dw = 2(y*-y)*x // 24/42/(24+42)/2=33\n",
        "6. dL/db = dL/dy*dy/db = 2(y*-y)*dy/db = 2(y*-y)*1 // 12/14/(12+14)/2=13\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "x = torch.tensor([2,3],dtype=torch.float32).view(-1, 1)\n",
        "y = torch.tensor([5,7],dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "print(\"x: \", x)\n",
        "print(\"y: \", y)\n",
        "\n",
        "# Build a fully connected layer.\n",
        "linear = nn.Linear(1, 1)\n",
        "optimizer = torch.optim.SGD(linear.parameters(), lr=0.01)\n",
        "\n",
        "\n",
        "\n",
        "state3 = OrderedDict()\n",
        "state3['weight'] = torch.tensor([3],dtype=torch.float32).view(-1, 1)\n",
        "state3['bias'] = torch.tensor([5],dtype=torch.float32)\n",
        "linear.load_state_dict(state3)\n",
        "\n",
        "print ('w: ', linear.weight)\n",
        "print ('b: ', linear.bias)\n",
        "\n",
        "# Build loss function and optimizer.\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "def compute_grad(sample, target,model,loss_fn):\n",
        "    sample = sample.unsqueeze(0)  # prepend batch dimension for processing\n",
        "    target = target.unsqueeze(0)\n",
        "\n",
        "\n",
        "    prediction = model(sample)\n",
        "    loss = loss_fn(prediction, target)\n",
        "\n",
        "    grads = torch.autograd.grad(loss, list(model.parameters())) #model clipping\n",
        "    print(\"grad value before clipping: \", grads)\n",
        "    #https://discuss.pytorch.org/t/how-to-clip-grad-norm-grads-from-torch-autograd-grad/137816/2\n",
        "\n",
        "    device = grads[0].device\n",
        "    print(\"device: \", device)\n",
        "    norm_type = 2.0\n",
        "    max_norm = 1.0 #This is clipping constant C\n",
        "\n",
        "    '''\n",
        "      https://discuss.pytorch.org/t/how-to-clip-grad-norm-grads-from-torch-autograd-grad/137816/2\n",
        "      We want to clip a tensor T in the way such that the normL2(clipped(T)) < max_norm --- or this is clipping constant C\n",
        "      step1. compute the total_norm\n",
        "            total_norm = torch.norm(torch.stack([torch.norm(grad.detach(), norm_type).to(device) for grad in grads]), norm_type)\n",
        "            step11. compute the sub_norm of sub_grad in grads\n",
        "                    torch.norm(grad.detach(), norm_type).to(device) for grad in grads\n",
        "                    grad.detach() makes sure we do the calculation without touch the computational graph\n",
        "            step12. stack all the values into a vector [sub_norm1, sub_norm2, ..., sub_normk]\n",
        "            step13. compute the total_norm\n",
        "\n",
        "      step2. we compute the multiplier clip_coef_clamp\n",
        "            step21. compute the clip_coef first\n",
        "                    clip_coef = max_norm / (total_norm + 1e-6)\n",
        "            step22. compute the clip_coef_clamped\n",
        "                    clip_coef_clamped = torch.clamp(clip_coef, max=1.0)\n",
        "\n",
        "      step3. we clip the tensor T as T*clip_coef_clamp\n",
        "    '''\n",
        "\n",
        "\n",
        "\n",
        "    # for grad in grads:\n",
        "    #   uu = torch.norm(grad.detach(), norm_type).to(device)\n",
        "    #   print(\"grad and uu: \", uu, grad)\n",
        "\n",
        "    # vv = torch.stack([torch.norm(grad.detach(), norm_type).to(device) for grad in grads])\n",
        "    # print(\"vv: \", vv)\n",
        "\n",
        "    # total_norm = torch.norm(torch.stack([torch.norm(grad.detach(), norm_type).to(device) for grad in grads]), norm_type)\n",
        "\n",
        "    # print(\"total_norm: \", total_norm)\n",
        "    # clip_coef = max_norm / (total_norm + 1e-6)\n",
        "    # print(\"clip_coef: \", clip_coef)\n",
        "    # #https://www.tutorialspoint.com/python-pytorch-clamp-method\n",
        "    # #clamp(tensor,min,max)\n",
        "    # clip_coef_clamped = torch.clamp(clip_coef, max=1.0)\n",
        "    # print(\"clip_coef_clamped: \", clip_coef_clamped)\n",
        "    # for grad in grads:\n",
        "    #     grad.detach().mul_(clip_coef_clamped.to(grad.device))\n",
        "\n",
        "    total_norm = torch.norm(torch.stack([torch.norm(grad.detach(), norm_type).to(device) for grad in grads]), norm_type)\n",
        "    clip_coef = max_norm / (total_norm + 1e-6)\n",
        "    #https://www.tutorialspoint.com/python-pytorch-clamp-method\n",
        "    #clamp(tensor,min,max)\n",
        "    clip_coef_clamped = torch.clamp(clip_coef, max=1.0)\n",
        "    for grad in grads:\n",
        "        grad.detach().mul_(clip_coef_clamped.to(grad.device))\n",
        "\n",
        "    return grads\n",
        "\n",
        "\n",
        "def compute_sample_grads(data, targets, model, loss_fn, batch_size):\n",
        "    \"\"\" manually process each sample with per sample gradient \"\"\"\n",
        "    sample_grads = [compute_grad(data[i], targets[i],model,loss_fn) for i in range(batch_size)]\n",
        "    print(\"sample_grads 1: \", sample_grads)\n",
        "    sample_grads = zip(*sample_grads)\n",
        "    print(\"sample_grads 2: \", sample_grads)\n",
        "    sample_grads = [torch.stack(shards) for shards in sample_grads]\n",
        "    print(\"sample_grads 3: \", sample_grads)\n",
        "    return sample_grads\n",
        "\n",
        "data = x\n",
        "targets = y\n",
        "model = linear\n",
        "loss_fn = criterion\n",
        "batch_size = len(x)\n",
        "\n",
        "per_sample_grads = compute_sample_grads(data, targets, model, loss_fn, batch_size)\n",
        "\n",
        "sum_grad = []\n",
        "for grad_layer in per_sample_grads:\n",
        "  sum_grad.append(sum(grad_layer))\n",
        "\n",
        "print(sum_grad)\n",
        "\n",
        "\n",
        "for model_par, grad in zip(linear.parameters(),sum_grad):\n",
        "    print(\"model_par: \", model_par.data)\n",
        "    print(\"grad: \", grad)\n",
        "    model_par.data = model_par.data + grad\n",
        "\n",
        "for param in linear.parameters():\n",
        "  print(\"param: \", param.data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dT83y2NWAgCa",
        "outputId": "d7240f69-dcf8-468f-8313-edb65e4ec398"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x:  tensor([[2.],\n",
            "        [3.]])\n",
            "y:  tensor([[5.],\n",
            "        [7.]])\n",
            "w:  Parameter containing:\n",
            "tensor([[3.]], requires_grad=True)\n",
            "b:  Parameter containing:\n",
            "tensor([5.], requires_grad=True)\n",
            "key:  weight  value:  tensor([[[24.]],\n",
            "\n",
            "        [[42.]]])\n",
            "key:  bias  value:  tensor([[12.],\n",
            "        [14.]])\n"
          ]
        }
      ],
      "source": [
        "#https://pytorch.org/tutorials/intermediate/per_sample_grads.html?utm_source=whats_new_tutorials&utm_medium=per_sample_grads\n",
        "#https://pytorch.org/docs/stable/generated/torch.func.grad.html\n",
        "#https://towardsdatascience.com/introduction-to-functional-pytorch-b5bf739e1e6e\n",
        "\n",
        "\n",
        "'''\n",
        "We define the weight of a network\n",
        "We want to define the input\n",
        "We want to compute the derive for each data point\n",
        "We want to compute the derive for many data points\n",
        "We generate noise\n",
        "We clip the gradient\n",
        "'''\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torchvision.transforms as transforms\n",
        "from collections import OrderedDict\n",
        "\n",
        "'''\n",
        "0. data true generation: y = 2x + 1\n",
        "1. y = wx + b // w -- weight and b -- bias\n",
        "2. (x=2/3/(2,3), y*= 5/7/(5,7))\n",
        "3. w = 3, b = 5\n",
        "4. L = (y*-y)^2\n",
        "5. dL/dw = dL/dy*dy/dw = 2(y*-y)*dy/dw = 2(y*-y)*x // 24/42/(24+42)/2=33\n",
        "6. dL/db = dL/dy*dy/db = 2(y*-y)*dy/db = 2(y*-y)*1 // 12/14/(12+14)/2=13\n",
        "'''\n",
        "x = torch.tensor([2,3],dtype=torch.float32).view(-1, 1)\n",
        "y = torch.tensor([5,7],dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "print(\"x: \", x)\n",
        "print(\"y: \", y)\n",
        "\n",
        "# Build a fully connected layer.\n",
        "linear = nn.Linear(1, 1)\n",
        "optimizer = torch.optim.SGD(linear.parameters(), lr=0.01)\n",
        "\n",
        "state3 = OrderedDict()\n",
        "state3['weight'] = torch.tensor([3],dtype=torch.float32).view(-1, 1)\n",
        "state3['bias'] = torch.tensor([5],dtype=torch.float32)\n",
        "linear.load_state_dict(state3)\n",
        "\n",
        "print ('w: ', linear.weight)\n",
        "print ('b: ', linear.bias)\n",
        "\n",
        "# Build loss function and optimizer.\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "\n",
        "from torch.func import functional_call, vmap, grad\n",
        "\n",
        "def compute_loss(params, buffers, sample, target):\n",
        "    batch = sample.unsqueeze(0)\n",
        "    targets = target.unsqueeze(0)\n",
        "\n",
        "    predictions = functional_call(model, (params, buffers), (batch,))\n",
        "    loss = loss_fn(predictions, targets)\n",
        "    return loss\n",
        "\n",
        "ft_compute_grad = grad(compute_loss)\n",
        "ft_compute_sample_grad = vmap(ft_compute_grad, in_dims=(None, None, 0, 0))\n",
        "\n",
        "'''\n",
        "The ft_compute_grad function computes the gradient for a single (sample, target) pair.\n",
        "We can use vmap to get it to compute the gradient over an entire batch of samples and targets.\n",
        "Note that in_dims=(None, None, 0, 0) because we wish to\n",
        "map ft_compute_grad over the 0th dimension of the data and targets, and use the same params and buffers for each.\n",
        "'''\n",
        "sample = x\n",
        "targets = y\n",
        "model = linear\n",
        "loss_fn = criterion\n",
        "\n",
        "params = {k: v.detach() for k, v in model.named_parameters()}\n",
        "buffers = {k: v.detach() for k, v in model.named_buffers()}\n",
        "\n",
        "ft_per_sample_grads = ft_compute_sample_grad(params, buffers, data, targets)\n",
        "\n",
        "for key in ft_per_sample_grads.keys():\n",
        "    print(\"key: \", key, \" value: \", ft_per_sample_grads[key])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQatq7XLxyVk",
        "outputId": "b717a24e-fd63-4ffe-a5d1-614242f4ab25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x:  tensor([[2.],\n",
            "        [3.]])\n",
            "y:  tensor([[5.],\n",
            "        [7.]])\n",
            "w:  Parameter containing:\n",
            "tensor([[3.]], requires_grad=True)\n",
            "b:  Parameter containing:\n",
            "tensor([5.], requires_grad=True)\n",
            "before param tensor([[33.]])\n",
            "before param tensor([13.])\n",
            "after param tensor([[0.9304]])\n",
            "after param tensor([0.3665])\n"
          ]
        }
      ],
      "source": [
        "#batch clipping\n",
        "\n",
        "#https://pytorch.org/tutorials/intermediate/per_sample_grads.html?utm_source=whats_new_tutorials&utm_medium=per_sample_grads\n",
        "#https://pytorch.org/docs/stable/generated/torch.func.grad.html\n",
        "#https://towardsdatascience.com/introduction-to-functional-pytorch-b5bf739e1e6e\n",
        "\n",
        "\n",
        "'''\n",
        "We define the weight of a network\n",
        "We want to define the input\n",
        "We want to compute the derive for each data point\n",
        "We want to compute the derive for many data points\n",
        "We generate noise\n",
        "We clip the gradient\n",
        "'''\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torchvision.transforms as transforms\n",
        "from collections import OrderedDict\n",
        "\n",
        "'''\n",
        "0. data true generation: y = 2x + 1\n",
        "1. y = wx + b // w -- weight and b -- bias\n",
        "2. (x=2/3/(2,3), y*= 5/7/(5,7))\n",
        "3. w = 3, b = 5\n",
        "4. L = (y*-y)^2\n",
        "5. dL/dw = dL/dy*dy/dw = 2(y*-y)*dy/dw = 2(y*-y)*x // 24/42/(24+42)/2=33\n",
        "6. dL/db = dL/dy*dy/db = 2(y*-y)*dy/db = 2(y*-y)*1 // 12/14/(12+14)/2=13\n",
        "'''\n",
        "x = torch.tensor([2,3],dtype=torch.float32).view(-1, 1)\n",
        "y = torch.tensor([5,7],dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "print(\"x: \", x)\n",
        "print(\"y: \", y)\n",
        "\n",
        "# Build a fully connected layer.\n",
        "linear = nn.Linear(1, 1)\n",
        "optimizer = torch.optim.SGD(linear.parameters(), lr=0.01)\n",
        "\n",
        "state3 = OrderedDict()\n",
        "state3['weight'] = torch.tensor([3],dtype=torch.float32).view(-1, 1)\n",
        "state3['bias'] = torch.tensor([5],dtype=torch.float32)\n",
        "linear.load_state_dict(state3)\n",
        "\n",
        "print ('w: ', linear.weight)\n",
        "print ('b: ', linear.bias)\n",
        "\n",
        "# Build loss function and optimizer.\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "samples = x\n",
        "targets = y\n",
        "model = linear\n",
        "\n",
        "# autgrad.grad approach\n",
        "model.zero_grad()\n",
        "outputs = model(samples)\n",
        "loss = criterion(outputs,targets)\n",
        "loss.backward()\n",
        "\n",
        "grads = list()\n",
        "for param in model.parameters():\n",
        "    print(f\"before param {param.grad}\")\n",
        "    grads.append(param.grad)\n",
        "\n",
        "device = grads[0].device\n",
        "norm_type = 2.0\n",
        "max_norm = 1.0\n",
        "total_norm = torch.norm(torch.stack([torch.norm(grad.detach(), norm_type).to(device) for grad in grads]), norm_type)\n",
        "clip_coef = max_norm / (total_norm + 1e-6)\n",
        "clip_coef_clamped = torch.clamp(clip_coef, max=1.0)\n",
        "\n",
        "for param in model.parameters():\n",
        "    param.grad.detach().mul_(clip_coef_clamped.to(grad.device))\n",
        "\n",
        "\n",
        "for param in model.parameters():\n",
        "  print(f\"after param {param.grad}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dUatvNN3wUA",
        "outputId": "064f39fb-016e-4075-c9bf-81fa05f69e6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x:  tensor([[2.],\n",
            "        [3.]])\n",
            "y:  tensor([[5.],\n",
            "        [7.]])\n",
            "w:  Parameter containing:\n",
            "tensor([[3.]], requires_grad=True)\n",
            "b:  Parameter containing:\n",
            "tensor([5.], requires_grad=True)\n",
            "key: weight, value: tensor([[[24.]],\n",
            "\n",
            "        [[42.]]])\n",
            "unstacked:  (tensor([[24.]]), tensor([[42.]]))\n",
            "key: bias, value: tensor([[12.],\n",
            "        [14.]])\n",
            "unstacked:  (tensor([12.]), tensor([14.]))\n",
            "sample 0 -- before grad tensor([[24.]])\n",
            "sample 0 -- after grad tensor([[0.8944]])\n",
            "sample 0 -- before grad tensor([12.])\n",
            "sample 0 -- after grad tensor([0.4472])\n",
            "sample 1 -- before grad tensor([[42.]])\n",
            "sample 1 -- after grad tensor([[0.9487]])\n",
            "sample 1 -- before grad tensor([14.])\n",
            "sample 1 -- after grad tensor([0.3162])\n"
          ]
        }
      ],
      "source": [
        "#https://pytorch.org/tutorials/intermediate/per_sample_grads.html?utm_source=whats_new_tutorials&utm_medium=per_sample_grads\n",
        "#https://pytorch.org/docs/stable/generated/torch.func.grad.html\n",
        "#https://towardsdatascience.com/introduction-to-functional-pytorch-b5bf739e1e6e\n",
        "#https://pytorch.org/functorch/stable/notebooks/per_sample_grads.html [PAY ATTENTION TO]\n",
        "\n",
        "\n",
        "'''\n",
        "We define the weight of a network\n",
        "We want to define the input\n",
        "We want to compute the derive for each data point\n",
        "We want to compute the derive for many data points\n",
        "We generate noise\n",
        "We clip the gradient\n",
        "'''\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torchvision.transforms as transforms\n",
        "from collections import OrderedDict\n",
        "from collections import defaultdict\n",
        "\n",
        "'''\n",
        "0. data true generation: y = 2x + 1\n",
        "1. y = wx + b // w -- weight and b -- bias\n",
        "2. (x=2/3/(2,3), y*= 5/7/(5,7))\n",
        "3. w = 3, b = 5\n",
        "4. L = (y*-y)^2\n",
        "5. dL/dw = dL/dy*dy/dw = 2(y*-y)*dy/dw = 2(y*-y)*x // 24/42/(24+42)/2=33\n",
        "6. dL/db = dL/dy*dy/db = 2(y*-y)*dy/db = 2(y*-y)*1 // 12/14/(12+14)/2=13\n",
        "'''\n",
        "x = torch.tensor([2,3],dtype=torch.float32).view(-1, 1)\n",
        "y = torch.tensor([5,7],dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "print(\"x: \", x)\n",
        "print(\"y: \", y)\n",
        "\n",
        "# Build a fully connected layer.\n",
        "linear = nn.Linear(1, 1)\n",
        "optimizer = torch.optim.SGD(linear.parameters(), lr=0.01)\n",
        "\n",
        "state3 = OrderedDict()\n",
        "state3['weight'] = torch.tensor([3],dtype=torch.float32).view(-1, 1)\n",
        "state3['bias'] = torch.tensor([5],dtype=torch.float32)\n",
        "linear.load_state_dict(state3)\n",
        "\n",
        "print ('w: ', linear.weight)\n",
        "print ('b: ', linear.bias)\n",
        "\n",
        "# Build loss function and optimizer.\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "from torch.func import functional_call, vmap, grad\n",
        "\n",
        "def compute_loss(params, buffers, model, loss_fn, sample, target):\n",
        "    batch = sample.unsqueeze(0)\n",
        "    targets = target.unsqueeze(0)\n",
        "\n",
        "    predictions = functional_call(model, (params, buffers), (batch,))\n",
        "    loss = loss_fn(predictions, targets)\n",
        "    return loss\n",
        "\n",
        "ft_compute_grad = grad(compute_loss)\n",
        "ft_compute_sample_grad = vmap(ft_compute_grad, in_dims=(None, None, None, None, 0, 0))\n",
        "\n",
        "'''\n",
        "The ft_compute_grad function computes the gradient for a single (sample, target) pair.\n",
        "# We can use vmap to get it to compute the gradient over an entire batch of samples and targets.\n",
        "# Note that in_dims=(None, None, 0, 0) because we wish to\n",
        "# map ft_compute_grad over the 0th dimension of the data and targets, and use the same params and buffers for each.\n",
        "# '''\n",
        "\n",
        "params = {k: v.detach() for k, v in linear.named_parameters()}\n",
        "buffers = {k: v.detach() for k, v in linear.named_buffers()}\n",
        "\n",
        "ft_per_sample_grads = ft_compute_sample_grad(params, buffers, linear, criterion, x, y)\n",
        "\n",
        "# for key in ft_per_sample_grads.keys():\n",
        "#     print(f\"key: {key}, value: {ft_per_sample_grads[key]}\")\n",
        "\n",
        "\n",
        "# for k, v in linear.named_parameters():\n",
        "#   print(f\"k: {k}, v.data: {v.data}, v.grad: {v.grad}\")\n",
        "\n",
        "for key,value in ft_per_sample_grads.items():\n",
        "    print(f\"key: {key}, value: {value}\")\n",
        "    unstacked = torch.unbind(value, dim=0)\n",
        "    print(\"unstacked: \", unstacked)\n",
        "\n",
        "\n",
        "#get back per_sample_grad\n",
        "num_samples = len(x)\n",
        "samples_grads = dict()\n",
        "\n",
        "for i in range(num_samples):\n",
        "  samples_grads[i] = OrderedDict()\n",
        "  samples_grads[i]['whole_grad'] = list()\n",
        "\n",
        "'''\n",
        "  1. Going through each layer in ft_per_sample_grads: key, value in ft_per_sample_grads.items()\n",
        "  2. unstack the stacked of len(x) layers: unstacked_grads = torch.unbind(value, dim=0)\n",
        "  3. redistribute the unstacked sample_layer_grad, i.e., samples_grads[i][key]\n",
        "  4. We create a new feature called \"whole_grad\" to combine all layer grads as a whole grad tensor. This is used for computing the full grad norm.\n",
        "     This full grad norm is used to compute the clipped sample grad later !!!!\n",
        "\n",
        "  Each sample has its own grad now but saved in the form of dictionary\n",
        "'''\n",
        "\n",
        "for key,value in ft_per_sample_grads.items():\n",
        "    #unstack the grads for each layer\n",
        "    unstacked_grads = torch.unbind(value, dim=0)\n",
        "    i = 0\n",
        "    for layer_grad in unstacked_grads:\n",
        "       samples_grads[i]['whole_grad'].append(layer_grad)\n",
        "       samples_grads[i][key] = layer_grad\n",
        "       i += 1\n",
        "\n",
        "#clipping the per_sample_grad\n",
        "for i in range(num_samples):\n",
        "    norm_type = 2.0\n",
        "    max_norm = 1.0 #This is clipping constant C\n",
        "\n",
        "    total_norm = torch.norm(torch.stack([torch.norm(grad.detach(), norm_type) for grad in samples_grads[i]['whole_grad']]), norm_type)\n",
        "    clip_coef = max_norm / (total_norm + 1e-6)\n",
        "    #https://www.tutorialspoint.com/python-pytorch-clamp-method\n",
        "    #clamp(tensor,min,max)\n",
        "    '''\n",
        "      It is interesting to see that the samples_grads[i]['whole_grad'] contains the pointer to each tensor rather than the value.\n",
        "      This is why if we clipping the grad in samples_grads[i]['whole_grad'], all samples_grads[i][layer_grad] is clipped at the same time.\n",
        "    '''\n",
        "    clip_coef_clamped = torch.clamp(clip_coef, max=1.0)\n",
        "    # for grad in samples_grads[i]['whole_grad']:\n",
        "\n",
        "    #     print(f\"sample {i} -- before grad {grad}\")\n",
        "    #     grad.detach().mul_(clip_coef_clamped)\n",
        "    #     print(f\"sample {i} -- after grad {grad}\")\n",
        "\n",
        "    samples_grads[i].pop(\"whole_grad\")\n",
        "    for layer, grad in samples_grads[i].items():\n",
        "        print(f\"sample {i} -- before grad {grad}\")\n",
        "        grad.detach().mul_(clip_coef_clamped)\n",
        "        print(f\"sample {i} -- after grad {grad}\")\n",
        "\n",
        "#     '''\n",
        "#       We want to save the information on total_norm, clip_coef and clip_coef_clamped for tracking purpose\n",
        "#     '''\n",
        "#     samples_grads[i]['total_norm'] = total_norm\n",
        "#     samples_grads[i]['clip_coef'] = clip_coef\n",
        "#     samples_grads[i]['clip_coef_clamped'] = clip_coef_clamped\n",
        "\n",
        "# for sample_grads in samples_grads.values():\n",
        "#     print(f\"sample_grads: {sample_grads}\")\n",
        "\n",
        "# #Aggregate clipped grads\n",
        "\n",
        "# aggregated_grad_dict = defaultdict(list)\n",
        "\n",
        "# for sample in samples_grads.values(): # you can list as many input dicts as you want here\n",
        "#     for key, value in sample.items():\n",
        "#         aggregated_grad_dict[key].append(value)\n",
        "\n",
        "# mean = 0\n",
        "# constC = 0.1\n",
        "# sigma = 10\n",
        "# std = sigma*constC\n",
        "# batch_size = num_samples\n",
        "# for key, list_grad in aggregated_grad_dict.items():\n",
        "#     #print(f\"key: {key}, list_grad: {list_grad}\")\n",
        "#     aggregated_grad_dict[key] = np.sum(list_grad)\n",
        "#     #add noise  N(0,(C\\sigma)^2I)\n",
        "#     noise = torch.normal(mean=mean, std=std, size=aggregated_grad_dict[key].shape)\n",
        "#     print(f\"noise: {noise}, shape: {aggregated_grad_dict[key].shape}\")\n",
        "#     #noisy gradient and normalize aggregate_grad_dict\n",
        "#     aggregated_grad_dict[key] = (aggregated_grad_dict[key] + noise)/batch_size\n",
        "#     print(f\"normalized noisy grad: {aggregated_grad_dict[key]}\")\n",
        "#     #update the model's grads\n",
        "\n",
        "# for k, param in linear.named_parameters():\n",
        "#    print(f\"before weight: key: {k}, weight: {param.data}\")\n",
        "#    print(f\"before param.grad: {param.grad}\")\n",
        "#    param.grad =  aggregated_grad_dict[k]\n",
        "#    print(f\"after param.grad: {param.grad}\")\n",
        "\n",
        "# optimizer.step()\n",
        "\n",
        "\n",
        "# for k, param in linear.named_parameters():\n",
        "#    print(f\"after weight: key: {k}, weight: {param.data}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zX8jewzoq0AO",
        "outputId": "c61d217b-db73-4cf7-d382-352b53a3a90b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x:  tensor([[2.],\n",
            "        [3.]])\n",
            "y:  tensor([[5.],\n",
            "        [7.]])\n",
            "w:  Parameter containing:\n",
            "tensor([[3.]], requires_grad=True)\n",
            "b:  Parameter containing:\n",
            "tensor([5.], requires_grad=True)\n",
            "key:  weight  value:  tensor([[[24.]],\n",
            "\n",
            "        [[42.]]])\n",
            "key:  bias  value:  tensor([[12.],\n",
            "        [14.]])\n",
            "k: weight, v.data: tensor([[3.]]), v.grad: None\n",
            "k: bias, v.data: tensor([5.]), v.grad: None\n",
            "key1:  weight  value1:  tensor([[[24.]],\n",
            "\n",
            "        [[42.]]])\n",
            "key1:  bias  value1:  tensor([[12.],\n",
            "        [14.]])\n",
            "k: weight, v.data: tensor([[3.]]), v.grad: None\n",
            "k: bias, v.data: tensor([5.]), v.grad: None\n"
          ]
        }
      ],
      "source": [
        "#https://pytorch.org/tutorials/intermediate/per_sample_grads.html?utm_source=whats_new_tutorials&utm_medium=per_sample_grads\n",
        "#https://pytorch.org/docs/stable/generated/torch.func.grad.html\n",
        "#https://towardsdatascience.com/introduction-to-functional-pytorch-b5bf739e1e6e\n",
        "#https://pytorch.org/functorch/stable/notebooks/per_sample_grads.html [PAY ATTENTION TO]\n",
        "\n",
        "\n",
        "'''\n",
        "We define the weight of a network\n",
        "We want to define the input\n",
        "We want to compute the derive for each data point\n",
        "We want to compute the derive for many data points\n",
        "We generate noise\n",
        "We clip the gradient\n",
        "'''\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torchvision.transforms as transforms\n",
        "from collections import OrderedDict\n",
        "\n",
        "'''\n",
        "0. data true generation: y = 2x + 1\n",
        "1. y = wx + b // w -- weight and b -- bias\n",
        "2. (x=2/3/(2,3), y*= 5/7/(5,7))\n",
        "3. w = 3, b = 5\n",
        "4. L = (y*-y)^2\n",
        "5. dL/dw = dL/dy*dy/dw = 2(y*-y)*dy/dw = 2(y*-y)*x // 24/42/(24+42)/2=33\n",
        "6. dL/db = dL/dy*dy/db = 2(y*-y)*dy/db = 2(y*-y)*1 // 12/14/(12+14)/2=13\n",
        "'''\n",
        "x = torch.tensor([2,3],dtype=torch.float32).view(-1, 1)\n",
        "y = torch.tensor([5,7],dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "print(\"x: \", x)\n",
        "print(\"y: \", y)\n",
        "\n",
        "# Build a fully connected layer.\n",
        "linear = nn.Linear(1, 1)\n",
        "optimizer = torch.optim.SGD(linear.parameters(), lr=0.01)\n",
        "\n",
        "# params = dict(linear.named_parameters())\n",
        "# for key in params.keys():\n",
        "#   print(\"key: \", key, \" , value: \", params[key].data, \" , value object: \", params[key])\n",
        "\n",
        "state3 = OrderedDict()\n",
        "state3['weight'] = torch.tensor([3],dtype=torch.float32).view(-1, 1)\n",
        "state3['bias'] = torch.tensor([5],dtype=torch.float32)\n",
        "linear.load_state_dict(state3)\n",
        "\n",
        "print ('w: ', linear.weight)\n",
        "print ('b: ', linear.bias)\n",
        "\n",
        "# Build loss function and optimizer.\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "\n",
        "from torch.func import functional_call, vmap, grad\n",
        "\n",
        "def compute_loss(params, buffers, model, loss_fn, sample, target):\n",
        "    batch = sample.unsqueeze(0)\n",
        "    targets = target.unsqueeze(0)\n",
        "\n",
        "    predictions = functional_call(model, (params, buffers), (batch,))\n",
        "    loss = loss_fn(predictions, targets)\n",
        "    return loss\n",
        "\n",
        "ft_compute_grad = grad(compute_loss)\n",
        "ft_compute_sample_grad = vmap(ft_compute_grad, in_dims=(None, None, None, None, 0, 0))\n",
        "\n",
        "'''\n",
        "The ft_compute_grad function computes the gradient for a single (sample, target) pair.\n",
        "# We can use vmap to get it to compute the gradient over an entire batch of samples and targets.\n",
        "# Note that in_dims=(None, None, 0, 0) because we wish to\n",
        "# map ft_compute_grad over the 0th dimension of the data and targets, and use the same params and buffers for each.\n",
        "# '''\n",
        "\n",
        "params = {k: v.detach() for k, v in linear.named_parameters()}\n",
        "buffers = {k: v.detach() for k, v in linear.named_buffers()}\n",
        "\n",
        "ft_per_sample_grads = ft_compute_sample_grad(params, buffers, linear, criterion, x, y)\n",
        "\n",
        "for key in ft_per_sample_grads.keys():\n",
        "    print(\"key: \", key, \" value: \", ft_per_sample_grads[key])\n",
        "\n",
        "\n",
        "for k, v in linear.named_parameters():\n",
        "  print(f\"k: {k}, v.data: {v.data}, v.grad: {v.grad}\")\n",
        "\n",
        "\n",
        "#We want to show that the grad does not change linear.param.grad value !!!!!! and even when we run the second time the model parameter does not change.\n",
        "\n",
        "ft_per_sample_grads1 = ft_compute_sample_grad(params, buffers, linear, criterion, x, y)\n",
        "\n",
        "for key in ft_per_sample_grads1.keys():\n",
        "    print(\"key1: \", key, \" value1: \", ft_per_sample_grads1[key])\n",
        "\n",
        "for k, v in linear.named_parameters():\n",
        "  print(f\"k: {k}, v.data: {v.data}, v.grad: {v.grad}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7trVT9m0aott",
        "outputId": "40130326-5686-4f36-8df7-e2d907f888f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x:  tensor([[2.],\n",
            "        [3.]])\n",
            "y:  tensor([[5.],\n",
            "        [7.]])\n",
            "w:  Parameter containing:\n",
            "tensor([[3.]], requires_grad=True)\n",
            "b:  Parameter containing:\n",
            "tensor([5.], requires_grad=True)\n",
            "key: weight, value: tensor([[[24.]],\n",
            "\n",
            "        [[42.]]])\n",
            "unstacked:  (tensor([[24.]]), tensor([[42.]]))\n",
            "key: bias, value: tensor([[12.],\n",
            "        [14.]])\n",
            "unstacked:  (tensor([12.]), tensor([14.]))\n",
            "sample_grads: OrderedDict([('whole_grad', [tensor([[0.8944]]), tensor([0.4472])]), ('weight', tensor([[0.8944]])), ('bias', tensor([0.4472])), ('total_norm', tensor(26.8328)), ('clip_coef', tensor(0.0373)), ('clip_coef_clamped', tensor(0.0373))])\n",
            "sample_grads: OrderedDict([('whole_grad', [tensor([[0.9487]]), tensor([0.3162])]), ('weight', tensor([[0.9487]])), ('bias', tensor([0.3162])), ('total_norm', tensor(44.2719)), ('clip_coef', tensor(0.0226)), ('clip_coef_clamped', tensor(0.0226))])\n",
            "noise: tensor([[0.9151]]), shape: torch.Size([1, 1])\n",
            "normalized noisy grad: tensor([[1.7608]])\n",
            "noise: tensor([[-0.4997]]), shape: torch.Size([1, 1])\n",
            "normalized noisy grad: tensor([[0.6717]])\n",
            "noise: tensor([-0.3201]), shape: torch.Size([1])\n",
            "normalized noisy grad: tensor([0.2217])\n",
            "noise: 0.9763436317443848, shape: ()\n",
            "normalized noisy grad: 36.040523529052734\n",
            "noise: -0.21512079238891602, shape: ()\n",
            "normalized noisy grad: -0.07763265073299408\n",
            "noise: -0.8676226139068604, shape: ()\n",
            "normalized noisy grad: -0.40388357639312744\n",
            "before weight: key: weight, weight: tensor([[3.]])\n",
            "before param.grad: None\n",
            "after param.grad: tensor([[0.6717]])\n",
            "before weight: key: bias, weight: tensor([5.])\n",
            "before param.grad: None\n",
            "after param.grad: tensor([0.2217])\n",
            "after weight: key: weight, weight: tensor([[2.9933]])\n",
            "after weight: key: bias, weight: tensor([4.9978])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:86: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
          ]
        }
      ],
      "source": [
        "#https://pytorch.org/tutorials/intermediate/per_sample_grads.html?utm_source=whats_new_tutorials&utm_medium=per_sample_grads\n",
        "#https://pytorch.org/docs/stable/generated/torch.func.grad.html\n",
        "#https://towardsdatascience.com/introduction-to-functional-pytorch-b5bf739e1e6e\n",
        "#https://pytorch.org/functorch/stable/notebooks/per_sample_grads.html [PAY ATTENTION TO]\n",
        "\n",
        "\n",
        "'''\n",
        "We define the weight of a network\n",
        "We want to define the input\n",
        "We want to compute the derive for each data point\n",
        "We want to compute the derive for many data points\n",
        "We generate noise\n",
        "We clip the gradient\n",
        "'''\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torchvision.transforms as transforms\n",
        "from collections import OrderedDict\n",
        "from collections import defaultdict\n",
        "\n",
        "'''\n",
        "0. data true generation: y = 2x + 1\n",
        "1. y = wx + b // w -- weight and b -- bias\n",
        "2. (x=2/3/(2,3), y*= 5/7/(5,7))\n",
        "3. w = 3, b = 5\n",
        "4. L = (y*-y)^2\n",
        "5. dL/dw = dL/dy*dy/dw = 2(y*-y)*dy/dw = 2(y*-y)*x // 24/42/(24+42)/2=33\n",
        "6. dL/db = dL/dy*dy/db = 2(y*-y)*dy/db = 2(y*-y)*1 // 12/14/(12+14)/2=13\n",
        "'''\n",
        "x = torch.tensor([2,3],dtype=torch.float32).view(-1, 1)\n",
        "y = torch.tensor([5,7],dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "print(\"x: \", x)\n",
        "print(\"y: \", y)\n",
        "\n",
        "# Build a fully connected layer.\n",
        "linear = nn.Linear(1, 1)\n",
        "optimizer = torch.optim.SGD(linear.parameters(), lr=0.01)\n",
        "\n",
        "# params = dict(linear.named_parameters())\n",
        "# for key in params.keys():\n",
        "#   print(\"key: \", key, \" , value: \", params[key].data, \" , value object: \", params[key])\n",
        "\n",
        "state3 = OrderedDict()\n",
        "state3['weight'] = torch.tensor([3],dtype=torch.float32).view(-1, 1)\n",
        "state3['bias'] = torch.tensor([5],dtype=torch.float32)\n",
        "linear.load_state_dict(state3)\n",
        "\n",
        "print ('w: ', linear.weight)\n",
        "print ('b: ', linear.bias)\n",
        "\n",
        "# Build loss function and optimizer.\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "\n",
        "from torch.func import functional_call, vmap, grad\n",
        "\n",
        "def compute_loss(params, buffers, model, loss_fn, sample, target):\n",
        "    batch = sample.unsqueeze(0)\n",
        "    targets = target.unsqueeze(0)\n",
        "\n",
        "    predictions = functional_call(model, (params, buffers), (batch,))\n",
        "    loss = loss_fn(predictions, targets)\n",
        "    return loss\n",
        "\n",
        "ft_compute_grad = grad(compute_loss)\n",
        "ft_compute_sample_grad = vmap(ft_compute_grad, in_dims=(None, None, None, None, 0, 0))\n",
        "\n",
        "'''\n",
        "The ft_compute_grad function computes the gradient for a single (sample, target) pair.\n",
        "# We can use vmap to get it to compute the gradient over an entire batch of samples and targets.\n",
        "# Note that in_dims=(None, None, 0, 0) because we wish to\n",
        "# map ft_compute_grad over the 0th dimension of the data and targets, and use the same params and buffers for each.\n",
        "# '''\n",
        "\n",
        "params = {k: v.detach() for k, v in linear.named_parameters()}\n",
        "buffers = {k: v.detach() for k, v in linear.named_buffers()}\n",
        "\n",
        "ft_per_sample_grads = ft_compute_sample_grad(params, buffers, linear, criterion, x, y)\n",
        "\n",
        "# for key in ft_per_sample_grads.keys():\n",
        "#     print(f\"key: {key}, value: {ft_per_sample_grads[key]}\")\n",
        "\n",
        "\n",
        "# for k, v in linear.named_parameters():\n",
        "#   print(f\"k: {k}, v.data: {v.data}, v.grad: {v.grad}\")\n",
        "\n",
        "for key,value in ft_per_sample_grads.items():\n",
        "    print(f\"key: {key}, value: {value}\")\n",
        "    unstacked = torch.unbind(value, dim=0)\n",
        "    print(\"unstacked: \", unstacked)\n",
        "\n",
        "\n",
        "#get back per_sample_grad\n",
        "num_samples = len(x)\n",
        "samples_grads = dict()\n",
        "\n",
        "for i in range(num_samples):\n",
        "  samples_grads[i] = OrderedDict()\n",
        "  samples_grads[i]['whole_grad'] = list()\n",
        "\n",
        "'''\n",
        "  1. Going through each layer in ft_per_sample_grads: key, value in ft_per_sample_grads.items()\n",
        "  2. unstack the stacked of len(x) layers: unstacked_grads = torch.unbind(value, dim=0)\n",
        "  3. redistribute the unstacked sample_layer_grad, i.e., samples_grads[i][key]\n",
        "  4. We create a new feature called \"whole_grad\" to combine all layer grads as a whole grad tensor. This is used for computing the full grad norm.\n",
        "     This full grad norm is used to compute the clipped sample grad later !!!!\n",
        "\n",
        "  Each sample has its own grad now but saved in the form of dictionary\n",
        "'''\n",
        "\n",
        "for key,value in ft_per_sample_grads.items():\n",
        "    #unstack the grads for each layer\n",
        "    unstacked_grads = torch.unbind(value, dim=0)\n",
        "    i = 0\n",
        "    for layer_grad in unstacked_grads:\n",
        "       samples_grads[i]['whole_grad'].append(layer_grad)\n",
        "       samples_grads[i][key] = layer_grad\n",
        "       i += 1\n",
        "\n",
        "#clipping the per_sample_grad\n",
        "for i in range(num_samples):\n",
        "    norm_type = 2.0\n",
        "    max_norm = 1.0 #This is clipping constant C\n",
        "\n",
        "    total_norm = torch.norm(torch.stack([torch.norm(grad.detach(), norm_type) for grad in samples_grads[i]['whole_grad']]), norm_type)\n",
        "    clip_coef = max_norm / (total_norm + 1e-6)\n",
        "    #https://www.tutorialspoint.com/python-pytorch-clamp-method\n",
        "    #clamp(tensor,min,max)\n",
        "    '''\n",
        "      It is interesting to see that the samples_grads[i]['whole_grad'] contains the pointer to each tensor rather than the value.\n",
        "      This is why if we clipping the grad in samples_grads[i]['whole_grad'], all samples_grads[i][layer_grad] is clipped at the same time.\n",
        "    '''\n",
        "    clip_coef_clamped = torch.clamp(clip_coef, max=1.0)\n",
        "    for grad in samples_grads[i]['whole_grad']:\n",
        "        grad.detach().mul_(clip_coef_clamped)\n",
        "\n",
        "    '''\n",
        "      We want to save the information on total_norm, clip_coef and clip_coef_clamped for tracking purpose\n",
        "    '''\n",
        "    samples_grads[i]['total_norm'] = total_norm\n",
        "    samples_grads[i]['clip_coef'] = clip_coef\n",
        "    samples_grads[i]['clip_coef_clamped'] = clip_coef_clamped\n",
        "\n",
        "for sample_grads in samples_grads.values():\n",
        "    print(f\"sample_grads: {sample_grads}\")\n",
        "\n",
        "#Aggregate clipped grads\n",
        "\n",
        "aggregated_grad_dict = defaultdict(list)\n",
        "\n",
        "for sample in samples_grads.values(): # you can list as many input dicts as you want here\n",
        "    for key, value in sample.items():\n",
        "        aggregated_grad_dict[key].append(value)\n",
        "\n",
        "mean = 0\n",
        "constC = 0.1\n",
        "sigma = 10\n",
        "std = sigma*constC\n",
        "batch_size = num_samples\n",
        "for key, list_grad in aggregated_grad_dict.items():\n",
        "    #print(f\"key: {key}, list_grad: {list_grad}\")\n",
        "    aggregated_grad_dict[key] = np.sum(list_grad)\n",
        "    #add noise  N(0,(C\\sigma)^2I)\n",
        "    noise = torch.normal(mean=mean, std=std, size=aggregated_grad_dict[key].shape)\n",
        "    print(f\"noise: {noise}, shape: {aggregated_grad_dict[key].shape}\")\n",
        "    #noisy gradient and normalize aggregate_grad_dict\n",
        "    aggregated_grad_dict[key] = (aggregated_grad_dict[key] + noise)/batch_size\n",
        "    print(f\"normalized noisy grad: {aggregated_grad_dict[key]}\")\n",
        "    #update the model's grads\n",
        "\n",
        "for k, param in linear.named_parameters():\n",
        "   print(f\"before weight: key: {k}, weight: {param.data}\")\n",
        "   print(f\"before param.grad: {param.grad}\")\n",
        "   param.grad =  aggregated_grad_dict[k]\n",
        "   print(f\"after param.grad: {param.grad}\")\n",
        "\n",
        "optimizer.step()\n",
        "\n",
        "\n",
        "for k, param in linear.named_parameters():\n",
        "   print(f\"after weight: key: {k}, weight: {param.data}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HosmHizprccs",
        "outputId": "83a6de5a-be19-4bc6-f6fc-65a9ae5a84c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x:  tensor([[2.],\n",
            "        [3.]])\n",
            "y:  tensor([[5.],\n",
            "        [7.]])\n",
            "w:  Parameter containing:\n",
            "tensor([[3.]], requires_grad=True)\n",
            "b:  Parameter containing:\n",
            "tensor([5.], requires_grad=True)\n",
            "before weight: key: weight, weight: tensor([[3.]])\n",
            "before weight: key: bias, weight: tensor([5.])\n",
            "after weight: key: weight, weight: tensor([[3.0022]])\n",
            "after weight: key: bias, weight: tensor([5.0205])\n"
          ]
        }
      ],
      "source": [
        "#https://pytorch.org/tutorials/intermediate/per_sample_grads.html?utm_source=whats_new_tutorials&utm_medium=per_sample_grads\n",
        "#https://pytorch.org/docs/stable/generated/torch.func.grad.html\n",
        "#https://towardsdatascience.com/introduction-to-functional-pytorch-b5bf739e1e6e\n",
        "#https://pytorch.org/functorch/stable/notebooks/per_sample_grads.html [PAY ATTENTION TO]\n",
        "\n",
        "\n",
        "'''\n",
        "We define the weight of a network\n",
        "We want to define the input\n",
        "We want to compute the derive for each data point\n",
        "We want to compute the derive for many data points\n",
        "We generate noise\n",
        "We clip the gradient\n",
        "'''\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torchvision.transforms as transforms\n",
        "from collections import OrderedDict\n",
        "from collections import defaultdict\n",
        "from torch.func import functional_call, vmap, grad\n",
        "\n",
        "def compute_loss(params, buffers, model, loss_fn, sample, target):\n",
        "    batch = sample.unsqueeze(0)\n",
        "    targets = target.unsqueeze(0)\n",
        "\n",
        "    predictions = functional_call(model, (params, buffers), (batch,))\n",
        "    loss = loss_fn(predictions, targets)\n",
        "    return loss\n",
        "\n",
        "def compute_gradients(model,loss_fn,samples,targets):\n",
        "    '''\n",
        "        We want to follow the tutorial in here to compute multiple grads in parallel:\n",
        "                #https://pytorch.org/tutorials/intermediate/per_sample_grads.html?utm_source=whats_new_tutorials&utm_medium=per_sample_grads\n",
        "                #https://pytorch.org/docs/stable/generated/torch.func.grad.html\n",
        "                #https://towardsdatascience.com/introduction-to-functional-pytorch-b5bf739e1e6e\n",
        "                #https://pytorch.org/functorch/stable/notebooks/per_sample_grads.html [PAY ATTENTION TO]\n",
        "            Typically, we generate all gradients gis of samples sis in parallel in the helper function: compute_gradients\n",
        "            The output of compute_gradients is an array called samples_grads\n",
        "                sample s[0]: samples_grads[0][layer_1], samples_grads[0][layer_2], .... //g0\n",
        "                    ...............\n",
        "                sample s[L-1]: samples_grads[L-1][layer_1], samples_grads[L-1][layer_2], ....//g[L-1]\n",
        "                where L is the number of samples in the mini-batch\n",
        "\n",
        "            The compute_gradients call another helper function called compute_loss. This is used for computing the gradients in parallel\n",
        "    '''\n",
        "\n",
        "    ft_compute_grad = grad(compute_loss)\n",
        "    ft_compute_sample_grad = vmap(ft_compute_grad, in_dims=(None, None, None, None, 0, 0))\n",
        "\n",
        "    '''\n",
        "    The ft_compute_grad function computes the gradient for a single (sample, target) pair.\n",
        "    We can use vmap to get it to compute the gradient over an entire batch of samples and targets.\n",
        "    Note that in_dims=(None, None, 0, 0) because we wish to\n",
        "    map ft_compute_grad over the 0th dimension of the data and targets, and use the same params and buffers for each.\n",
        "    '''\n",
        "\n",
        "    params = {k: v.detach() for k, v in model.named_parameters()}\n",
        "    buffers = {k: v.detach() for k, v in model.named_buffers()}\n",
        "\n",
        "    ft_per_sample_grads = ft_compute_sample_grad(params, buffers,model, loss_fn,samples,targets)\n",
        "\n",
        "    '''\n",
        "    ft_per_sample_grads contains the STACKED gradients per layer.\n",
        "    For example, we have two samples s0 and s1 and we have only two layers \"bias\" and \"weight\"\n",
        "        s0 = (\"weight\": 1, \"layer\": 2)\n",
        "        s1 = (\"weight\": 3, \"layer\": 4)\n",
        "    Stacked gradients per layer means  = (\"weight\": [1,3], \"bias\":[2,4])\n",
        "    Therefore, we have to unstack this stacked gradients to get back the gradient for each sample\n",
        "    '''\n",
        "\n",
        "    #get back per_sample_grad\n",
        "    num_samples = len(samples)\n",
        "    samples_grads = dict()\n",
        "\n",
        "    for i in range(num_samples):\n",
        "      samples_grads[i] = OrderedDict()\n",
        "      samples_grads[i]['whole_grad'] = list()\n",
        "\n",
        "    '''\n",
        "    1. Going through each layer in ft_per_sample_grads: key, value in ft_per_sample_grads.items()\n",
        "    2. unstack the stacked of len(x) layers: unstacked_grads = torch.unbind(value, dim=0)\n",
        "    3. redistribute the unstacked sample_layer_grad, i.e., samples_grads[i][key]\n",
        "    4. We create a new feature called \"whole_grad\" to combine all layer grads as a whole grad tensor. This is used for computing the full grad norm.\n",
        "        This full grad norm is used to compute the clipped sample grad later !!!!\n",
        "\n",
        "    Each sample has its own grad now but saved in the form of dictionary\n",
        "    '''\n",
        "\n",
        "    for key,value in ft_per_sample_grads.items():\n",
        "        #unstack the grads for each layer\n",
        "        unstacked_grads = torch.unbind(value, dim=0)\n",
        "        i = 0\n",
        "        for layer_grad in unstacked_grads:\n",
        "            samples_grads[i]['whole_grad'].append(layer_grad)\n",
        "            samples_grads[i][key] = layer_grad\n",
        "            i += 1\n",
        "\n",
        "\n",
        "    return samples_grads\n",
        "\n",
        "\n",
        "def generate_private_grad(model,loss_fn,samples,targets,sigma,constC):\n",
        "    '''\n",
        "        We generate private grad given a batch of samples (samples,targets) as introduced here https://arxiv.org/pdf/1607.00133.pdf\n",
        "        The implementation flow is as follows:\n",
        "            1. sample xi\n",
        "            2. ===> gradient gi\n",
        "            3. ===> clipped gradient gci\n",
        "            4. ===> noisy aggregated (sum gci + noise)\n",
        "            5. ===> normalized 1/B (sum gci + noise)\n",
        "\n",
        "        We want to follow the tutorial in here to compute multiple grads in parallel:\n",
        "            #https://pytorch.org/tutorials/intermediate/per_sample_grads.html?utm_source=whats_new_tutorials&utm_medium=per_sample_grads\n",
        "            #https://pytorch.org/docs/stable/generated/torch.func.grad.html\n",
        "            #https://towardsdatascience.com/introduction-to-functional-pytorch-b5bf739e1e6e\n",
        "            #https://pytorch.org/functorch/stable/notebooks/per_sample_grads.html [PAY ATTENTION TO]\n",
        "        Typically, we generate all gradients gis of samples sis in parallel in the helper function: compute_gradients\n",
        "        The output of compute_gradients is an array called samples_grads\n",
        "            sample s[0]: samples_grads[0][layer_1], samples_grads[0][layer_2], .... //g0\n",
        "                ...............\n",
        "            sample s[L-1]: samples_grads[L-1][layer_1], samples_grads[L-1][layer_2], ....//g[L-1]\n",
        "            where L is the number of samples in the mini-batch\n",
        "\n",
        "        The compute_gradients call another helper function called compute_loss. This is used for computing the gradients in parallel\n",
        "\n",
        "        After that we compute the clipped gradients gci for each gi. In this case we use the following approach proposed here\n",
        "            #https://www.tutorialspoint.com/python-pytorch-clamp-method\n",
        "        To do it, we need to create a new field called whole_grad which containing all gradients of layers for a given sample si\n",
        "        whole_grad allows us to compute the total_norm of sample si and then we can do the clipping\n",
        "\n",
        "        After computing all clipped gradients, we need to aggregate all the clipped gradient per layer. This step helps us\n",
        "        to compute the sum (clipped gradient gi) and then we add noise to each entry in the sum (clipped gradient gi)\n",
        "\n",
        "        Finally, we normalize the private gradient and update the model.grad. This step allows optimizer update the model\n",
        "    '''\n",
        "\n",
        "    samples_grads = compute_gradients(model,loss_fn,samples,targets)\n",
        "\n",
        "    num_samples = len(samples)\n",
        "\n",
        "    #clipping the per_sample_grad\n",
        "    for i in range(num_samples):\n",
        "        norm_type = 2.0\n",
        "        max_norm = constC #This is clipping constant C\n",
        "\n",
        "        total_norm = torch.norm(torch.stack([torch.norm(grad.detach(), norm_type) for grad in samples_grads[i]['whole_grad']]), norm_type)\n",
        "        clip_coef = max_norm / (total_norm + 1e-6)\n",
        "        #https://www.tutorialspoint.com/python-pytorch-clamp-method\n",
        "        #clamp(tensor,min,max)\n",
        "        '''\n",
        "            It is interesting to see that the samples_grads[i]['whole_grad'] contains the pointer to each tensor rather than the value.\n",
        "            This is why if we clipping the grad in samples_grads[i]['whole_grad'], all samples_grads[i][layer_grad] is clipped at the same time.\n",
        "        '''\n",
        "        clip_coef_clamped = torch.clamp(clip_coef, max=1.0)\n",
        "        for grad in samples_grads[i]['whole_grad']:\n",
        "            grad.detach().mul_(clip_coef_clamped)\n",
        "\n",
        "    #Aggregate clipped grads\n",
        "    '''\n",
        "        aggregated_grad_dict looks like as follows if we have two samples s0 and s1 as described above.\n",
        "            aggreated_grad_dict[key=weight]= {1, 3}\n",
        "            aggreated_grad_dict[key=bias]= {2, 4}\n",
        "        To get it, we have to loop through all samples and for each sample, we loop through each layer (key) to get it grad (value)\n",
        "    '''\n",
        "\n",
        "    aggregated_grad_dict = defaultdict(list)\n",
        "\n",
        "    for sample in samples_grads.values():\n",
        "        for layer, grad in sample.items():\n",
        "            aggregated_grad_dict[layer].append(grad)\n",
        "\n",
        "    #generate private grad per layer\n",
        "    mean = 0\n",
        "    std = sigma*constC\n",
        "    batch_size = num_samples\n",
        "    for layer, list_grad in aggregated_grad_dict.items():\n",
        "        #compute the sum of clipped gradients gi\n",
        "        aggregated_grad_dict[layer] = np.sum(list_grad)\n",
        "        #generate the noise ~ N(0,(C\\sigma)^2I)\n",
        "        noise = torch.normal(mean=mean, std=std, size=aggregated_grad_dict[layer].shape)\n",
        "        #generate private gradient per layer\n",
        "        aggregated_grad_dict[layer] = (aggregated_grad_dict[layer] + noise)/batch_size\n",
        "\n",
        "    #update the model's grads\n",
        "    '''\n",
        "        Because we do not use loss_fn.backward() function to generate model.grad, model.grad is NONE\n",
        "        We need to update the model.grad to make sure that optim.step() can operate normally\n",
        "    '''\n",
        "\n",
        "    for layer, param in model.named_parameters():\n",
        "        param.grad =  aggregated_grad_dict[layer]\n",
        "\n",
        "    return 0\n",
        "\n",
        "\n",
        "'''\n",
        "0. data true generation: y = 2x + 1\n",
        "1. y = wx + b // w -- weight and b -- bias\n",
        "2. (x=2/3/(2,3), y*= 5/7/(5,7))\n",
        "3. w = 3, b = 5\n",
        "4. L = (y*-y)^2\n",
        "5. dL/dw = dL/dy*dy/dw = 2(y*-y)*dy/dw = 2(y*-y)*x // 24/42/(24+42)/2=33\n",
        "6. dL/db = dL/dy*dy/db = 2(y*-y)*dy/db = 2(y*-y)*1 // 12/14/(12+14)/2=13\n",
        "'''\n",
        "x = torch.tensor([2,3],dtype=torch.float32).view(-1, 1)\n",
        "y = torch.tensor([5,7],dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "print(\"x: \", x)\n",
        "print(\"y: \", y)\n",
        "\n",
        "# Build a fully connected layer.\n",
        "linear = nn.Linear(1, 1)\n",
        "optimizer = torch.optim.SGD(linear.parameters(), lr=0.01)\n",
        "\n",
        "state3 = OrderedDict()\n",
        "state3['weight'] = torch.tensor([3],dtype=torch.float32).view(-1, 1)\n",
        "state3['bias'] = torch.tensor([5],dtype=torch.float32)\n",
        "linear.load_state_dict(state3)\n",
        "\n",
        "print ('w: ', linear.weight)\n",
        "print ('b: ', linear.bias)\n",
        "\n",
        "# Build loss function and optimizer.\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "model = linear\n",
        "loss_fn = criterion\n",
        "samples = x\n",
        "targets = y\n",
        "sigma = 10\n",
        "constC = 1\n",
        "generate_private_grad(model,loss_fn,samples,targets,sigma,constC)\n",
        "\n",
        "\n",
        "for k, param in linear.named_parameters():\n",
        "   print(f\"before weight: key: {k}, weight: {param.data}\")\n",
        "\n",
        "optimizer.step()\n",
        "\n",
        "for k, param in linear.named_parameters():\n",
        "   print(f\"after weight: key: {k}, weight: {param.data}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "nkmza-pMGFeN",
        "outputId": "ab5f5302-cc04-4503-94a3-ffe81c7c9fe3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x:  tensor([[2.],\n",
            "        [3.]])\n",
            "y:  tensor([[5.],\n",
            "        [7.]])\n",
            "w:  Parameter containing:\n",
            "tensor([[3.]], requires_grad=True)\n",
            "b:  Parameter containing:\n",
            "tensor([5.], requires_grad=True)\n",
            "type type <class 'tuple'>\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-0e543765153c>\u001b[0m in \u001b[0;36m<cell line: 74>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;31m#   print(f\"per grad: {pre_sample.value()}, {pre_sample.key()} \")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_sample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mft_per_sample_grads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m   \u001b[0;31m#print(\"per grad: \", pre_sample, \" , \", type(pre_sample))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"per grad: {pre_sample}, {key} \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'items'"
          ]
        }
      ],
      "source": [
        "#https://pytorch.org/functorch/stable/notebooks/per_sample_grads.html [PAY ATTENTION TO]\n",
        "[THIS IS AN OUTDATED TECHNIQUE]\n",
        "\n",
        "'''\n",
        "We define the weight of a network\n",
        "We want to define the input\n",
        "We want to compute the derive for each data point\n",
        "We want to compute the derive for many data points\n",
        "We generate noise\n",
        "We clip the gradient\n",
        "'''\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torchvision.transforms as transforms\n",
        "from collections import OrderedDict\n",
        "\n",
        "'''\n",
        "0. data true generation: y = 2x + 1\n",
        "1. y = wx + b // w -- weight and b -- bias\n",
        "2. (x=2/3/(2,3), y*= 5/7/(5,7))\n",
        "3. w = 3, b = 5\n",
        "4. L = (y*-y)^2\n",
        "5. dL/dw = dL/dy*dy/dw = 2(y*-y)*dy/dw = 2(y*-y)*x // 24/42/(24+42)/2=33\n",
        "6. dL/db = dL/dy*dy/db = 2(y*-y)*dy/db = 2(y*-y)*1 // 12/14/(12+14)/2=13\n",
        "'''\n",
        "x = torch.tensor([2,3],dtype=torch.float32).view(-1, 1)\n",
        "y = torch.tensor([5,7],dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "print(\"x: \", x)\n",
        "print(\"y: \", y)\n",
        "\n",
        "# Build a fully connected layer.\n",
        "linear = nn.Linear(1, 1)\n",
        "optimizer = torch.optim.SGD(linear.parameters(), lr=0.01)\n",
        "\n",
        "# params = dict(linear.named_parameters())\n",
        "# for key in params.keys():\n",
        "#   print(\"key: \", key, \" , value: \", params[key].data, \" , value object: \", params[key])\n",
        "\n",
        "state3 = OrderedDict()\n",
        "state3['weight'] = torch.tensor([3],dtype=torch.float32).view(-1, 1)\n",
        "state3['bias'] = torch.tensor([5],dtype=torch.float32)\n",
        "linear.load_state_dict(state3)\n",
        "\n",
        "print ('w: ', linear.weight)\n",
        "print ('b: ', linear.bias)\n",
        "\n",
        "# Build loss function and optimizer.\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "from functorch import make_functional_with_buffers, vmap, grad\n",
        "\n",
        "fmodel, params, buffers = make_functional_with_buffers(linear)\n",
        "\n",
        "def compute_loss(params, buffers, fmodel, loss_fn, sample, target):\n",
        "    batch = sample.unsqueeze(0)\n",
        "    targets = target.unsqueeze(0)\n",
        "    predictions = fmodel(params, buffers, batch)\n",
        "    loss = loss_fn(predictions, targets)\n",
        "    return loss\n",
        "\n",
        "ft_compute_grad = grad(compute_loss)\n",
        "ft_compute_sample_grad = vmap(ft_compute_grad, in_dims=(None, None, None, None, 0, 0))\n",
        "ft_per_sample_grads = ft_compute_sample_grad(params, buffers, fmodel, criterion, x, y)\n",
        "\n",
        "print(f\"type type {type(ft_per_sample_grads)}\")\n",
        "\n",
        "# for pre_sample in ft_per_sample_grads:\n",
        "#   #print(\"per grad: \", pre_sample, \" , \", type(pre_sample))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOcoU5bRx-lP",
        "outputId": "08d482c0-f121-413c-eb4d-5e849a68d2bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x:  tensor([[2.],\n",
            "        [3.]])\n",
            "y:  tensor([[5.],\n",
            "        [7.]])\n",
            "w:  Parameter containing:\n",
            "tensor([[3.]], requires_grad=True)\n",
            "b:  Parameter containing:\n",
            "tensor([5.], requires_grad=True)\n",
            "grads:  {'weight': BatchedTensor(lvl=1, bdim=0, value=\n",
            "    tensor([[[24.]],\n",
            "\n",
            "            [[42.]]])\n",
            "), 'bias': BatchedTensor(lvl=1, bdim=0, value=\n",
            "    tensor([[12.],\n",
            "            [14.]])\n",
            ")}\n",
            "grads:  {'weight': BatchedTensor(lvl=1, bdim=0, value=\n",
            "    tensor([[[24.]],\n",
            "\n",
            "            [[42.]]])\n",
            "), 'bias': BatchedTensor(lvl=1, bdim=0, value=\n",
            "    tensor([[12.],\n",
            "            [14.]])\n",
            ")}\n",
            "grad:  weight\n",
            "grads:  {'weight': BatchedTensor(lvl=1, bdim=0, value=\n",
            "    tensor([[[24.]],\n",
            "\n",
            "            [[42.]]])\n",
            "), 'bias': BatchedTensor(lvl=1, bdim=0, value=\n",
            "    tensor([[12.],\n",
            "            [14.]])\n",
            ")}\n",
            "grad:  bias\n"
          ]
        }
      ],
      "source": [
        "#https://pytorch.org/tutorials/intermediate/per_sample_grads.html?utm_source=whats_new_tutorials&utm_medium=per_sample_grads\n",
        "#https://pytorch.org/docs/stable/generated/torch.func.grad.html\n",
        "#https://towardsdatascience.com/introduction-to-functional-pytorch-b5bf739e1e6e\n",
        "\n",
        "\n",
        "'''\n",
        "We define the weight of a network\n",
        "We want to define the input\n",
        "We want to compute the derive for each data point\n",
        "We want to compute the derive for many data points\n",
        "We generate noise\n",
        "We clip the gradient\n",
        "'''\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torchvision.transforms as transforms\n",
        "from collections import OrderedDict\n",
        "\n",
        "'''\n",
        "0. data true generation: y = 2x + 1\n",
        "1. y = wx + b // w -- weight and b -- bias\n",
        "2. (x=2/3/(2,3), y*= 5/7/(5,7))\n",
        "3. w = 3, b = 5\n",
        "4. L = (y*-y)^2\n",
        "5. dL/dw = dL/dy*dy/dw = 2(y*-y)*dy/dw = 2(y*-y)*x // 24/42/(24+42)/2=33\n",
        "6. dL/db = dL/dy*dy/db = 2(y*-y)*dy/db = 2(y*-y)*1 // 12/14/(12+14)/2=13\n",
        "'''\n",
        "x = torch.tensor([2,3],dtype=torch.float32).view(-1, 1)\n",
        "y = torch.tensor([5,7],dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "print(\"x: \", x)\n",
        "print(\"y: \", y)\n",
        "\n",
        "# Build a fully connected layer.\n",
        "linear = nn.Linear(1, 1)\n",
        "optimizer = torch.optim.SGD(linear.parameters(), lr=0.01)\n",
        "\n",
        "# params = dict(linear.named_parameters())\n",
        "# for key in params.keys():\n",
        "#   print(\"key: \", key, \" , value: \", params[key].data, \" , value object: \", params[key])\n",
        "\n",
        "state3 = OrderedDict()\n",
        "state3['weight'] = torch.tensor([3],dtype=torch.float32).view(-1, 1)\n",
        "state3['bias'] = torch.tensor([5],dtype=torch.float32)\n",
        "linear.load_state_dict(state3)\n",
        "\n",
        "print ('w: ', linear.weight)\n",
        "print ('b: ', linear.bias)\n",
        "\n",
        "# Build loss function and optimizer.\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "\n",
        "from torch.func import functional_call, vmap, grad\n",
        "\n",
        "def compute_loss(params, buffers, model, loss_fn, sample, target):\n",
        "    batch = sample.unsqueeze(0)\n",
        "    targets = target.unsqueeze(0)\n",
        "\n",
        "    predictions = functional_call(model, (params, buffers), (batch,))\n",
        "    loss = loss_fn(predictions, targets)\n",
        "    return loss\n",
        "\n",
        "ft_compute_grad = grad(compute_loss)\n",
        "\n",
        "def compute_clamped_grad(params, buffers, model, loss_fn, sample, target):\n",
        "    #compute the gradient\n",
        "    grads = ft_compute_grad(params, buffers, model, loss_fn, sample, target)\n",
        "    #grads = OrderedDict(ft_compute_grad(params, buffers, model, loss_fn, sample, target))\n",
        "    # #The output is dictionary and thus, we cannot apply the torch.clipping operation\n",
        "    # #We have to use multiplier trick\n",
        "    # norm_type = 2.0\n",
        "    # max_norm = 1.0 #This is clipping constant C\n",
        "    # total_norm = torch.norm(torch.stack([torch.norm(grad.detach(), norm_type) for key,grad in grads.items()]), norm_type)\n",
        "    # print(f\"total_norm: {total_norm}\")\n",
        "    # clip_coef = max_norm / (total_norm + 1e-6)\n",
        "    # print(f\"clip_coef: {clip_coef}\")\n",
        "    # #https://www.tutorialspoint.com/python-pytorch-clamp-method\n",
        "    # #clamp(tensor,min,max)\n",
        "    # clip_coef_clamped = torch.clamp(clip_coef, max=1000)\n",
        "    # print(f\"clip_coef_clamped: {clip_coef_clamped}\")\n",
        "    # for key,grad in grads.items():\n",
        "    #     print(f\"before clipping: {grad}\")\n",
        "    #     grads[key] = grad.detach().mul_(clip_coef_clamped)\n",
        "    #     print(f\"after clipping: {grad}\")\n",
        "\n",
        "    print(\"grads: \", grads)\n",
        "    # #clipping the grads\n",
        "    for grad in grads:\n",
        "      print(\"grads: \", grads)\n",
        "      print(\"grad: \", grad)\n",
        "\n",
        "    return grads\n",
        "\n",
        "'''\n",
        "The ft_compute_grad function computes the gradient for a single (sample, target) pair.\n",
        "# We can use vmap to get it to compute the gradient over an entire batch of samples and targets.\n",
        "# Note that in_dims=(None, None, 0, 0) because we wish to\n",
        "# map ft_compute_grad over the 0th dimension of the data and targets, and use the same params and buffers for each.\n",
        "# '''\n",
        "\n",
        "params = {k: v.detach() for k, v in linear.named_parameters()}\n",
        "buffers = {k: v.detach() for k, v in linear.named_buffers()}\n",
        "\n",
        "#ft_per_sample_grads = compute_clamped_grad(params, buffers, linear, criterion, x[0], y[0])\n",
        "ft_compute_sample_grad = vmap(compute_clamped_grad, in_dims=(None, None, None, None, 0, 0))\n",
        "ft_per_sample_grads = ft_compute_sample_grad(params, buffers, linear, criterion, x, y)\n",
        "\n",
        "# for key in ft_per_sample_grads.keys():\n",
        "#     print(\"key: \", key, \" value: \", ft_per_sample_grads[key])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFzDtVgKxrXe",
        "outputId": "02178928-d1fa-4c8f-f900-0629876a17da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torchopt\n",
            "  Downloading torchopt-0.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (691 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m691.3/691.3 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from torchopt) (2.0.1+cu118)\n",
            "Collecting optree>=0.4.1 (from torchopt)\n",
            "  Downloading optree-0.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (319 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m319.2/319.2 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchopt) (1.23.5)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from torchopt) (0.20.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from torchopt) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->torchopt) (3.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->torchopt) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->torchopt) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->torchopt) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->torchopt) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.13->torchopt) (3.27.4.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.13->torchopt) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13->torchopt) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->torchopt) (1.3.0)\n",
            "Installing collected packages: optree, torchopt\n",
            "Successfully installed optree-0.9.2 torchopt-0.7.2\n"
          ]
        }
      ],
      "source": [
        "!pip install torchopt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        },
        "id": "lxLMQChtxd_Z",
        "outputId": "b756fba1-2b98-4fed-bdf7-88460cd65e89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 0 with loss 2.6665844917297363\n",
            "Iteration 100 with loss 0.28552597761154175\n",
            "Iteration 200 with loss 0.1436481773853302\n",
            "Iteration 300 with loss 0.12077581882476807\n",
            "Iteration 400 with loss 0.009211739525198936\n",
            "Loss on the test set: 0.0008826549164950848\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#https://gist.github.com/madagra/64afe1b56ff5656b2b1acb19cc68f477\n",
        "#https://towardsdatascience.com/introduction-to-functional-pytorch-b5bf739e1e6e\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import Tensor\n",
        "from torch.func import functional_call\n",
        "import torchopt\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_layers: int = 1,\n",
        "        num_neurons: int = 5,\n",
        "    ) -> None:\n",
        "        \"\"\"Basic neural network architecture with linear layers\n",
        "        Args:\n",
        "            num_layers (int, optional): number of hidden layers\n",
        "            num_neurons (int, optional): neurons for each hidden layer\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        layers = []\n",
        "\n",
        "        # input layer\n",
        "        layers.append(nn.Linear(1, num_neurons))\n",
        "\n",
        "        # hidden layers\n",
        "        for _ in range(num_layers):\n",
        "            layers.extend([nn.Linear(num_neurons, num_neurons), nn.Tanh()])\n",
        "\n",
        "        # output layer\n",
        "        layers.append(nn.Linear(num_neurons, 1))\n",
        "\n",
        "        # build the network\n",
        "        self.network = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.network(x.reshape(-1, 1)).squeeze()\n",
        "\n",
        "\n",
        "def make_functional_fwd(model: torch.nn.Module):\n",
        "    \"\"\"Make a functional forward pass for a generic module\n",
        "    This function is compatible with the torchopt library which\n",
        "    returns the updated parameters as a tuple while the `functional_call`\n",
        "    routine requires parameters dictionary. This conversion is automatically\n",
        "    implemented by this function\n",
        "    \"\"\"\n",
        "\n",
        "    keys = list(dict(model.named_parameters()).keys())\n",
        "\n",
        "    def fn(data: Tensor, parameters: tuple[Tensor, ...]):\n",
        "        params_dict = {k: v for k, v in zip(keys, parameters)}\n",
        "        return functional_call(model, params_dict, (data,))\n",
        "\n",
        "    return fn\n",
        "\n",
        "\n",
        "def get_data(n_points: int = 20) -> tuple[Tensor, Tensor]:\n",
        "    \"\"\"Prepare the input data for training/test sets\"\"\"\n",
        "    x = torch.rand(n_points) * 2.0 * torch.pi\n",
        "    y = 2.0 * torch.sin(x + 2.0 * torch.pi)\n",
        "    return x, y\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    torch.manual_seed(42)\n",
        "\n",
        "    model = SimpleNN(num_layers=2)\n",
        "    model_func = make_functional_fwd(model)\n",
        "\n",
        "    x_train, y_train = get_data(n_points=40)\n",
        "    x_test, y_test = get_data(n_points=10)\n",
        "\n",
        "    # choose optimizer with functional API using functorch\n",
        "    num_epochs = 500\n",
        "    lr = 0.01\n",
        "    optimizer = torchopt.FuncOptimizer(torchopt.adam(lr=lr))\n",
        "    loss_fn = torch.nn.MSELoss()\n",
        "\n",
        "    # train the model\n",
        "    loss_evolution = []\n",
        "    params = tuple(model.parameters())\n",
        "\n",
        "    for i in range(num_epochs):\n",
        "        # update the parameters\n",
        "        y = model_func(x_train, params)\n",
        "        loss = loss_fn(y, y_train)\n",
        "        params = optimizer.step(loss, params)\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print(f\"Iteration {i} with loss {float(loss)}\")\n",
        "        loss_evolution.append(float(loss))\n",
        "\n",
        "    # performance on the model on the test set\n",
        "    y_pred = model_func(x_test, params)\n",
        "    print(f\"Loss on the test set: {loss_fn(y_pred, y_test)}\")\n",
        "\n",
        "    # plot the final predictions\n",
        "    def fn_to_fit(x):\n",
        "        return 2.0 * np.sin(x + 2.0 * np.pi)\n",
        "\n",
        "    x_analyt = np.linspace(0, 2.0 * np.pi, 100)\n",
        "    y_analyt = fn_to_fit(x_analyt)\n",
        "\n",
        "    x_test_np = x_test.detach().numpy()\n",
        "    y_test_np = y_test.detach().numpy()\n",
        "    y_pred_np = y_pred.detach().numpy()\n",
        "\n",
        "    plt.figure(1)\n",
        "    plt.plot(x_analyt, y_analyt, label=\"Analytical\")\n",
        "    plt.scatter(x_test_np, y_test_np, label=\"True\")\n",
        "    plt.scatter(x_test_np, y_pred_np, label=\"Predicted\")\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1CnbmnZrxVDw",
        "outputId": "e600866e-8a81-49fd-d9f6-e39ebd71ea2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x: tensor([[ 0.0327,  1.1478,  0.2456, -1.1227, -0.6412],\n",
            "        [ 0.1334,  1.1113, -0.1386,  0.0078,  0.0959]])\n",
            "y: tensor([[-0.5977,  0.2078,  1.6205, -0.8795, -0.1493],\n",
            "        [-0.2656, -0.2342,  2.2183,  0.8813, -0.0677]])\n",
            "z: tensor([ 1.7001, -0.6027])\n",
            "weights: tensor([ 0.1164,  0.8006, -0.6367,  2.2237,  0.5500], requires_grad=True)\n",
            "examples: tensor([[-0.2163, -0.0234, -0.4069,  0.1827, -1.9061],\n",
            "        [-1.0411, -0.4820,  2.7922,  1.0468, -1.2729],\n",
            "        [ 0.6814, -2.2780, -0.1667,  0.6843, -2.3623]])\n",
            "result: tensor([[[1, 2],\n",
            "         [3, 4]],\n",
            "\n",
            "        [[1, 2],\n",
            "         [3, 4]],\n",
            "\n",
            "        [[1, 2],\n",
            "         [3, 4]]])\n",
            "expected: tensor([[[1, 2],\n",
            "         [3, 4]],\n",
            "\n",
            "        [[1, 2],\n",
            "         [3, 4]],\n",
            "\n",
            "        [[1, 2],\n",
            "         [3, 4]]])\n"
          ]
        }
      ],
      "source": [
        "#https://github.com/pytorch/tutorials/blob/main/prototype_source/vmap_recipe.py\n",
        "import torch\n",
        "# NB: vmap is only available on nightly builds of PyTorch.\n",
        "# You can download one at pytorch.org if you're interested in testing it out.\n",
        "from torch import vmap\n",
        "\n",
        "####################################################################\n",
        "# The first use case for vmap is making it easier to handle\n",
        "# batch dimensions in your code. One can write a function `func`\n",
        "# that runs on examples and then lift it to a function that can\n",
        "# take batches of examples with `vmap(func)`. `func` however\n",
        "# is subject to many restrictions:\n",
        "#\n",
        "# - it must be functional (one cannot mutate a Python data structure\n",
        "#   inside of it), with the exception of in-place PyTorch operations.\n",
        "# - batches of examples must be provided as Tensors. This means that\n",
        "#   vmap doesn't handle variable-length sequences out of the box.\n",
        "#\n",
        "# One example of using `vmap` is to compute batched dot products. PyTorch\n",
        "# doesn't provide a batched `torch.dot` API; instead of unsuccessfully\n",
        "# rummaging through docs, use `vmap` to construct a new function:\n",
        "\n",
        "torch.dot                            # [D], [D] -> []\n",
        "batched_dot = torch.vmap(torch.dot)  # [N, D], [N, D] -> [N]\n",
        "x, y = torch.randn(2, 5), torch.randn(2, 5)\n",
        "z = batched_dot(x, y)\n",
        "\n",
        "print(f\"x: {x}\")\n",
        "print(f\"y: {y}\")\n",
        "print(f\"z: {z}\")\n",
        "\n",
        "####################################################################\n",
        "# `vmap` can be helpful in hiding batch dimensions, leading to a simpler\n",
        "# model authoring experience.\n",
        "batch_size, feature_size = 3, 5\n",
        "weights = torch.randn(feature_size, requires_grad=True)\n",
        "print(f\"weights: {weights}\")\n",
        "\n",
        "# Note that model doesn't work with a batch of feature vectors because\n",
        "# torch.dot must take 1D tensors. It's pretty easy to rewrite this\n",
        "# to use `torch.matmul` instead, but if we didn't want to do that or if\n",
        "# the code is more complicated (e.g., does some advanced indexing\n",
        "# shenanigins), we can simply call `vmap`. `vmap` batches over ALL\n",
        "# inputs, unless otherwise specified (with the in_dims argument,\n",
        "# please see the documentation for more details).\n",
        "def model(feature_vec):\n",
        "    # Very simple linear model with activation\n",
        "    # v = feature_vec.dot(weights)\n",
        "    # return torch.tensor([v,v+1])\n",
        "    data = [[1, 2], [3, 4]]\n",
        "    x_data = torch.tensor(data)\n",
        "    return x_data\n",
        "\n",
        "\n",
        "examples = torch.randn(batch_size, feature_size)\n",
        "print(f\"examples: {examples}\")\n",
        "\n",
        "result = torch.vmap(model)(examples)\n",
        "print(f\"result: {result}\")\n",
        "\n",
        "expected = torch.stack([model(example) for example in examples.unbind()])\n",
        "print(f\"expected: {expected}\")\n",
        "\n",
        "assert torch.allclose(result, expected)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1BI87N9N2yXi"
      },
      "outputs": [],
      "source": [
        "#how to write a file in google colab\n",
        "\n",
        "# First we need to mount the Google drive\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/Hadrive')\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "data = {\n",
        "  \"calories\": [420, 380, 390],\n",
        "  \"duration\": [50, 40, 45]\n",
        "}\n",
        "\n",
        "#load data into a DataFrame object:\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "print(df)\n",
        "\n",
        "!mkdir /content/Hadrive/MyDrive/Test1\n",
        "df.to_csv(\"/content/Hadrive/MyDrive/Test1/dframe.csv\")\n",
        "\n",
        "\n",
        "df1 = pd.read_csv(\"/content/Hadrive/MyDrive/Test1/dframe.csv\")\n",
        "df1.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NnZoe1m7psO0",
        "outputId": "d98cd401-a210-4f73-bcf9-7700f6f0cf9a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "63.00249944258167"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sum = 0\n",
        "\n",
        "for i in range(1,21,1):\n",
        "    sum = sum + 1.1**i\n",
        "\n",
        "sum"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
