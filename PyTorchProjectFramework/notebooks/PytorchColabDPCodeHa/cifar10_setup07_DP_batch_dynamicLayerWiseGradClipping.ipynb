{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23448,"status":"ok","timestamp":1697750795510,"user":{"displayName":"Phuong Ha Nguyen","userId":"01448229299682072761"},"user_tz":420},"id":"r7Rl1QFk5icv","outputId":"048e9a6f-71ee-433c-c53f-855728b291ef"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/Hadrive\n"]}],"source":["# First we need to mount the Google drive\n","import os\n","from google.colab import drive\n","drive.mount('/content/Hadrive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":216,"status":"ok","timestamp":1697750838478,"user":{"displayName":"Phuong Ha Nguyen","userId":"01448229299682072761"},"user_tz":420},"id":"9r5K3CJU236g","outputId":"cf1601cd-6b12-4df6-c5ec-7b6d0cab8c62"},"outputs":[{"name":"stdout","output_type":"stream","text":["index: 1\n","config: {'lr_initial': 0.1, 'decay': 0.9, 'sigma': 1e-08, 'const_C': 1000}\n","key: lr_initial, value: 0.1\n","key: decay, value: 0.9\n","key: sigma, value: 1e-08\n","key: const_C, value: 1000\n"]}],"source":["\n","configs = dict({\n","\"1\": { \"lr_initial\": 0.1, \"decay\": 0.9, \"sigma\": 0.00000001, \"const_C\": 1000}\n","})\n","\n","\n","for index, config in configs.items():\n","  print(f\"index: {index}\")\n","  print(f\"config: {config}\")\n","  for key,value in config.items():\n","    print(f\"key: {key}, value: {value}\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q8wxEMaM3C59"},"outputs":[],"source":["#!mkdir /content/Hadrive/MyDrive/Test1\n","#!mkdir /content/Hadrive/MyDrive/Test1/Tutorial1/"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":4724,"status":"ok","timestamp":1697750846866,"user":{"displayName":"Phuong Ha Nguyen","userId":"01448229299682072761"},"user_tz":420},"id":"vkpkT6pfieuj"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torch.optim as optim\n","import torch.optim.lr_scheduler as lr_scheduler\n","# import torch.optim.lr_scheduler.StepLR as StepLR\n","# import torch.optim.lr_scheduler.LinearLR as LinearLR\n","\n","\n","\n","from torchvision import datasets\n","from torchvision import transforms\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8010,"status":"ok","timestamp":1697750861021,"user":{"displayName":"Phuong Ha Nguyen","userId":"01448229299682072761"},"user_tz":420},"id":"j0_-UJk0izg6","outputId":"82b57f79-d153-406e-d260-8d3b8ae47059"},"outputs":[{"name":"stdout","output_type":"stream","text":["lencifar10: 50000\n","lencifar10_val: 10000\n","lencifar10_surrogate: 10\n"]}],"source":["data_path = '/content/Hadrive/MyDrive/Test1/Tutorial1/'\n","\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.4915, 0.4823, 0.4468), (0.2470, 0.2435, 0.2616))\n","])\n","\n","# cifar10 = datasets.CIFAR10(data_path, train=True, download=True, transform=transform)\n","# cifar10_val = datasets.CIFAR10(data_path, train=False, download=True, transform=transform)\n","\n","nbsamples = 10\n","\n","#https://stackoverflow.com/questions/47432168/taking-subsets-of-a-pytorch-dataset\n","cifar10_org = datasets.CIFAR10(data_path, train=True, download=False, transform=transform)\n","#subset_org = list(range(0, len(cifar10_org), len(cifar10_org)//100))\n","subset_org = list(range(0, nbsamples))\n","#cifar10 = torch.utils.data.Subset(cifar10_org, subset_org)\n","cifar10 = cifar10_org\n","\n","cifar10_val_org = datasets.CIFAR10(data_path, train=False, download=False, transform=transform)\n","#subset_org = list(range(0, len(cifar10_val_org), len(cifar10_val_org)//100))\n","subset_org = list(range(0, nbsamples))\n","#cifar10_val = torch.utils.data.Subset(cifar10_val_org, subset_org)\n","cifar10_val = cifar10_val_org\n","\n","#A subset from test dataset for computing the layer_wise_constant Ci\n","cifar10_surrogate = torch.utils.data.Subset(cifar10_val_org, list(range(0, nbsamples)))\n","\n","print(f\"lencifar10: {len(cifar10)}\")\n","print(f\"lencifar10_val: {len(cifar10_val)}\")\n","print(f\"lencifar10_surrogate: {len(cifar10_surrogate)}\")"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":197,"status":"ok","timestamp":1697750890416,"user":{"displayName":"Phuong Ha Nguyen","userId":"01448229299682072761"},"user_tz":420},"id":"i4W2GOsdjK72"},"outputs":[],"source":["train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64,\n","                                           shuffle=True)\n","val_loader = torch.utils.data.DataLoader(cifar10_val, batch_size=64,\n","                                           shuffle=True)\n","data_surrogate_loader = torch.utils.data.DataLoader(cifar10_surrogate, batch_size=nbsamples,\n","                                           shuffle=True)"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":192,"status":"ok","timestamp":1697750893221,"user":{"displayName":"Phuong Ha Nguyen","userId":"01448229299682072761"},"user_tz":420},"id":"iy4nacdKjQGu"},"outputs":[],"source":["# model\n","class Net(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n","        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n","        self.fc1 = nn.Linear(8 * 8 * 8, 32)\n","        # bài toán phân loại 10 lớp nên output ra 10 nodes\n","        self.fc2 = nn.Linear(32, 10)\n","\n","    def forward(self, x):\n","        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n","        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n","        # flatten về dạng vector để cho vào neural network\n","        out = out.view(-1, 8 * 8 * 8)\n","        out = torch.tanh(self.fc1(out))\n","        out = self.fc2(out)\n","        return out\n"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":179,"status":"ok","timestamp":1697750902236,"user":{"displayName":"Phuong Ha Nguyen","userId":"01448229299682072761"},"user_tz":420},"id":"0Pn9A9tujUnW"},"outputs":[],"source":["import pickle\n","\n","import torch\n","import torchvision\n","import torch.nn as nn\n","import numpy as np\n","import torchvision.transforms as transforms\n","from collections import OrderedDict\n","from collections import defaultdict\n","from torch.func import functional_call, vmap, grad\n","\n","def generate_private_grad(model,loss_fn,samples,targets,sigma,dict_const_Ci):\n","    '''\n","        We generate private grad given a batch of samples (samples,targets) as introduced here https://arxiv.org/pdf/1607.00133.pdf\n","        The implementation flow is as follows:\n","            1. sample xi\n","            2. ===> gradient gi\n","            3. ===> clipped gradient gci\n","            4. ===> noisy aggregated (sum gci + noise)\n","            5. ===> normalized 1/B (sum gci + noise)\n","\n","        We want to follow the tutorial in here to compute multiple grads in parallel:\n","            #https://pytorch.org/tutorials/intermediate/per_sample_grads.html?utm_source=whats_new_tutorials&utm_medium=per_sample_grads\n","            #https://pytorch.org/docs/stable/generated/torch.func.grad.html\n","            #https://towardsdatascience.com/introduction-to-functional-pytorch-b5bf739e1e6e\n","            #https://pytorch.org/functorch/stable/notebooks/per_sample_grads.html [PAY ATTENTION TO]\n","        Typically, we generate all gradients gis of samples sis in parallel in the helper function: compute_gradients\n","        The output of compute_gradients is an array called samples_grads\n","            sample s[0]: samples_grads[0][layer_1], samples_grads[0][layer_2], .... //g0\n","                ...............\n","            sample s[L-1]: samples_grads[L-1][layer_1], samples_grads[L-1][layer_2], ....//g[L-1]\n","            where L is the number of samples in the mini-batch\n","\n","        The compute_gradients call another helper function called compute_loss. This is used for computing the gradients in parallel\n","\n","        After that we compute the clipped gradients gci for each gi. In this case we use the following approach proposed here\n","            #https://www.tutorialspoint.com/python-pytorch-clamp-method\n","\n","        We apply dynamic clipping methods, i.e., each layer has its own clipping constant Ci which is computed based on an known dataset (called surrogate).\n","        Typically, we compute the gradients of this known dataset surrogate dataset. We compute the average of the norm of each layer_i (Ci) and then we define\n","        the dynamic clipping constant of layer_i Ci = C*(Ci/max Ci). Note that master constant C can be diminised every epoch.\n","\n","        After computing all clipped gradients, we need to aggregate all the clipped gradient per layer. This step helps us\n","        to compute the sum (clipped gradient gi) and then we add noise to each entry in the sum (clipped gradient gi)\n","\n","        Finally, we normalize the private gradient and update the model.grad. This step allows optimizer update the model\n","    '''\n","\n","    #copute the gradient of the whole batch\n","    outputs = model(samples)\n","    loss = loss_fn(outputs, targets)\n","    model.zero_grad()\n","    loss.backward()\n","\n","    #generate private grad per layer\n","    mean = 0\n","    batch_size = len(samples)\n","    norm_type = 2.0\n","\n","    for layer, param in model.named_parameters():\n","        #clipping the gradient\n","        #https://discuss.pytorch.org/t/how-to-clip-grad-norm-grads-from-torch-autograd-grad/137816/2\n","        max_norm = dict_const_Ci[layer] #This is clipping constant Ci for layer_i\n","        grad = param.grad\n","        total_norm = torch.norm(grad, norm_type)\n","        clip_coef = max_norm / (total_norm + 1e-6)\n","        #https://www.tutorialspoint.com/python-pytorch-clamp-method\n","        #clamp(tensor,min,max)\n","        clip_coef_clamped = torch.clamp(clip_coef, max=1.0)\n","        param.grad.detach().mul_(clip_coef_clamped)\n","\n","        #generate the noise and add it to the clipped gradients\n","        grad = param.grad\n","        std = sigma*dict_const_Ci[layer]\n","        #generate the noise ~ N(0,(C\\sigma)^2I)\n","        #std -- is C\\sigma as explain this in wikipage https://en.wikipedia.org/wiki/Normal_distribution N(mu,\\sigma^2) and sigma is std\n","        noise = torch.normal(mean=mean, std=std, size=grad.shape)\n","        #generate private gradient per layer\n","        param.grad = (grad*batch_size + noise)/batch_size\n","\n","    return 0\n","\n","def generate_layerwise_clipping_constants(model,optimizer,loss_fn,data_surrogate_loader,const_C):\n","    '''\n","      We compute the layerwise clipping constant Ci based on data_surrogate\n","      Step 1. We compute the layer norm Ci\n","      Step 2. We redefine Ci = Const_C * (Ci/(max_i Ci))\n","    '''\n","    for imgs, labels in data_surrogate_loader:\n","        outputs = model(imgs)\n","        loss = loss_fn(outputs, labels)\n","        optimizer.zero_grad()\n","        loss.backward()\n","\n","    dict_clipping_const = dict()\n","    norm_type = 2.0\n","    maxC = 0.0\n","    for layer, param in model.named_parameters():\n","        grad = param.grad\n","        dict_clipping_const[layer] = torch.norm(grad, norm_type)\n","        if(dict_clipping_const[layer] > maxC):\n","          maxC = dict_clipping_const[layer]\n","\n","    #delete the information in the model.param.grad\n","    optimizer.zero_grad()\n","\n","    #normalize the clipping constant Ci\n","    for layer in dict_clipping_const.keys():\n","      dict_clipping_const[layer] = const_C*(dict_clipping_const[layer]/maxC)\n","\n","    return dict_clipping_const\n","\n","\n","\n","def training_loop(n_epochs, optimizer, model, loss_fn, sigma, const_C, train_loader, val_loader, data_surrogate_loader, data_path):\n","    for epoch in range(1, n_epochs + 1):\n","        #generate the layerwise clipping constant Ci\n","        dict_const_Ci = generate_layerwise_clipping_constants(model,optimizer,loss_fn,data_surrogate_loader,const_C)\n","\n","        loss_train = 0.0\n","\n","        for imgs, labels in train_loader:\n","\n","          outputs = model(imgs)\n","          loss = loss_fn(outputs, labels)\n","          loss_train += loss.item()\n","\n","          optimizer.zero_grad()\n","          '''\n","            generate_private_grad(model,loss_fn,imgs,labels,sigma,const_C)\n","              1. Compute the grad per sample\n","              2. Clipping the grad per sample\n","              3. Aggregate the clipped grads and add noise to sum of clipped grads\n","              4. Update the model.grad. This helps optimizer.step works as normal.\n","          '''\n","          #loss.backward()\n","          generate_private_grad(model,loss_fn,imgs,labels,sigma,dict_const_Ci)\n","          optimizer.step()\n","\n","        correct = 0\n","        with torch.no_grad():\n","            for data in val_loader:\n","                images, labels = data\n","                outputs = model(images)\n","                _, predicted = torch.max(outputs, 1)\n","                c = (predicted == labels).squeeze()\n","                correct += c.sum()\n","        if epoch == 1 or epoch % 1 == 0:\n","            print('Epoch {}, Training loss {}, Val accuracy {}'.format(\n","                epoch,\n","                loss_train / len(train_loader),\n","                correct / len(cifar10_val)))\n","\n","        before_lr = optimizer.param_groups[0][\"lr\"]\n","        scheduler.step()\n","        after_lr = optimizer.param_groups[0][\"lr\"]\n","        print(\"Epoch %d: SGD lr %.4f -> %.4f\" % (epoch, before_lr, after_lr))\n","\n","\n","        #save the model config\n","        model_state = model.state_dict()\n","        optimizer_state = optimizer.state_dict()\n","        scheduler_state = scheduler.state_dict()\n","        dict_state = dict()\n","        dict_state[\"epoch\"] = epoch\n","        dict_state[\"sigma\"] = sigma\n","        dict_state[\"const_C\"] = const_C\n","        dict_state[\"dic_const_Ci\"] = dict_const_Ci\n","        dict_state[\"model_state\"] = model_state\n","        dict_state[\"optimizer_state\"] = optimizer_state\n","        dict_state[\"scheduler_state\"] = scheduler_state\n","        dict_state[\"train_loss\"] = loss_train / len(train_loader)\n","        dict_state[\"val_acc\"] = correct / len(cifar10_val)\n","\n","        try:\n","            geeky_file = open(data_path + \"epoch_\" + str(epoch), 'wb')\n","            pickle.dump(dict_state, geeky_file)\n","            geeky_file.close()\n","\n","        except:\n","            print(\"Something went wrong\")\n","\n","        #print(f\"scheduler state: {scheduler_state}\")"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":878625,"status":"ok","timestamp":1697751795318,"user":{"displayName":"Phuong Ha Nguyen","userId":"01448229299682072761"},"user_tz":420},"id":"auhHQgx8jcbV","outputId":"25ce1889-85b9-441c-f979-6cb0e355791a"},"outputs":[{"name":"stdout","output_type":"stream","text":["index: 1\n","Epoch 1, Training loss 1.5772466089414514, Val accuracy 0.424699991941452\n","Epoch 1: SGD lr 0.1000 -> 0.0900\n","Epoch 2, Training loss 1.2488595805966947, Val accuracy 0.5325999855995178\n","Epoch 2: SGD lr 0.0900 -> 0.0810\n","Epoch 3, Training loss 1.1349233345454917, Val accuracy 0.5580999851226807\n","Epoch 3: SGD lr 0.0810 -> 0.0729\n","Epoch 4, Training loss 1.0674584114643009, Val accuracy 0.5428000092506409\n","Epoch 4: SGD lr 0.0729 -> 0.0656\n","Epoch 5, Training loss 1.0214700245338937, Val accuracy 0.5730999708175659\n","Epoch 5: SGD lr 0.0656 -> 0.0590\n","Epoch 6, Training loss 0.9808162244232109, Val accuracy 0.6226999759674072\n","Epoch 6: SGD lr 0.0590 -> 0.0531\n","Epoch 7, Training loss 0.9489028581877803, Val accuracy 0.6092000007629395\n","Epoch 7: SGD lr 0.0531 -> 0.0478\n","Epoch 8, Training loss 0.9235048895449285, Val accuracy 0.5909000039100647\n","Epoch 8: SGD lr 0.0478 -> 0.0430\n","Epoch 9, Training loss 0.8997492524973877, Val accuracy 0.6121000051498413\n","Epoch 9: SGD lr 0.0430 -> 0.0387\n","Epoch 10, Training loss 0.8802936987956161, Val accuracy 0.6172000169754028\n","Epoch 10: SGD lr 0.0387 -> 0.0349\n","Epoch 11, Training loss 0.8612092473470342, Val accuracy 0.6417999863624573\n","Epoch 11: SGD lr 0.0349 -> 0.0314\n","Epoch 12, Training loss 0.8461799232856088, Val accuracy 0.6499000191688538\n","Epoch 12: SGD lr 0.0314 -> 0.0282\n","Epoch 13, Training loss 0.8320144232352982, Val accuracy 0.6395999789237976\n","Epoch 13: SGD lr 0.0282 -> 0.0254\n","Epoch 14, Training loss 0.8182586384246416, Val accuracy 0.6425999999046326\n","Epoch 14: SGD lr 0.0254 -> 0.0229\n","Epoch 15, Training loss 0.8062966364576384, Val accuracy 0.6474999785423279\n","Epoch 15: SGD lr 0.0229 -> 0.0206\n","Epoch 16, Training loss 0.7985754591958297, Val accuracy 0.6345999836921692\n","Epoch 16: SGD lr 0.0206 -> 0.0185\n","Epoch 17, Training loss 0.7898366503093553, Val accuracy 0.6437000036239624\n","Epoch 17: SGD lr 0.0185 -> 0.0167\n","Epoch 18, Training loss 0.778532884538631, Val accuracy 0.6531000137329102\n","Epoch 18: SGD lr 0.0167 -> 0.0150\n","Epoch 19, Training loss 0.7711543072291347, Val accuracy 0.6541000008583069\n","Epoch 19: SGD lr 0.0150 -> 0.0135\n","Epoch 20, Training loss 0.7656334226622301, Val accuracy 0.6539999842643738\n","Epoch 20: SGD lr 0.0135 -> 0.0122\n"]}],"source":["for index, config in configs.items():\n","  print(f\"index: {index}\")\n","  data_path_index = data_path + \"config_\" + str(index) + \"_\"\n","  model = Net()\n","  optimizer = optim.SGD(model.parameters(), lr=config[\"lr_initial\"])\n","  loss_fn = nn.CrossEntropyLoss()\n","  '''\n","    LinearLR =>> new LR = initial LR - nb_epochs*(start_factor-end_factor)/total_iters\n","    example, initialLR = 0.1, start = 1.0, end_factor = 0.5, total_iters = 20\n","    (start_factor-end_factor)/total_iters = 0.025.\n","    ===> epoch 1: 0.1 - 1*0.025 = 0.0975\n","    ===> epoch 2: 0.1 - 2*0.025 = 0.0950....\n","  '''\n","  #scheduler = lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.5, total_iters=20)\n","  '''\n","   StepLR =>>> new LR = old LR * gamma\n","  '''\n","  scheduler = lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n","  '''\n","    LambdaLR: new LR = initialLR * f(epoch)\n","    For example: f(epoch) = 1/t\n","  '''\n","  # lambda1 = lambda epoch: 1/(epoch+1)\n","  # scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=[lambda1])\n","\n","  training_loop(\n","      n_epochs = 20,\n","      optimizer = optimizer,\n","      model = model,\n","      sigma = config[\"sigma\"],\n","      const_C = config[\"const_C\"],\n","      loss_fn = loss_fn,\n","      train_loader = train_loader,\n","      val_loader = val_loader,\n","      data_surrogate_loader = data_surrogate_loader,\n","      data_path = data_path_index\n","  )\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":574,"status":"ok","timestamp":1696992505070,"user":{"displayName":"Phuong Ha Nguyen","userId":"01448229299682072761"},"user_tz":420},"id":"IkrX1-y8oABE","outputId":"d1d8326c-52d1-4a2f-d0ab-d31f8204867d"},"outputs":[{"name":"stdout","output_type":"stream","text":["dict_keys(['epoch', 'model_state', 'optimizer_state'])\n"]}],"source":["import pandas as pd\n","epoch = 1\n","path = data_path + \"epoch_\" + str(epoch)\n","obj = pd.read_pickle(path)\n","print(obj.keys())"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPmSS+uErKaUp1UwliQl6QQ","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
