{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20277,"status":"ok","timestamp":1698071254494,"user":{"displayName":"Phuong Ha Nguyen","userId":"01448229299682072761"},"user_tz":420},"id":"r7Rl1QFk5icv","outputId":"d6810c1c-7f24-4147-8d18-872ed4750d54"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/Hadrive\n"]}],"source":["# First we need to mount the Google drive\n","import os\n","from google.colab import drive\n","drive.mount('/content/Hadrive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q8wxEMaM3C59"},"outputs":[],"source":["#!mkdir /content/Hadrive/MyDrive/Test1\n","#!mkdir /content/Hadrive/MyDrive/Test1/Tutorial1/"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":6865,"status":"ok","timestamp":1698071264850,"user":{"displayName":"Phuong Ha Nguyen","userId":"01448229299682072761"},"user_tz":420},"id":"vkpkT6pfieuj"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torch.optim as optim\n","import torch.optim.lr_scheduler as lr_scheduler\n","\n","from torchvision import datasets\n","from torchvision import transforms\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6475,"status":"ok","timestamp":1698071274260,"user":{"displayName":"Phuong Ha Nguyen","userId":"01448229299682072761"},"user_tz":420},"id":"j0_-UJk0izg6","outputId":"a04d5e9b-1cbd-42c5-8716-7a43ce3c8502"},"outputs":[{"output_type":"stream","name":"stdout","text":["lencifar10: 50000\n","lencifar10_val: 10000\n"]}],"source":["data_path = '/content/Hadrive/MyDrive/Test1/Tutorial1/'\n","\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.4915, 0.4823, 0.4468), (0.2470, 0.2435, 0.2616))\n","])\n","\n","# cifar10 = datasets.CIFAR10(data_path, train=True, download=True, transform=transform)\n","# cifar10_val = datasets.CIFAR10(data_path, train=False, download=True, transform=transform)\n","\n","number_samples = 100\n","\n","#https://stackoverflow.com/questions/47432168/taking-subsets-of-a-pytorch-dataset\n","cifar10_org = datasets.CIFAR10(data_path, train=True, download=False, transform=transform)\n","#cifar10 = torch.utils.data.Subset(cifar10_org, list(range(0, number_samples)))\n","cifar10 = cifar10_org\n","\n","cifar10_val_org = datasets.CIFAR10(data_path, train=False, download=False, transform=transform)\n","#cifar10_val = torch.utils.data.Subset(cifar10_val_org, list(range(0, number_samples)))\n","cifar10_val = cifar10_val_org\n","\n","print(f\"lencifar10: {len(cifar10)}\")\n","print(f\"lencifar10_val: {len(cifar10_val)}\")"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":196,"status":"ok","timestamp":1698071275584,"user":{"displayName":"Phuong Ha Nguyen","userId":"01448229299682072761"},"user_tz":420},"id":"iy4nacdKjQGu"},"outputs":[],"source":["# model\n","class Net(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n","        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n","        self.fc1 = nn.Linear(8 * 8 * 8, 32)\n","        # bài toán phân loại 10 lớp nên output ra 10 nodes\n","        self.fc2 = nn.Linear(32, 10)\n","\n","    def forward(self, x):\n","        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n","        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n","        # flatten về dạng vector để cho vào neural network\n","        out = out.view(-1, 8 * 8 * 8)\n","        out = torch.tanh(self.fc1(out))\n","        out = self.fc2(out)\n","        return out\n","\n","#https://www.kaggle.com/code/grayphantom/cnn-on-cifar10-using-pytorch\n","#https://tek4.vn/batch-norm-trong-pytorch-lap-trinh-neural-network-voi-pytorch\n","#https://viblo.asia/p/3-cap-do-hieu-ve-batch-normalization-bai-dich-johann-huber-Yym40mRmJ91\n","\n","class NetBNL(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n","        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n","        self.fc1 = nn.Linear(8 * 8 * 8, 32)\n","        # bài toán phân loại 10 lớp nên output ra 10 nodes\n","        self.fc2 = nn.Linear(32, 10)\n","        self.BatchNorm1d_32 = nn.BatchNorm1d(32)\n","        self.BatchNorm2d_8 = nn.BatchNorm2d(8)\n","        self.BatchNorm2d_16 = nn.BatchNorm2d(16)\n","\n","    def forward(self, x):\n","        out = self.BatchNorm2d_16(F.max_pool2d(torch.tanh(self.conv1(x)), 2))\n","        out = self.BatchNorm2d_8(F.max_pool2d(torch.tanh(self.conv2(out)), 2))\n","        # flatten về dạng vector để cho vào neural network\n","        out = out.view(-1, 8 * 8 * 8)\n","        out = torch.tanh(self.fc1(out))\n","        out = self.BatchNorm1d_32(out)\n","        out = self.fc2(out)\n","        return out\n","\n","    # , nn.BatchNorm2d(6)\n","    # , nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n","    # , nn.ReLU()\n","    # , nn.MaxPool2d(kernel_size=2, stride=2)\n","    # , nn.Flatten(start_dim=1)\n","    # , nn.Linear(in_features=12*4*4, out_features=120)\n","    # , nn.ReLU()\n","    # , nn.BatchNorm1d(120)\n","\n","class CNNModel_hasBNLhasDropOut(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.network=nn.Sequential(\n","            nn.Conv2d(3,32,3,padding=1),\n","            nn.BatchNorm2d(32),#2D Batch Normalization since our inputs are 4D.\n","            nn.ReLU(),\n","            nn.MaxPool2d(2,2),\n","            nn.Dropout(0.25),\n","\n","            nn.Conv2d(32,64,3,padding=1),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(),\n","            nn.Dropout(0.25),\n","\n","            nn.Conv2d(64,128,3,padding=1),\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2,2),\n","            nn.Dropout(0.25),\n","\n","            nn.Conv2d(128,128,3,padding=1),\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(),\n","            nn.Dropout(0.25),\n","\n","            nn.Conv2d(128,128,3,padding=1),\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2,2),\n","            nn.Dropout(0.25),\n","\n","            nn.Flatten(),\n","            nn.Linear(128*4*4,512),\n","            nn.ReLU(),\n","            nn.Linear(512,256),\n","            nn.ReLU(),\n","            nn.Linear(256,10))\n","    def forward(self,x):\n","        return self.network(x)\n","\n","class CNNModel_noBNLhasDropOut(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.network=nn.Sequential(\n","            nn.Conv2d(3,32,3,padding=1),\n","            #nn.BatchNorm2d(32),#2D Batch Normalization since our inputs are 4D.\n","            nn.ReLU(),\n","            nn.MaxPool2d(2,2),\n","            nn.Dropout(0.25),\n","\n","            nn.Conv2d(32,64,3,padding=1),\n","            #nn.BatchNorm2d(64),\n","            nn.ReLU(),\n","            nn.Dropout(0.25),\n","\n","            nn.Conv2d(64,128,3,padding=1),\n","            #nn.BatchNorm2d(128),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2,2),\n","            nn.Dropout(0.25),\n","\n","            nn.Conv2d(128,128,3,padding=1),\n","            #nn.BatchNorm2d(128),\n","            nn.ReLU(),\n","            nn.Dropout(0.25),\n","\n","            nn.Conv2d(128,128,3,padding=1),\n","            #nn.BatchNorm2d(128),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2,2),\n","            nn.Dropout(0.25),\n","\n","            nn.Flatten(),\n","            nn.Linear(128*4*4,512),\n","            nn.ReLU(),\n","            nn.Linear(512,256),\n","            nn.ReLU(),\n","            nn.Linear(256,10))\n","    def forward(self,x):\n","        return self.network(x)\n","\n","\n","class CNNModel_noBNLnoDropOut(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.network=nn.Sequential(\n","            nn.Conv2d(3,32,3,padding=1),\n","            #nn.BatchNorm2d(32),#2D Batch Normalization since our inputs are 4D.\n","            nn.ReLU(),\n","            nn.MaxPool2d(2,2),\n","            #nn.Dropout(0.25),\n","\n","            nn.Conv2d(32,64,3,padding=1),\n","            #nn.BatchNorm2d(64),\n","            nn.ReLU(),\n","            #nn.Dropout(0.25),\n","\n","            nn.Conv2d(64,128,3,padding=1),\n","            #nn.BatchNorm2d(128),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2,2),\n","            #nn.Dropout(0.25),\n","\n","            nn.Conv2d(128,128,3,padding=1),\n","            #nn.BatchNorm2d(128),\n","            nn.ReLU(),\n","            #nn.Dropout(0.25),\n","\n","            nn.Conv2d(128,128,3,padding=1),\n","            #nn.BatchNorm2d(128),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2,2),\n","            #nn.Dropout(0.25),\n","\n","            nn.Flatten(),\n","            nn.Linear(128*4*4,512),\n","            nn.ReLU(),\n","            nn.Linear(512,256),\n","            nn.ReLU(),\n","            nn.Linear(256,10))\n","    def forward(self,x):\n","        return self.network(x)\n","\n","class CNNModel_hasBNLnoDropOut(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.network=nn.Sequential(\n","            nn.Conv2d(3,32,3,padding=1),\n","            nn.BatchNorm2d(32),#2D Batch Normalization since our inputs are 4D.\n","            nn.ReLU(),\n","            nn.MaxPool2d(2,2),\n","            #nn.Dropout(0.25),\n","\n","            nn.Conv2d(32,64,3,padding=1),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(),\n","            #nn.Dropout(0.25),\n","\n","            nn.Conv2d(64,128,3,padding=1),\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2,2),\n","            #nn.Dropout(0.25),\n","\n","            nn.Conv2d(128,128,3,padding=1),\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(),\n","            #nn.Dropout(0.25),\n","\n","            nn.Conv2d(128,128,3,padding=1),\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2,2),\n","            #nn.Dropout(0.25),\n","\n","            nn.Flatten(),\n","            nn.Linear(128*4*4,512),\n","            nn.ReLU(),\n","            nn.Linear(512,256),\n","            nn.ReLU(),\n","            nn.Linear(256,10))\n","    def forward(self,x):\n","        return self.network(x)\n","\n","\n","class BasicBlock_hasBNLnoDropOut(nn.Module):\n","    expansion = 1\n","\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(BasicBlock_hasBNLnoDropOut, self).__init__()\n","\n","        DROPOUT = 0.0  #zeros probability of DROPOUT -- noDropOut == 0.0\n","\n","        self.conv1 = nn.Conv2d(\n","            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.dropout = nn.Dropout(DROPOUT)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n","                               stride=1, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.dropout = nn.Dropout(DROPOUT)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes,\n","                          kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes),\n","                nn.Dropout(DROPOUT)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.dropout(self.bn1(self.conv1(x))))\n","        out = self.dropout(self.bn2(self.conv2(out)))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class ResNet_hasBNLnoDropOut(nn.Module):\n","    def __init__(self, block, num_blocks, num_classes=10):\n","        super(ResNet_hasBNLnoDropOut, self).__init__()\n","        self.in_planes = 64\n","\n","        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n","                               stride=1, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n","        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n","        self.linear = nn.Linear(512*block.expansion, num_classes)\n","\n","    def _make_layer(self, block, planes, num_blocks, stride):\n","        strides = [stride] + [1]*(num_blocks-1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(self.in_planes, planes, stride))\n","            self.in_planes = planes * block.expansion\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.layer1(out)\n","        out = self.layer2(out)\n","        out = self.layer3(out)\n","        out = self.layer4(out)\n","        out = F.avg_pool2d(out, 4)\n","        out = out.view(out.size(0), -1)\n","        out = self.linear(out)\n","        return F.log_softmax(out, dim=-1)\n","\n","\n","def ResNet18_hasBNLnoDropOut():\n","    return ResNet_hasBNLnoDropOut(BasicBlock_hasBNLnoDropOut, [2, 2, 2, 2])\n","\n","\n","###\n","class BasicBlock_noBNLnoDropOut(nn.Module):\n","    expansion = 1\n","\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(BasicBlock_noBNLnoDropOut, self).__init__()\n","\n","        DROPOUT = 0.0  #zeros probability of DROPOUT -- noDropOut == 0.0\n","\n","        self.conv1 = nn.Conv2d(\n","            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.dropout = nn.Dropout(DROPOUT)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n","                               stride=1, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.dropout = nn.Dropout(DROPOUT)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes,\n","                          kernel_size=1, stride=stride, bias=False)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.conv1(x))\n","        out = self.conv2(out)\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class ResNet_noBNLnoDropOut(nn.Module):\n","    def __init__(self, block, num_blocks, num_classes=10):\n","        super(ResNet_noBNLnoDropOut, self).__init__()\n","        self.in_planes = 64\n","\n","        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n","                               stride=1, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n","        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n","        self.linear = nn.Linear(512*block.expansion, num_classes)\n","\n","    def _make_layer(self, block, planes, num_blocks, stride):\n","        strides = [stride] + [1]*(num_blocks-1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(self.in_planes, planes, stride))\n","            self.in_planes = planes * block.expansion\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        out = F.relu(self.conv1(x))\n","        out = self.layer1(out)\n","        out = self.layer2(out)\n","        out = self.layer3(out)\n","        out = self.layer4(out)\n","        out = F.avg_pool2d(out, 4)\n","        out = out.view(out.size(0), -1)\n","        out = self.linear(out)\n","        return F.log_softmax(out, dim=-1)\n","\n","\n","def ResNet18_noBNLnoDropOut():\n","    return ResNet_noBNLnoDropOut(BasicBlock_noBNLnoDropOut, [2, 2, 2, 2])"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":193,"status":"ok","timestamp":1698071287575,"user":{"displayName":"Phuong Ha Nguyen","userId":"01448229299682072761"},"user_tz":420},"id":"0Pn9A9tujUnW"},"outputs":[],"source":["import pickle\n","\n","def training_loop(n_epochs, optimizer, model, loss_fn, train_loader, val_loader, data_path):\n","    for epoch in range(1, n_epochs + 1):\n","        loss_train = 0.0\n","        for imgs, labels in train_loader:\n","\n","            outputs = model(imgs)\n","            loss = loss_fn(outputs, labels)\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","            loss_train += loss.item()\n","        correct = 0\n","        with torch.no_grad():\n","            for data in val_loader:\n","                images, labels = data\n","                outputs = model(images)\n","                _, predicted = torch.max(outputs, 1)\n","                c = (predicted == labels).squeeze()\n","                correct += c.sum()\n","        if epoch == 1 or epoch % 1 == 0:\n","            print('Epoch {}, Training loss {}, Val accuracy {}'.format(\n","                epoch,\n","                loss_train / len(train_loader),\n","                correct / len(cifar10_val)))\n","\n","        scheduler.step()\n","        # before_lr = optimizer.param_groups[0][\"lr\"]\n","        # scheduler.step()\n","        # after_lr = optimizer.param_groups[0][\"lr\"]\n","        # print(\"Epoch %d: SGD lr %.4f -> %.4f\" % (epoch, before_lr, after_lr))\n","\n","\n","        #save the model config\n","        model_state = model.state_dict()\n","        optimizer_state = optimizer.state_dict()\n","        scheduler_state = scheduler.state_dict()\n","        dict_state = dict()\n","        dict_state[\"epoch\"] = epoch\n","        dict_state[\"model_state\"] = model_state\n","        dict_state[\"optimizer_state\"] = optimizer_state\n","        dict_state[\"scheduler_state\"] = scheduler_state\n","        dict_state[\"train_loss\"] = loss_train / len(train_loader)\n","        dict_state[\"val_acc\"] = correct / len(cifar10_val)\n","\n","        try:\n","            geeky_file = open(data_path + \"epoch_\" + str(epoch), 'wb')\n","            pickle.dump(dict_state, geeky_file)\n","            geeky_file.close()\n","\n","        except:\n","            print(\"Something went wrong\")\n","\n","        #print(f\"scheduler state: {scheduler_state}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TnZGTdzCxxyu"},"outputs":[],"source":["configs = dict({\n","#\"1\": { \"model_type\": Net(), \"n_epochs\": 20, \"lr_initial\": 0.1, \"decay\": 0.9, \"batch_size\": 64},\n","#\"2\": { \"model_type\": NetBNL(), \"n_epochs\": 20, \"lr_initial\": 0.1, \"decay\": 0.9, \"batch_size\": 64},\n","#\"3\": { \"model_type\": CNNModel_hasBNLhasDropOut(), \"n_epochs\": 20, \"lr_initial\": 0.1, \"decay\": 0.9, \"batch_size\": 64},\n","#\"4\": { \"model_type\": CNNModel_noBNLhasDropOut(), \"n_epochs\": 10, \"lr_initial\": 0.1, \"decay\": 0.9, \"batch_size\": 64},\n","#\"5\": { \"model_type\": CNNModel_noBNLnoDropOut(), \"n_epochs\": 10, \"lr_initial\": 0.1, \"decay\": 0.9, \"batch_size\": 64},\n","#\"6\": { \"model_type\": CNNModel_hasBNLnoDropOut(), \"n_epochs\": 10, \"lr_initial\": 0.1, \"decay\": 0.9, \"batch_size\": 64},\n","\"7\": { \"model_type\": ResNet18_hasBNLnoDropOut(),\"n_epochs\": 10, \"lr_initial\": 0.1, \"decay\": 0.9, \"batch_size\": 64},\n","\"8\": { \"model_type\": ResNet18_noBNLnoDropOut(),\"n_epochs\": 10, \"lr_initial\": 0.1, \"decay\": 0.9, \"batch_size\": 64}\n","})\n","\n","\n","for index, config in configs.items():\n","  print(f\"index: {index}\")\n","  print(f\"config: {config}\")\n","  for key,value in config.items():\n","    print(f\"key: {key}, value: {value}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"auhHQgx8jcbV","outputId":"40d691d9-75c8-44d9-dcbf-cfd5c5baf6fe"},"outputs":[{"output_type":"stream","name":"stdout","text":["index: 7\n","Epoch 1, Training loss 1.470175827631865, Val accuracy 0.5629000067710876\n","Epoch 2, Training loss 0.8520316731975511, Val accuracy 0.6366999745368958\n"]}],"source":["for index, config in configs.items():\n","  print(f\"index: {index}\")\n","  data_path_index = data_path + \"config_\" + str(index) + \"_\"\n","  model = config[\"model_type\"]\n","  optimizer = optim.SGD(model.parameters(), lr=config[\"lr_initial\"])\n","  loss_fn = nn.CrossEntropyLoss()\n","\n","  train_loader = torch.utils.data.DataLoader(cifar10, batch_size=config[\"batch_size\"], shuffle=True)\n","  val_loader = torch.utils.data.DataLoader(cifar10_val, batch_size=config[\"batch_size\"], shuffle=True)\n","\n","  scheduler = lr_scheduler.StepLR(optimizer, step_size=1, gamma=config[\"decay\"])\n","\n","  training_loop(\n","      n_epochs = config[\"n_epochs\"],\n","      optimizer = optimizer,\n","      model = model,\n","      loss_fn = loss_fn,\n","      train_loader = train_loader,\n","      val_loader = val_loader,\n","      data_path = data_path_index\n","  )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZVcYdYDKyFOX"},"outputs":[],"source":["\n","\n","  '''\n","    LinearLR =>> new LR = initial LR - nb_epochs*(start_factor-end_factor)/total_iters\n","    example, initialLR = 0.1, start = 1.0, end_factor = 0.5, total_iters = 20\n","    (start_factor-end_factor)/total_iters = 0.025.\n","    ===> epoch 1: 0.1 - 1*0.025 = 0.0975\n","    ===> epoch 2: 0.1 - 2*0.025 = 0.0950....\n","  '''\n","  #scheduler = lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.5, total_iters=20)\n","  '''\n","   StepLR =>>> new LR = old LR * gamma\n","  '''\n","  scheduler = lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n","  '''\n","    LambdaLR: new LR = initialLR * f(epoch)\n","    For example: f(epoch) = 1/t\n","  '''\n","  # lambda1 = lambda epoch: 1/(epoch+1)\n","  # scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=[lambda1])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":574,"status":"ok","timestamp":1696992505070,"user":{"displayName":"Phuong Ha Nguyen","userId":"01448229299682072761"},"user_tz":420},"id":"IkrX1-y8oABE","outputId":"d1d8326c-52d1-4a2f-d0ab-d31f8204867d"},"outputs":[{"name":"stdout","output_type":"stream","text":["dict_keys(['epoch', 'model_state', 'optimizer_state'])\n"]}],"source":["import pandas as pd\n","epoch = 1\n","path = data_path + \"epoch_\" + str(epoch)\n","obj = pd.read_pickle(path)\n","print(obj.keys())"]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN3j5Jam5mrxAGsRUYViRbX"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}