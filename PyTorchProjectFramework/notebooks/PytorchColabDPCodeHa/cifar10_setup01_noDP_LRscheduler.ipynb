{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM8oH6ML/PCpb8/kUnaDAlR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r7Rl1QFk5icv","executionInfo":{"status":"ok","timestamp":1697302219542,"user_tz":420,"elapsed":20939,"user":{"displayName":"Phuong Ha Nguyen","userId":"01448229299682072761"}},"outputId":"c36d6485-ef83-4e83-ddc1-3b144e58cae3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/Hadrive\n"]}],"source":["# First we need to mount the Google drive\n","import os\n","from google.colab import drive\n","drive.mount('/content/Hadrive')"]},{"cell_type":"code","source":["\n","configs = dict({\n","\"1\": { \"lr_initial\": 0.1, \"decay\": 0.9}\n","})\n","\n","\n","for index, config in configs.items():\n","  print(f\"index: {index}\")\n","  print(f\"config: {config}\")\n","  for key,value in config.items():\n","    print(f\"key: {key}, value: {value}\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9r5K3CJU236g","executionInfo":{"status":"ok","timestamp":1697302296827,"user_tz":420,"elapsed":220,"user":{"displayName":"Phuong Ha Nguyen","userId":"01448229299682072761"}},"outputId":"af1764f5-6c7c-47d9-daca-41d1fc29c766"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["index: 1\n","config: {'lr_initial': 0.1, 'decay': 0.9}\n","key: lr_initial, value: 0.1\n","key: decay, value: 0.9\n"]}]},{"cell_type":"code","source":["#!mkdir /content/Hadrive/MyDrive/Test1\n","#!mkdir /content/Hadrive/MyDrive/Test1/Tutorial1/\n","df.to_csv(\"/content/Hadrive/MyDrive/Test1/Tutorial1/dframe.csv\")"],"metadata":{"id":"q8wxEMaM3C59"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torch.optim as optim\n","import torch.optim.lr_scheduler as lr_scheduler\n","# import torch.optim.lr_scheduler.StepLR as StepLR\n","# import torch.optim.lr_scheduler.LinearLR as LinearLR\n","\n","\n","\n","from torchvision import datasets\n","from torchvision import transforms\n","import matplotlib.pyplot as plt"],"metadata":{"id":"vkpkT6pfieuj","executionInfo":{"status":"ok","timestamp":1697302532003,"user_tz":420,"elapsed":611,"user":{"displayName":"Phuong Ha Nguyen","userId":"01448229299682072761"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["data_path = '/content/Hadrive/MyDrive/Test1/Tutorial1/'\n","\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.4915, 0.4823, 0.4468), (0.2470, 0.2435, 0.2616))\n","])\n","\n","# cifar10 = datasets.CIFAR10(data_path, train=True, download=True, transform=transform)\n","# cifar10_val = datasets.CIFAR10(data_path, train=False, download=True, transform=transform)\n","\n","\n","#https://stackoverflow.com/questions/47432168/taking-subsets-of-a-pytorch-dataset\n","cifar10_org = datasets.CIFAR10(data_path, train=True, download=False, transform=transform)\n","#subset_org = list(range(0, len(cifar10_org), len(cifar10_org)//100))\n","subset_org = list(range(0, 100))\n","cifar10 = torch.utils.data.Subset(cifar10_org, subset_org)\n","\n","cifar10_val_org = datasets.CIFAR10(data_path, train=False, download=False, transform=transform)\n","#subset_org = list(range(0, len(cifar10_val_org), len(cifar10_val_org)//100))\n","subset_org = list(range(0, 100))\n","cifar10_val = torch.utils.data.Subset(cifar10_val_org, subset_org)\n","\n","print(f\"lencifar10: {len(cifar10)}\")\n","print(f\"lencifar10_val: {len(cifar10_val)}\")"],"metadata":{"id":"j0_-UJk0izg6","executionInfo":{"status":"ok","timestamp":1697303480010,"user_tz":420,"elapsed":1883,"user":{"displayName":"Phuong Ha Nguyen","userId":"01448229299682072761"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"f5f67cf8-dc73-4d2a-f06a-b5f3117bd068"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["lencifar10: 100\n","lencifar10_val: 100\n"]}]},{"cell_type":"code","source":["train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64,\n","                                           shuffle=True)\n","val_loader = torch.utils.data.DataLoader(cifar10_val, batch_size=64,\n","                                           shuffle=True)"],"metadata":{"id":"i4W2GOsdjK72","executionInfo":{"status":"ok","timestamp":1697303497412,"user_tz":420,"elapsed":172,"user":{"displayName":"Phuong Ha Nguyen","userId":"01448229299682072761"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["# model\n","class Net(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n","        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n","        self.fc1 = nn.Linear(8 * 8 * 8, 32)\n","        # bài toán phân loại 10 lớp nên output ra 10 nodes\n","        self.fc2 = nn.Linear(32, 10)\n","\n","    def forward(self, x):\n","        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n","        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n","        # flatten về dạng vector để cho vào neural network\n","        out = out.view(-1, 8 * 8 * 8)\n","        out = torch.tanh(self.fc1(out))\n","        out = self.fc2(out)\n","        return out\n"],"metadata":{"id":"iy4nacdKjQGu","executionInfo":{"status":"ok","timestamp":1697303505780,"user_tz":420,"elapsed":186,"user":{"displayName":"Phuong Ha Nguyen","userId":"01448229299682072761"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["import pickle\n","\n","def training_loop(n_epochs, optimizer, model, loss_fn, train_loader, val_loader, data_path):\n","    for epoch in range(1, n_epochs + 1):\n","        loss_train = 0.0\n","        for imgs, labels in train_loader:\n","\n","            outputs = model(imgs)\n","            loss = loss_fn(outputs, labels)\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","            loss_train += loss.item()\n","        correct = 0\n","        with torch.no_grad():\n","            for data in val_loader:\n","                images, labels = data\n","                outputs = model(images)\n","                _, predicted = torch.max(outputs, 1)\n","                c = (predicted == labels).squeeze()\n","                correct += c.sum()\n","        if epoch == 1 or epoch % 1 == 0:\n","            print('Epoch {}, Training loss {}, Val accuracy {}'.format(\n","                epoch,\n","                loss_train / len(train_loader),\n","                correct / len(cifar10_val)))\n","\n","        before_lr = optimizer.param_groups[0][\"lr\"]\n","        scheduler.step()\n","        after_lr = optimizer.param_groups[0][\"lr\"]\n","        print(\"Epoch %d: SGD lr %.4f -> %.4f\" % (epoch, before_lr, after_lr))\n","\n","\n","        #save the model config\n","        model_state = model.state_dict()\n","        optimizer_state = optimizer.state_dict()\n","        scheduler_state = scheduler.state_dict()\n","        dict_state = dict()\n","        dict_state[\"epoch\"] = epoch\n","        dict_state[\"model_state\"] = model_state\n","        dict_state[\"optimizer_state\"] = optimizer_state\n","        dict_state[\"scheduler_state\"] = scheduler_state\n","        dict_state[\"train_loss\"] = loss_train / len(train_loader)\n","        dict_state[\"val_acc\"] = correct / len(cifar10_val)\n","\n","        try:\n","            geeky_file = open(data_path + \"epoch_\" + str(epoch), 'wb')\n","            pickle.dump(dict_state, geeky_file)\n","            geeky_file.close()\n","\n","        except:\n","            print(\"Something went wrong\")\n","\n","        print(f\"scheduler state: {scheduler_state}\")"],"metadata":{"id":"0Pn9A9tujUnW","executionInfo":{"status":"ok","timestamp":1697308985562,"user_tz":420,"elapsed":191,"user":{"displayName":"Phuong Ha Nguyen","userId":"01448229299682072761"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["for index, config in configs.items():\n","  print(f\"index: {index}\")\n","  data_path_index = data_path + \"config_\" + str(index) + \"_\"\n","  model = Net()\n","  optimizer = optim.SGD(model.parameters(), lr=config[\"lr_initial\"])\n","  loss_fn = nn.CrossEntropyLoss()\n","  '''\n","    LinearLR =>> new LR = initial LR - nb_epochs*(start_factor-end_factor)/total_iters\n","    example, initialLR = 0.1, start = 1.0, end_factor = 0.5, total_iters = 20\n","    (start_factor-end_factor)/total_iters = 0.025.\n","    ===> epoch 1: 0.1 - 1*0.025 = 0.0975\n","    ===> epoch 2: 0.1 - 2*0.025 = 0.0950....\n","  '''\n","  scheduler = lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.5, total_iters=20)\n","  '''\n","   StepLR =>>> new LR = old LR * gamma\n","  '''\n","  #scheduler = lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n","  '''\n","    LambdaLR: new LR = initialLR * f(epoch)\n","    For example: f(epoch) = 1/t\n","  '''\n","  # lambda1 = lambda epoch: 1/(epoch+1)\n","  # scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=[lambda1])\n","\n","  training_loop(\n","      n_epochs = 2,\n","      optimizer = optimizer,\n","      model = model,\n","      loss_fn = loss_fn,\n","      train_loader = train_loader,\n","      val_loader = val_loader,\n","      data_path = data_path_index\n","  )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"auhHQgx8jcbV","executionInfo":{"status":"ok","timestamp":1697314879278,"user_tz":420,"elapsed":4333,"user":{"displayName":"Phuong Ha Nguyen","userId":"01448229299682072761"}},"outputId":"4405ee50-2a0e-439e-f237-e8da900099a9"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["index: 1\n","Epoch 1, Training loss 2.3269673585891724, Val accuracy 0.15000000596046448\n","Epoch 1: SGD lr 0.1000 -> 0.0975\n","scheduler state: {'start_factor': 1.0, 'end_factor': 0.5, 'total_iters': 20, 'base_lrs': [0.1], 'last_epoch': 1, 'verbose': False, '_step_count': 2, '_get_lr_called_within_step': False, '_last_lr': [0.0975]}\n","Epoch 2, Training loss 2.282986879348755, Val accuracy 0.10999999940395355\n","Epoch 2: SGD lr 0.0975 -> 0.0950\n","scheduler state: {'start_factor': 1.0, 'end_factor': 0.5, 'total_iters': 20, 'base_lrs': [0.1], 'last_epoch': 2, 'verbose': False, '_step_count': 3, '_get_lr_called_within_step': False, '_last_lr': [0.095]}\n","Epoch 3, Training loss 2.24941623210907, Val accuracy 0.12999999523162842\n","Epoch 3: SGD lr 0.0950 -> 0.0925\n","scheduler state: {'start_factor': 1.0, 'end_factor': 0.5, 'total_iters': 20, 'base_lrs': [0.1], 'last_epoch': 3, 'verbose': False, '_step_count': 4, '_get_lr_called_within_step': False, '_last_lr': [0.0925]}\n","Epoch 4, Training loss 2.208550453186035, Val accuracy 0.10999999940395355\n","Epoch 4: SGD lr 0.0925 -> 0.0900\n","scheduler state: {'start_factor': 1.0, 'end_factor': 0.5, 'total_iters': 20, 'base_lrs': [0.1], 'last_epoch': 4, 'verbose': False, '_step_count': 5, '_get_lr_called_within_step': False, '_last_lr': [0.09000000000000001]}\n","Epoch 5, Training loss 2.16526460647583, Val accuracy 0.10000000149011612\n","Epoch 5: SGD lr 0.0900 -> 0.0875\n","scheduler state: {'start_factor': 1.0, 'end_factor': 0.5, 'total_iters': 20, 'base_lrs': [0.1], 'last_epoch': 5, 'verbose': False, '_step_count': 6, '_get_lr_called_within_step': False, '_last_lr': [0.08750000000000001]}\n","Epoch 6, Training loss 2.130734443664551, Val accuracy 0.09000000357627869\n","Epoch 6: SGD lr 0.0875 -> 0.0850\n","scheduler state: {'start_factor': 1.0, 'end_factor': 0.5, 'total_iters': 20, 'base_lrs': [0.1], 'last_epoch': 6, 'verbose': False, '_step_count': 7, '_get_lr_called_within_step': False, '_last_lr': [0.085]}\n","Epoch 7, Training loss 2.065089464187622, Val accuracy 0.09000000357627869\n","Epoch 7: SGD lr 0.0850 -> 0.0825\n","scheduler state: {'start_factor': 1.0, 'end_factor': 0.5, 'total_iters': 20, 'base_lrs': [0.1], 'last_epoch': 7, 'verbose': False, '_step_count': 8, '_get_lr_called_within_step': False, '_last_lr': [0.0825]}\n","Epoch 8, Training loss 2.0480107069015503, Val accuracy 0.07999999821186066\n","Epoch 8: SGD lr 0.0825 -> 0.0800\n","scheduler state: {'start_factor': 1.0, 'end_factor': 0.5, 'total_iters': 20, 'base_lrs': [0.1], 'last_epoch': 8, 'verbose': False, '_step_count': 9, '_get_lr_called_within_step': False, '_last_lr': [0.08]}\n","Epoch 9, Training loss 2.0106601119041443, Val accuracy 0.10000000149011612\n","Epoch 9: SGD lr 0.0800 -> 0.0775\n","scheduler state: {'start_factor': 1.0, 'end_factor': 0.5, 'total_iters': 20, 'base_lrs': [0.1], 'last_epoch': 9, 'verbose': False, '_step_count': 10, '_get_lr_called_within_step': False, '_last_lr': [0.0775]}\n","Epoch 10, Training loss 1.9651480317115784, Val accuracy 0.10000000149011612\n","Epoch 10: SGD lr 0.0775 -> 0.0750\n","scheduler state: {'start_factor': 1.0, 'end_factor': 0.5, 'total_iters': 20, 'base_lrs': [0.1], 'last_epoch': 10, 'verbose': False, '_step_count': 11, '_get_lr_called_within_step': False, '_last_lr': [0.075]}\n","Epoch 11, Training loss 1.951348602771759, Val accuracy 0.10000000149011612\n","Epoch 11: SGD lr 0.0750 -> 0.0725\n","scheduler state: {'start_factor': 1.0, 'end_factor': 0.5, 'total_iters': 20, 'base_lrs': [0.1], 'last_epoch': 11, 'verbose': False, '_step_count': 12, '_get_lr_called_within_step': False, '_last_lr': [0.0725]}\n","Epoch 12, Training loss 1.9124661684036255, Val accuracy 0.11999999731779099\n","Epoch 12: SGD lr 0.0725 -> 0.0700\n","scheduler state: {'start_factor': 1.0, 'end_factor': 0.5, 'total_iters': 20, 'base_lrs': [0.1], 'last_epoch': 12, 'verbose': False, '_step_count': 13, '_get_lr_called_within_step': False, '_last_lr': [0.06999999999999999]}\n","Epoch 13, Training loss 1.873761534690857, Val accuracy 0.11999999731779099\n","Epoch 13: SGD lr 0.0700 -> 0.0675\n","scheduler state: {'start_factor': 1.0, 'end_factor': 0.5, 'total_iters': 20, 'base_lrs': [0.1], 'last_epoch': 13, 'verbose': False, '_step_count': 14, '_get_lr_called_within_step': False, '_last_lr': [0.06749999999999999]}\n","Epoch 14, Training loss 1.8465393781661987, Val accuracy 0.11999999731779099\n","Epoch 14: SGD lr 0.0675 -> 0.0650\n","scheduler state: {'start_factor': 1.0, 'end_factor': 0.5, 'total_iters': 20, 'base_lrs': [0.1], 'last_epoch': 14, 'verbose': False, '_step_count': 15, '_get_lr_called_within_step': False, '_last_lr': [0.06499999999999999]}\n","Epoch 15, Training loss 1.813804566860199, Val accuracy 0.10000000149011612\n","Epoch 15: SGD lr 0.0650 -> 0.0625\n","scheduler state: {'start_factor': 1.0, 'end_factor': 0.5, 'total_iters': 20, 'base_lrs': [0.1], 'last_epoch': 15, 'verbose': False, '_step_count': 16, '_get_lr_called_within_step': False, '_last_lr': [0.06249999999999999]}\n","Epoch 16, Training loss 1.773470401763916, Val accuracy 0.10999999940395355\n","Epoch 16: SGD lr 0.0625 -> 0.0600\n","scheduler state: {'start_factor': 1.0, 'end_factor': 0.5, 'total_iters': 20, 'base_lrs': [0.1], 'last_epoch': 16, 'verbose': False, '_step_count': 17, '_get_lr_called_within_step': False, '_last_lr': [0.05999999999999999]}\n","Epoch 17, Training loss 1.7603300213813782, Val accuracy 0.10999999940395355\n","Epoch 17: SGD lr 0.0600 -> 0.0575\n","scheduler state: {'start_factor': 1.0, 'end_factor': 0.5, 'total_iters': 20, 'base_lrs': [0.1], 'last_epoch': 17, 'verbose': False, '_step_count': 18, '_get_lr_called_within_step': False, '_last_lr': [0.057499999999999996]}\n","Epoch 18, Training loss 1.7149469256401062, Val accuracy 0.11999999731779099\n","Epoch 18: SGD lr 0.0575 -> 0.0550\n","scheduler state: {'start_factor': 1.0, 'end_factor': 0.5, 'total_iters': 20, 'base_lrs': [0.1], 'last_epoch': 18, 'verbose': False, '_step_count': 19, '_get_lr_called_within_step': False, '_last_lr': [0.055]}\n","Epoch 19, Training loss 1.7094306945800781, Val accuracy 0.14000000059604645\n","Epoch 19: SGD lr 0.0550 -> 0.0525\n","scheduler state: {'start_factor': 1.0, 'end_factor': 0.5, 'total_iters': 20, 'base_lrs': [0.1], 'last_epoch': 19, 'verbose': False, '_step_count': 20, '_get_lr_called_within_step': False, '_last_lr': [0.052500000000000005]}\n","Epoch 20, Training loss 1.6713788509368896, Val accuracy 0.11999999731779099\n","Epoch 20: SGD lr 0.0525 -> 0.0500\n","scheduler state: {'start_factor': 1.0, 'end_factor': 0.5, 'total_iters': 20, 'base_lrs': [0.1], 'last_epoch': 20, 'verbose': False, '_step_count': 21, '_get_lr_called_within_step': False, '_last_lr': [0.05]}\n","Epoch 21, Training loss 1.6635726690292358, Val accuracy 0.11999999731779099\n","Epoch 21: SGD lr 0.0500 -> 0.0500\n","scheduler state: {'start_factor': 1.0, 'end_factor': 0.5, 'total_iters': 20, 'base_lrs': [0.1], 'last_epoch': 21, 'verbose': False, '_step_count': 22, '_get_lr_called_within_step': False, '_last_lr': [0.05]}\n","Epoch 22, Training loss 1.651675522327423, Val accuracy 0.14000000059604645\n","Epoch 22: SGD lr 0.0500 -> 0.0500\n","scheduler state: {'start_factor': 1.0, 'end_factor': 0.5, 'total_iters': 20, 'base_lrs': [0.1], 'last_epoch': 22, 'verbose': False, '_step_count': 23, '_get_lr_called_within_step': False, '_last_lr': [0.05]}\n","Epoch 23, Training loss 1.6064268350601196, Val accuracy 0.14000000059604645\n","Epoch 23: SGD lr 0.0500 -> 0.0500\n","scheduler state: {'start_factor': 1.0, 'end_factor': 0.5, 'total_iters': 20, 'base_lrs': [0.1], 'last_epoch': 23, 'verbose': False, '_step_count': 24, '_get_lr_called_within_step': False, '_last_lr': [0.05]}\n","Epoch 24, Training loss 1.5997296571731567, Val accuracy 0.14000000059604645\n","Epoch 24: SGD lr 0.0500 -> 0.0500\n","scheduler state: {'start_factor': 1.0, 'end_factor': 0.5, 'total_iters': 20, 'base_lrs': [0.1], 'last_epoch': 24, 'verbose': False, '_step_count': 25, '_get_lr_called_within_step': False, '_last_lr': [0.05]}\n","Epoch 25, Training loss 1.5466747879981995, Val accuracy 0.14000000059604645\n","Epoch 25: SGD lr 0.0500 -> 0.0500\n","scheduler state: {'start_factor': 1.0, 'end_factor': 0.5, 'total_iters': 20, 'base_lrs': [0.1], 'last_epoch': 25, 'verbose': False, '_step_count': 26, '_get_lr_called_within_step': False, '_last_lr': [0.05]}\n","Epoch 26, Training loss 1.5527207851409912, Val accuracy 0.14000000059604645\n","Epoch 26: SGD lr 0.0500 -> 0.0500\n","scheduler state: {'start_factor': 1.0, 'end_factor': 0.5, 'total_iters': 20, 'base_lrs': [0.1], 'last_epoch': 26, 'verbose': False, '_step_count': 27, '_get_lr_called_within_step': False, '_last_lr': [0.05]}\n","Epoch 27, Training loss 1.5405749678611755, Val accuracy 0.14000000059604645\n","Epoch 27: SGD lr 0.0500 -> 0.0500\n","scheduler state: {'start_factor': 1.0, 'end_factor': 0.5, 'total_iters': 20, 'base_lrs': [0.1], 'last_epoch': 27, 'verbose': False, '_step_count': 28, '_get_lr_called_within_step': False, '_last_lr': [0.05]}\n","Epoch 28, Training loss 1.491086184978485, Val accuracy 0.1599999964237213\n","Epoch 28: SGD lr 0.0500 -> 0.0500\n","scheduler state: {'start_factor': 1.0, 'end_factor': 0.5, 'total_iters': 20, 'base_lrs': [0.1], 'last_epoch': 28, 'verbose': False, '_step_count': 29, '_get_lr_called_within_step': False, '_last_lr': [0.05]}\n","Epoch 29, Training loss 1.48628830909729, Val accuracy 0.1599999964237213\n","Epoch 29: SGD lr 0.0500 -> 0.0500\n","scheduler state: {'start_factor': 1.0, 'end_factor': 0.5, 'total_iters': 20, 'base_lrs': [0.1], 'last_epoch': 29, 'verbose': False, '_step_count': 30, '_get_lr_called_within_step': False, '_last_lr': [0.05]}\n","Epoch 30, Training loss 1.467682123184204, Val accuracy 0.17000000178813934\n","Epoch 30: SGD lr 0.0500 -> 0.0500\n","scheduler state: {'start_factor': 1.0, 'end_factor': 0.5, 'total_iters': 20, 'base_lrs': [0.1], 'last_epoch': 30, 'verbose': False, '_step_count': 31, '_get_lr_called_within_step': False, '_last_lr': [0.05]}\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","epoch = 1\n","path = data_path + \"epoch_\" + str(epoch)\n","obj = pd.read_pickle(path)\n","print(obj.keys())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IkrX1-y8oABE","executionInfo":{"status":"ok","timestamp":1696992505070,"user_tz":420,"elapsed":574,"user":{"displayName":"Phuong Ha Nguyen","userId":"01448229299682072761"}},"outputId":"d1d8326c-52d1-4a2f-d0ab-d31f8204867d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["dict_keys(['epoch', 'model_state', 'optimizer_state'])\n"]}]}]}