{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP3PjCgEo5PyWaBPRUfiZZT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"r7Rl1QFk5icv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697641951964,"user_tz":420,"elapsed":23966,"user":{"displayName":"Phuong Ha Nguyen","userId":"01448229299682072761"}},"outputId":"33556912-6f1f-49f8-d5c9-40e3ad1e152c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/Hadrive\n"]}],"source":["# First we need to mount the Google drive\n","import os\n","from google.colab import drive\n","drive.mount('/content/Hadrive')"]},{"cell_type":"code","source":["\n","configs = dict({\n","\"1\": { \"lr_initial\": 0.1, \"decay\": 0.9, \"sigma\": 0.00000001, \"const_C\": 1000}\n","})\n","\n","\n","for index, config in configs.items():\n","  print(f\"index: {index}\")\n","  print(f\"config: {config}\")\n","  for key,value in config.items():\n","    print(f\"key: {key}, value: {value}\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9r5K3CJU236g","executionInfo":{"status":"ok","timestamp":1697641954567,"user_tz":420,"elapsed":529,"user":{"displayName":"Phuong Ha Nguyen","userId":"01448229299682072761"}},"outputId":"068d69ed-2ee4-4b10-a1a3-eb3a6d89c522"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["index: 1\n","config: {'lr_initial': 0.1, 'decay': 0.9, 'sigma': 1e-08, 'const_C': 1000}\n","key: lr_initial, value: 0.1\n","key: decay, value: 0.9\n","key: sigma, value: 1e-08\n","key: const_C, value: 1000\n"]}]},{"cell_type":"code","source":["#!mkdir /content/Hadrive/MyDrive/Test1\n","#!mkdir /content/Hadrive/MyDrive/Test1/Tutorial1/"],"metadata":{"id":"q8wxEMaM3C59"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torch.optim as optim\n","import torch.optim.lr_scheduler as lr_scheduler\n","# import torch.optim.lr_scheduler.StepLR as StepLR\n","# import torch.optim.lr_scheduler.LinearLR as LinearLR\n","\n","\n","\n","from torchvision import datasets\n","from torchvision import transforms\n","import matplotlib.pyplot as plt"],"metadata":{"id":"vkpkT6pfieuj","executionInfo":{"status":"ok","timestamp":1697641964390,"user_tz":420,"elapsed":5004,"user":{"displayName":"Phuong Ha Nguyen","userId":"01448229299682072761"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["data_path = '/content/Hadrive/MyDrive/Test1/Tutorial1/'\n","\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.4915, 0.4823, 0.4468), (0.2470, 0.2435, 0.2616))\n","])\n","\n","# cifar10 = datasets.CIFAR10(data_path, train=True, download=True, transform=transform)\n","# cifar10_val = datasets.CIFAR10(data_path, train=False, download=True, transform=transform)\n","\n","nbsamples = 100000\n","\n","#https://stackoverflow.com/questions/47432168/taking-subsets-of-a-pytorch-dataset\n","cifar10_org = datasets.CIFAR10(data_path, train=True, download=False, transform=transform)\n","#subset_org = list(range(0, len(cifar10_org), len(cifar10_org)//100))\n","subset_org = list(range(0, nbsamples))\n","#cifar10 = torch.utils.data.Subset(cifar10_org, subset_org)\n","cifar10 = cifar10_org\n","\n","cifar10_val_org = datasets.CIFAR10(data_path, train=False, download=False, transform=transform)\n","#subset_org = list(range(0, len(cifar10_val_org), len(cifar10_val_org)//100))\n","subset_org = list(range(0, nbsamples))\n","#cifar10_val = torch.utils.data.Subset(cifar10_val_org, subset_org)\n","cifar10_val = cifar10_val_org\n","\n","print(f\"lencifar10: {len(cifar10)}\")\n","print(f\"lencifar10_val: {len(cifar10_val)}\")"],"metadata":{"id":"j0_-UJk0izg6","executionInfo":{"status":"ok","timestamp":1697641986087,"user_tz":420,"elapsed":13958,"user":{"displayName":"Phuong Ha Nguyen","userId":"01448229299682072761"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"dcd5d12a-f768-4f35-c4a1-7d78c2586988"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["lencifar10: 50000\n","lencifar10_val: 10000\n"]}]},{"cell_type":"code","source":["train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64,\n","                                           shuffle=True)\n","val_loader = torch.utils.data.DataLoader(cifar10_val, batch_size=64,\n","                                           shuffle=True)"],"metadata":{"id":"i4W2GOsdjK72","executionInfo":{"status":"ok","timestamp":1697641989652,"user_tz":420,"elapsed":328,"user":{"displayName":"Phuong Ha Nguyen","userId":"01448229299682072761"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# model\n","class Net(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n","        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n","        self.fc1 = nn.Linear(8 * 8 * 8, 32)\n","        # bài toán phân loại 10 lớp nên output ra 10 nodes\n","        self.fc2 = nn.Linear(32, 10)\n","\n","    def forward(self, x):\n","        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n","        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n","        # flatten về dạng vector để cho vào neural network\n","        out = out.view(-1, 8 * 8 * 8)\n","        out = torch.tanh(self.fc1(out))\n","        out = self.fc2(out)\n","        return out\n"],"metadata":{"id":"iy4nacdKjQGu","executionInfo":{"status":"ok","timestamp":1697641992797,"user_tz":420,"elapsed":337,"user":{"displayName":"Phuong Ha Nguyen","userId":"01448229299682072761"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["import pickle\n","\n","import torch\n","import torchvision\n","import torch.nn as nn\n","import numpy as np\n","import torchvision.transforms as transforms\n","from collections import OrderedDict\n","from collections import defaultdict\n","from torch.func import functional_call, vmap, grad\n","\n","def compute_loss(params, buffers, model, loss_fn, sample, target):\n","    batch = sample.unsqueeze(0)\n","    targets = target.unsqueeze(0)\n","\n","    predictions = functional_call(model, (params, buffers), (batch,))\n","    loss = loss_fn(predictions, targets)\n","    return loss\n","\n","def compute_gradients(model,loss_fn,samples,targets):\n","    '''\n","        We want to follow the tutorial in here to compute multiple grads in parallel:\n","                #https://pytorch.org/tutorials/intermediate/per_sample_grads.html?utm_source=whats_new_tutorials&utm_medium=per_sample_grads\n","                #https://pytorch.org/docs/stable/generated/torch.func.grad.html\n","                #https://towardsdatascience.com/introduction-to-functional-pytorch-b5bf739e1e6e\n","                #https://pytorch.org/functorch/stable/notebooks/per_sample_grads.html [PAY ATTENTION TO]\n","            Typically, we generate all gradients gis of samples sis in parallel in the helper function: compute_gradients\n","            The output of compute_gradients is an array called samples_grads\n","                sample s[0]: samples_grads[0][layer_1], samples_grads[0][layer_2], .... //g0\n","                    ...............\n","                sample s[L-1]: samples_grads[L-1][layer_1], samples_grads[L-1][layer_2], ....//g[L-1]\n","                where L is the number of samples in the mini-batch\n","\n","            The compute_gradients call another helper function called compute_loss. This is used for computing the gradients in parallel\n","    '''\n","\n","    ft_compute_grad = grad(compute_loss)\n","    ft_compute_sample_grad = vmap(ft_compute_grad, in_dims=(None, None, None, None, 0, 0))\n","\n","    '''\n","    The ft_compute_grad function computes the gradient for a single (sample, target) pair.\n","    We can use vmap to get it to compute the gradient over an entire batch of samples and targets.\n","    Note that in_dims=(None, None, 0, 0) because we wish to\n","    map ft_compute_grad over the 0th dimension of the data and targets, and use the same params and buffers for each.\n","    '''\n","\n","    params = {k: v.detach() for k, v in model.named_parameters()}\n","    buffers = {k: v.detach() for k, v in model.named_buffers()}\n","\n","    ft_per_sample_grads = ft_compute_sample_grad(params, buffers,model, loss_fn,samples,targets)\n","\n","    '''\n","    ft_per_sample_grads contains the STACKED gradients per layer.\n","    For example, we have two samples s0 and s1 and we have only two layers \"bias\" and \"weight\"\n","        s0 = (\"weight\": 1, \"layer\": 2)\n","        s1 = (\"weight\": 3, \"layer\": 4)\n","    Stacked gradients per layer means  = (\"weight\": [1,3], \"bias\":[2,4])\n","    Therefore, we have to unstack this stacked gradients to get back the gradient for each sample\n","    '''\n","\n","    #get back per_sample_grad\n","    num_samples = len(samples)\n","    samples_grads = dict()\n","\n","    for i in range(num_samples):\n","      samples_grads[i] = OrderedDict()\n","\n","    '''\n","    1. Going through each layer in ft_per_sample_grads: key, value in ft_per_sample_grads.items()\n","    2. unstack the stacked of len(x) layers: unstacked_grads = torch.unbind(value, dim=0)\n","    3. redistribute the unstacked sample_layer_grad, i.e., samples_grads[i][key]\n","\n","    Each sample has its own grad now but saved in the form of dictionary\n","    '''\n","\n","    for key,value in ft_per_sample_grads.items():\n","        #unstack the grads for each layer\n","        unstacked_grads = torch.unbind(value, dim=0)\n","        i = 0\n","        for layer_grad in unstacked_grads:\n","            samples_grads[i][key] = layer_grad\n","            i += 1\n","\n","\n","    return samples_grads\n","\n","\n","def generate_private_grad(model,loss_fn,samples,targets,sigma,const_C):\n","    '''\n","        We generate private grad given a batch of samples (samples,targets) as introduced here https://arxiv.org/pdf/1607.00133.pdf\n","        The implementation flow is as follows:\n","            1. sample xi\n","            2. ===> gradient gi\n","            3. ===> clipped gradient gci\n","            4. ===> noisy aggregated (sum gci + noise)\n","            5. ===> normalized 1/B (sum gci + noise)\n","\n","        We want to follow the tutorial in here to compute multiple grads in parallel:\n","            #https://pytorch.org/tutorials/intermediate/per_sample_grads.html?utm_source=whats_new_tutorials&utm_medium=per_sample_grads\n","            #https://pytorch.org/docs/stable/generated/torch.func.grad.html\n","            #https://towardsdatascience.com/introduction-to-functional-pytorch-b5bf739e1e6e\n","            #https://pytorch.org/functorch/stable/notebooks/per_sample_grads.html [PAY ATTENTION TO]\n","        Typically, we generate all gradients gis of samples sis in parallel in the helper function: compute_gradients\n","        The output of compute_gradients is an array called samples_grads\n","            sample s[0]: samples_grads[0][layer_1], samples_grads[0][layer_2], .... //g0\n","                ...............\n","            sample s[L-1]: samples_grads[L-1][layer_1], samples_grads[L-1][layer_2], ....//g[L-1]\n","            where L is the number of samples in the mini-batch\n","\n","        The compute_gradients call another helper function called compute_loss. This is used for computing the gradients in parallel\n","\n","        After that we compute the clipped gradients gci for each gi. In this case we use the following approach proposed here\n","            #https://www.tutorialspoint.com/python-pytorch-clamp-method\n","        To do it, we need to create a new field called whole_grad which containing all gradients of layers for a given sample si\n","        whole_grad allows us to compute the total_norm of sample si and then we can do the clipping\n","\n","        After computing all clipped gradients, we need to aggregate all the clipped gradient per layer. This step helps us\n","        to compute the sum (clipped gradient gi) and then we add noise to each entry in the sum (clipped gradient gi)\n","\n","        Finally, we normalize the private gradient and update the model.grad. This step allows optimizer update the model\n","    '''\n","\n","    samples_grads = compute_gradients(model,loss_fn,samples,targets)\n","\n","    #compute the size of batch for normalizing the private grad as done next steps\n","    num_samples = len(samples)\n","\n","    #clipping the per_sample_grad\n","    for i in range(num_samples):\n","        norm_type = 2.0\n","        max_norm = const_C #This is clipping constant C\n","        '''\n","          This is naive layer wise clipping, i.e., compute the norm of the layer and clipping the layer grad based on that norm and const_C\n","          It means, we do not need to compute the norm of the gradiets of the whole model !!!!!\n","          step 1. For each sample, we loop through all the layer\n","          step 2. For each layer, we compute its norm and clip its gradient based on that norm and const_C\n","          #https://discuss.pytorch.org/t/how-to-clip-grad-norm-grads-from-torch-autograd-grad/137816/2\n","        '''\n","        for layer, grad in samples_grads[i].items():\n","          total_norm = torch.norm(grad, norm_type)\n","          clip_coef = max_norm / (total_norm + 1e-6)\n","          #https://www.tutorialspoint.com/python-pytorch-clamp-method\n","          #clamp(tensor,min,max)\n","          clip_coef_clamped = torch.clamp(clip_coef, max=1.0)\n","          grad.detach().mul_(clip_coef_clamped)\n","\n","    #Aggregate clipped grads\n","    '''\n","        aggregated_grad_dict looks like as follows if we have two samples s0 and s1 as described above.\n","            aggreated_grad_dict[key=weight]= {1, 3}\n","            aggreated_grad_dict[key=bias]= {2, 4}\n","        To get it, we have to loop through all samples and for each sample, we loop through each layer (key) to get it grad (value)\n","    '''\n","\n","    aggregated_grad_dict = defaultdict(list)\n","\n","    for sample in samples_grads.values():\n","        for layer, grad in sample.items():\n","            aggregated_grad_dict[layer].append(grad)\n","\n","    #generate private grad per layer\n","    mean = 0\n","    std = sigma*const_C\n","    batch_size = num_samples\n","    for layer, list_grad in aggregated_grad_dict.items():\n","        #compute the sum of clipped gradients gi\n","        aggregated_grad_dict[layer] = np.sum(list_grad)\n","        #generate the noise ~ N(0,(C\\sigma)^2I)\n","        #std -- is C\\sigma as explain this in wikipage https://en.wikipedia.org/wiki/Normal_distribution N(mu,\\sigma^2) and sigma is std\n","        noise = torch.normal(mean=mean, std=std, size=aggregated_grad_dict[layer].shape)\n","        #generate private gradient per layer\n","        aggregated_grad_dict[layer] = (aggregated_grad_dict[layer] + noise)/batch_size\n","\n","    #update the model's grads\n","    '''\n","        Because we do not use loss_fn.backward() function to generate model.grad, model.grad is NONE\n","        We need to update the model.grad to make sure that optim.step() can operate normally\n","    '''\n","\n","    for layer, param in model.named_parameters():\n","        param.grad =  aggregated_grad_dict[layer]\n","\n","    return 0\n","\n","def training_loop(n_epochs, optimizer, model, loss_fn, sigma, const_C, train_loader, val_loader, data_path):\n","    for epoch in range(1, n_epochs + 1):\n","        loss_train = 0.0\n","\n","        for imgs, labels in train_loader:\n","\n","          outputs = model(imgs)\n","          loss = loss_fn(outputs, labels)\n","          loss_train += loss.item()\n","\n","          optimizer.zero_grad()\n","          '''\n","            generate_private_grad(model,loss_fn,imgs,labels,sigma,const_C)\n","              1. Compute the grad per sample\n","              2. Clipping the grad per sample\n","              3. Aggregate the clipped grads and add noise to sum of clipped grads\n","              4. Update the model.grad. This helps optimizer.step works as normal.\n","          '''\n","          #loss.backward()\n","          generate_private_grad(model,loss_fn,imgs,labels,sigma,const_C)\n","          optimizer.step()\n","\n","        correct = 0\n","        with torch.no_grad():\n","            for data in val_loader:\n","                images, labels = data\n","                outputs = model(images)\n","                _, predicted = torch.max(outputs, 1)\n","                c = (predicted == labels).squeeze()\n","                correct += c.sum()\n","        if epoch == 1 or epoch % 1 == 0:\n","            print('Epoch {}, Training loss {}, Val accuracy {}'.format(\n","                epoch,\n","                loss_train / len(train_loader),\n","                correct / len(cifar10_val)))\n","\n","        before_lr = optimizer.param_groups[0][\"lr\"]\n","        scheduler.step()\n","        after_lr = optimizer.param_groups[0][\"lr\"]\n","        print(\"Epoch %d: SGD lr %.4f -> %.4f\" % (epoch, before_lr, after_lr))\n","\n","\n","        #save the model config\n","        model_state = model.state_dict()\n","        optimizer_state = optimizer.state_dict()\n","        scheduler_state = scheduler.state_dict()\n","        dict_state = dict()\n","        dict_state[\"epoch\"] = epoch\n","        dict_state[\"sigma\"] = sigma\n","        dict_state[\"const_C\"] = const_C\n","        dict_state[\"model_state\"] = model_state\n","        dict_state[\"optimizer_state\"] = optimizer_state\n","        dict_state[\"scheduler_state\"] = scheduler_state\n","        dict_state[\"train_loss\"] = loss_train / len(train_loader)\n","        dict_state[\"val_acc\"] = correct / len(cifar10_val)\n","\n","        try:\n","            geeky_file = open(data_path + \"epoch_\" + str(epoch), 'wb')\n","            pickle.dump(dict_state, geeky_file)\n","            geeky_file.close()\n","\n","        except:\n","            print(\"Something went wrong\")\n","\n","        #print(f\"scheduler state: {scheduler_state}\")"],"metadata":{"id":"0Pn9A9tujUnW","executionInfo":{"status":"ok","timestamp":1697641998291,"user_tz":420,"elapsed":504,"user":{"displayName":"Phuong Ha Nguyen","userId":"01448229299682072761"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["for index, config in configs.items():\n","  print(f\"index: {index}\")\n","  data_path_index = data_path + \"config_\" + str(index) + \"_\"\n","  model = Net()\n","  optimizer = optim.SGD(model.parameters(), lr=config[\"lr_initial\"])\n","  loss_fn = nn.CrossEntropyLoss()\n","  '''\n","    LinearLR =>> new LR = initial LR - nb_epochs*(start_factor-end_factor)/total_iters\n","    example, initialLR = 0.1, start = 1.0, end_factor = 0.5, total_iters = 20\n","    (start_factor-end_factor)/total_iters = 0.025.\n","    ===> epoch 1: 0.1 - 1*0.025 = 0.0975\n","    ===> epoch 2: 0.1 - 2*0.025 = 0.0950....\n","  '''\n","  #scheduler = lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.5, total_iters=20)\n","  '''\n","   StepLR =>>> new LR = old LR * gamma\n","  '''\n","  scheduler = lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n","  '''\n","    LambdaLR: new LR = initialLR * f(epoch)\n","    For example: f(epoch) = 1/t\n","  '''\n","  # lambda1 = lambda epoch: 1/(epoch+1)\n","  # scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=[lambda1])\n","\n","  training_loop(\n","      n_epochs = 20,\n","      optimizer = optimizer,\n","      model = model,\n","      sigma = config[\"sigma\"],\n","      const_C = config[\"const_C\"],\n","      loss_fn = loss_fn,\n","      train_loader = train_loader,\n","      val_loader = val_loader,\n","      data_path = data_path_index\n","  )\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"auhHQgx8jcbV","executionInfo":{"status":"ok","timestamp":1697643843692,"user_tz":420,"elapsed":1826222,"user":{"displayName":"Phuong Ha Nguyen","userId":"01448229299682072761"}},"outputId":"e359a8e3-4d63-42b9-e71e-ad66e6a50650"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["index: 1\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:86: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n","  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n","/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, Training loss 1.5926964417138063, Val accuracy 0.38440001010894775\n","Epoch 1: SGD lr 0.1000 -> 0.0900\n","Epoch 2, Training loss 1.2679662905690614, Val accuracy 0.5392000079154968\n","Epoch 2: SGD lr 0.0900 -> 0.0810\n","Epoch 3, Training loss 1.1451899649389565, Val accuracy 0.5745000243186951\n","Epoch 3: SGD lr 0.0810 -> 0.0729\n","Epoch 4, Training loss 1.0653740172953252, Val accuracy 0.5764999985694885\n","Epoch 4: SGD lr 0.0729 -> 0.0656\n","Epoch 5, Training loss 1.0145335215741715, Val accuracy 0.5806999802589417\n","Epoch 5: SGD lr 0.0656 -> 0.0590\n","Epoch 6, Training loss 0.9744223578811606, Val accuracy 0.6075000166893005\n","Epoch 6: SGD lr 0.0590 -> 0.0531\n","Epoch 7, Training loss 0.942285154481678, Val accuracy 0.6039999723434448\n","Epoch 7: SGD lr 0.0531 -> 0.0478\n","Epoch 8, Training loss 0.9159496396856235, Val accuracy 0.6105999946594238\n","Epoch 8: SGD lr 0.0478 -> 0.0430\n","Epoch 9, Training loss 0.8887649205944422, Val accuracy 0.6162999868392944\n","Epoch 9: SGD lr 0.0430 -> 0.0387\n","Epoch 10, Training loss 0.8696775381903514, Val accuracy 0.6353999972343445\n","Epoch 10: SGD lr 0.0387 -> 0.0349\n","Epoch 11, Training loss 0.8515049667309618, Val accuracy 0.6452000141143799\n","Epoch 11: SGD lr 0.0349 -> 0.0314\n","Epoch 12, Training loss 0.8339211888935255, Val accuracy 0.6504999995231628\n","Epoch 12: SGD lr 0.0314 -> 0.0282\n","Epoch 13, Training loss 0.8176790149620426, Val accuracy 0.6517000198364258\n","Epoch 13: SGD lr 0.0282 -> 0.0254\n","Epoch 14, Training loss 0.8057979436024375, Val accuracy 0.6421999931335449\n","Epoch 14: SGD lr 0.0254 -> 0.0229\n","Epoch 15, Training loss 0.7941094681887371, Val accuracy 0.6430000066757202\n","Epoch 15: SGD lr 0.0229 -> 0.0206\n","Epoch 16, Training loss 0.7839479939559536, Val accuracy 0.6373999714851379\n","Epoch 16: SGD lr 0.0206 -> 0.0185\n","Epoch 17, Training loss 0.7742326454357114, Val accuracy 0.6575999855995178\n","Epoch 17: SGD lr 0.0185 -> 0.0167\n","Epoch 18, Training loss 0.7658443775628229, Val accuracy 0.6467000246047974\n","Epoch 18: SGD lr 0.0167 -> 0.0150\n","Epoch 19, Training loss 0.7571704566021404, Val accuracy 0.6550999879837036\n","Epoch 19: SGD lr 0.0150 -> 0.0135\n","Epoch 20, Training loss 0.7496743554349445, Val accuracy 0.6629999876022339\n","Epoch 20: SGD lr 0.0135 -> 0.0122\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","epoch = 1\n","path = data_path + \"epoch_\" + str(epoch)\n","obj = pd.read_pickle(path)\n","print(obj.keys())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IkrX1-y8oABE","executionInfo":{"status":"ok","timestamp":1696992505070,"user_tz":420,"elapsed":574,"user":{"displayName":"Phuong Ha Nguyen","userId":"01448229299682072761"}},"outputId":"d1d8326c-52d1-4a2f-d0ab-d31f8204867d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["dict_keys(['epoch', 'model_state', 'optimizer_state'])\n"]}]}]}