{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM7IXanhQI8Ro9MvZKyHi/z"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":18,"metadata":{"id":"r7Rl1QFk5icv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697923788781,"user_tz":420,"elapsed":1067,"user":{"displayName":"Phuong Ha Nguyen","userId":"01448229299682072761"}},"outputId":"81c45361-e288-4ac4-ee3b-4d5cb15b77b9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/Hadrive; to attempt to forcibly remount, call drive.mount(\"/content/Hadrive\", force_remount=True).\n"]}],"source":["# First we need to mount the Google drive\n","import os\n","from google.colab import drive\n","drive.mount('/content/Hadrive')"]},{"cell_type":"code","source":["\n","configs = dict({\n","\"1\": {\"outer_n_epochs\": 10, \"outer_batch_size\": 5000, \"lr_outer_initial\": 0.5,\n","      \"inner_n_epochs\": 2, \"inner_batch_size\": 64, \"lr_inner_initial\": 0.01, \"decay\": 0.9,\n","      \"sigma\": 0.00000001, \"const_C\": 1000}\n","})\n","\n","\n","for index, config in configs.items():\n","  print(f\"index: {index}\")\n","  print(f\"config: {config}\")\n","  for key,value in config.items():\n","    print(f\"key: {key}, value: {value}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9r5K3CJU236g","executionInfo":{"status":"ok","timestamp":1697938301445,"user_tz":420,"elapsed":231,"user":{"displayName":"Phuong Ha Nguyen","userId":"01448229299682072761"}},"outputId":"57086c3a-a789-4268-c3cd-e349e870c3f7"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["index: 1\n","config: {'outer_n_epochs': 10, 'outer_batch_size': 5000, 'lr_outer_initial': 0.5, 'inner_n_epochs': 2, 'inner_batch_size': 64, 'lr_inner_initial': 0.01, 'decay': 0.9, 'sigma': 1e-08, 'const_C': 1000}\n","key: outer_n_epochs, value: 10\n","key: outer_batch_size, value: 5000\n","key: lr_outer_initial, value: 0.5\n","key: inner_n_epochs, value: 2\n","key: inner_batch_size, value: 64\n","key: lr_inner_initial, value: 0.01\n","key: decay, value: 0.9\n","key: sigma, value: 1e-08\n","key: const_C, value: 1000\n"]}]},{"cell_type":"code","source":["#!mkdir /content/Hadrive/MyDrive/Test1\n","#!mkdir /content/Hadrive/MyDrive/Test1/Tutorial1/"],"metadata":{"id":"q8wxEMaM3C59"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torch.optim as optim\n","import torch.optim.lr_scheduler as lr_scheduler\n","import torchvision\n","import torchvision.transforms as transforms\n","from torchvision import datasets\n","from torchvision import transforms\n","from torch.func import functional_call, vmap, grad\n","from torch.utils.data import TensorDataset, DataLoader\n","\n","\n","import pickle\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from collections import OrderedDict\n","from collections import defaultdict\n"],"metadata":{"id":"vkpkT6pfieuj","executionInfo":{"status":"ok","timestamp":1697938311308,"user_tz":420,"elapsed":209,"user":{"displayName":"Phuong Ha Nguyen","userId":"01448229299682072761"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["data_path = '/content/Hadrive/MyDrive/Test1/Tutorial1/'\n","\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.4915, 0.4823, 0.4468), (0.2470, 0.2435, 0.2616))\n","])\n","\n","# cifar10 = datasets.CIFAR10(data_path, train=True, download=True, transform=transform)\n","# cifar10_val = datasets.CIFAR10(data_path, train=False, download=True, transform=transform)\n","\n","nbsamples = 100\n","\n","#https://stackoverflow.com/questions/47432168/taking-subsets-of-a-pytorch-dataset\n","cifar10_org = datasets.CIFAR10(data_path, train=True, download=False, transform=transform)\n","#cifar10 = torch.utils.data.Subset(cifar10_org, list(range(0, nbsamples)))\n","cifar10 = cifar10_org\n","\n","cifar10_val_org = datasets.CIFAR10(data_path, train=False, download=False, transform=transform)\n","#cifar10_val = torch.utils.data.Subset(cifar10_val_org, list(range(0, nbsamples)))\n","cifar10_val = cifar10_val_org\n","\n","print(f\"lencifar10: {len(cifar10)}\")\n","print(f\"lencifar10_val: {len(cifar10_val)}\")"],"metadata":{"id":"j0_-UJk0izg6","executionInfo":{"status":"ok","timestamp":1697938318351,"user_tz":420,"elapsed":3650,"user":{"displayName":"Phuong Ha Nguyen","userId":"01448229299682072761"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"52a28aa7-31f3-4436-bbe5-e14d98a8383b"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["lencifar10: 50000\n","lencifar10_val: 10000\n"]}]},{"cell_type":"code","source":["# model\n","class Net(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n","        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n","        self.fc1 = nn.Linear(8 * 8 * 8, 32)\n","        # bài toán phân loại 10 lớp nên output ra 10 nodes\n","        self.fc2 = nn.Linear(32, 10)\n","\n","    def forward(self, x):\n","        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n","        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n","        # flatten về dạng vector để cho vào neural network\n","        out = out.view(-1, 8 * 8 * 8)\n","        out = torch.tanh(self.fc1(out))\n","        out = self.fc2(out)\n","        return out\n"],"metadata":{"id":"iy4nacdKjQGu","executionInfo":{"status":"ok","timestamp":1697938328256,"user_tz":420,"elapsed":193,"user":{"displayName":"Phuong Ha Nguyen","userId":"01448229299682072761"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["import pickle\n","\n","import torch\n","import torchvision\n","import torch.nn as nn\n","import numpy as np\n","import torchvision.transforms as transforms\n","from collections import OrderedDict\n","from collections import defaultdict\n","from torch.func import functional_call, vmap, grad\n","\n","def generate_private_grad(model,loss_fn,samples,targets,inner_n_epochs,inner_batch_size,sigma,const_C,val_loader):\n","    '''\n","        We generate private grad given a batch of samples (samples,targets) in batchclipping mode for classical mini-batch SGD\n","    '''\n","\n","    #prepare a new dataloader based on given mini-batch\n","    mini_dataset = TensorDataset(samples,targets)\n","    mini_dataloader = DataLoader(mini_dataset,inner_batch_size,shuffle=True)\n","\n","    #save the starting model state for compute the sum of gradients in final step\n","    model_state_start = model.state_dict()\n","\n","\n","    #training the model with given sub-dataset\n","    for epoch in range(1, inner_n_epochs + 1):\n","      for inputs,labels in mini_dataloader:\n","        #copute the gradient of the whole batch\n","        outputs = model(inputs)\n","        loss = loss_fn(outputs, labels)\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","      # #print the test accuracy\n","      # correct = 0\n","      # with torch.no_grad():\n","      #     for data in val_loader:\n","      #         images, labelsx = data\n","      #         outputsx = model(images)\n","      #         _, predicted = torch.max(outputsx, 1)\n","      #         c = (predicted == labelsx).squeeze()\n","      #         correct += c.sum()\n","\n","      # if epoch == 1 or epoch % 1 == 0:\n","      #     print('Inner Epoch {}, Val accuracy {}'.format(epoch, correct / len(cifar10_val)))\n","\n","    #extract the sum of gradients, i.e., sum_grads = model.state_dict_last - model.state_dict_start\n","    # sum_grads contains tensor\n","    model_state_last = model.state_dict()\n","\n","    sum_grads = OrderedDict()\n","    for layer in model_state_start.keys():\n","         sum_grads[layer] = model_state_last[layer] - model_state_start[layer]\n","\n","\n","    #generate private grad per layer\n","    mean = 0\n","    std = sigma*const_C\n","    norm_type = 2.0\n","    #clipping the gradient\n","    #https://discuss.pytorch.org/t/how-to-clip-grad-norm-grads-from-torch-autograd-grad/137816/2\n","    for layer, grad in sum_grads.items():\n","        #clip the gradients\n","        max_norm = const_C #clipping constant C\n","        total_norm = torch.norm(grad.detach(), norm_type)\n","        clip_coef = max_norm / (total_norm + 1e-6)\n","        clip_coef_clamped = torch.clamp(clip_coef, max=1.0)\n","        grad.detach().mul_(clip_coef_clamped)\n","        #generate the noise and add it to the clipped grads\n","        #generate the noise ~ N(0,(C\\sigma)^2I)\n","        #std -- is C\\sigma as explain this in wikipage https://en.wikipedia.org/wiki/Normal_distribution N(mu,\\sigma^2) and sigma is std\n","        noise = torch.normal(mean=mean, std=std, size=grad.shape)\n","        #generate private gradient per layer\n","        grad = grad + noise\n","\n","    #reset the model\n","    model.load_state_dict(model_state_start)\n","    #update the model.param.grad with noisy grads\n","    for layer, param in model.named_parameters():\n","        param.grad = sum_grads[layer]\n","\n","    return 0\n","\n","\n","def training_loop(outer_n_epochs, optimizer, model, loss_fn, inner_n_epochs, inner_batch_size, lr_outer, sigma, const_C, train_loader, val_loader, data_path):\n","    '''\n","        Outer phrase: model = model - lr_outer*private_grad\n","        Inner phrase: compute private_grad using batch_clipping and running classical SGD\n","    '''\n","    #Outer phrase\n","    for epoch in range(1, outer_n_epochs + 1):\n","        loss_train = 0.0\n","\n","        #extract mini_batch from train_loader and input it to inner phrase\n","        for imgs, labels in train_loader:\n","\n","          outputs = model(imgs)\n","          loss = loss_fn(outputs, labels)\n","          loss_train += loss.item()\n","\n","          optimizer.zero_grad()\n","          '''\n","            generate_private_grad(model,loss_fn,imgs,labels,inner_n_epochs,inner_batch_size,sigma,const_C,val_loader)\n","              1. Compute the grad for whole batch of samples\n","              2. Clip the gradient of the batch of samples\n","              3. Add noise to the clipped grad of the whole batch of samples\n","              4. Update the model.grad. This helps optimizer.step works as normal.\n","          '''\n","          #loss.backward()\n","          generate_private_grad(model,loss_fn,imgs,labels,inner_n_epochs,inner_batch_size,sigma,const_C,val_loader)\n","\n","          #update the model\n","          for param in model.parameters():\n","              param.data = param.data - lr_outer*param.grad\n","\n","        correct = 0\n","        with torch.no_grad():\n","            for data in val_loader:\n","                images, labels = data\n","                outputs = model(images)\n","                _, predicted = torch.max(outputs, 1)\n","                c = (predicted == labels).squeeze()\n","                correct += c.sum()\n","        if epoch == 1 or epoch % 1 == 0:\n","            print('Outer Epoch {}, Training loss {}, Val accuracy {}'.format(\n","                epoch,\n","                loss_train / len(train_loader),\n","                correct / len(cifar10_val)))\n","\n","        # before_lr = optimizer.param_groups[0][\"lr\"]\n","        # scheduler.step()\n","        # after_lr = optimizer.param_groups[0][\"lr\"]\n","        # print(\"Epoch %d: SGD lr %.4f -> %.4f\" % (epoch, before_lr, after_lr))\n","\n","\n","        #save the model config\n","        model_state = model.state_dict()\n","        optimizer_state = optimizer.state_dict()\n","        scheduler_state = scheduler.state_dict()\n","        dict_state = dict()\n","        dict_state[\"epoch\"] = epoch\n","        dict_state[\"sigma\"] = sigma\n","        dict_state[\"const_C\"] = const_C\n","        dict_state[\"model_state\"] = model_state\n","        dict_state[\"optimizer_state\"] = optimizer_state\n","        dict_state[\"scheduler_state\"] = scheduler_state\n","        dict_state[\"train_loss\"] = loss_train / len(train_loader)\n","        dict_state[\"val_acc\"] = correct / len(cifar10_val)\n","\n","        try:\n","            geeky_file = open(data_path + \"epoch_\" + str(epoch), 'wb')\n","            pickle.dump(dict_state, geeky_file)\n","            geeky_file.close()\n","\n","        except:\n","            print(\"Something went wrong\")\n","\n","        #print(f\"scheduler state: {scheduler_state}\")"],"metadata":{"id":"0Pn9A9tujUnW","executionInfo":{"status":"ok","timestamp":1697938330553,"user_tz":420,"elapsed":237,"user":{"displayName":"Phuong Ha Nguyen","userId":"01448229299682072761"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["'''\n","    = This is the implementation of the idea generalization of batch clipping.\n","    = In this setup has two phrases\n","        Outer phrase: model = model - lr_outer*private_grad\n","        Inner phrase: compute private_grad using batch_clipping and running classical SGD\n","\n","      Pseudo_code:\n","          #Outer_phrase\n","          for epoch in range(1,nb_outer_epoches+1,1):\n","              A. for mini_batch in data_loader:\n","                  #Inner_phrase:\n","                  a. generate_private_gradient:\n","                    1. model_state_begin = model\n","                    2. for epoch' in range(1,nb_inner_epoches+1,1):\n","                          for inner_mini_batch in DataLoader(mini_batch):\n","                              = compute gradient of inner_mini_batch given model\n","                              = update the model with computed gradient from inner_mini_batch\n","\n","                    3. model_state_last = model\n","                    4. compute the sum of all gradients = model_state_last - model_state_begin\n","                    5. compute clipped naive layerwise gradients = clipping(model_state_last - model_state_begin)\n","                    6. add noise to clipped naive layerwise to create private_gradient\n","                  b. Update the model, i.e., model = model - lr_outer*private_gradient\n","              B. Update lr_inner\n","\n","\n","    = Note: there are two learning rate schemes lr_inner and lr_outer\n","            Optimizer uses lr_inner in Inner_phrase\n","            Outer phrase updates its own learning rate lr_outer by itself. We may set lr_outer = 1/2.\n","'''\n","\n","for index, config in configs.items():\n","  print(f\"index: {index}\")\n","  data_path_index = data_path + \"config_\" + str(index) + \"_\"\n","  model = Net()\n","  loss_fn = nn.CrossEntropyLoss()\n","\n","  #inner setup\n","  optimizer = optim.SGD(model.parameters(), lr=config[\"lr_inner_initial\"])\n","  scheduler = lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n","\n","  train_loader = torch.utils.data.DataLoader(cifar10, batch_size=config[\"outer_batch_size\"],shuffle=True)\n","  val_loader = torch.utils.data.DataLoader(cifar10_val, batch_size=config[\"outer_batch_size\"],shuffle=True)\n","\n","\n","\n","  training_loop(\n","      outer_n_epochs = config[\"outer_n_epochs\"],\n","      optimizer = optimizer,\n","      model = model,\n","      loss_fn = loss_fn,\n","      inner_n_epochs = config[\"inner_n_epochs\"],\n","      inner_batch_size = config[\"inner_batch_size\"],\n","      lr_outer = config[\"lr_outer_initial\"],\n","      sigma = config[\"sigma\"],\n","      const_C = config[\"const_C\"],\n","      train_loader = train_loader,\n","      val_loader = val_loader,\n","      data_path = data_path_index\n","  )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"auhHQgx8jcbV","executionInfo":{"status":"ok","timestamp":1697939299434,"user_tz":420,"elapsed":959342,"user":{"displayName":"Phuong Ha Nguyen","userId":"01448229299682072761"}},"outputId":"b8bf0151-dcff-4210-d929-3e5ab71ad30b"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["index: 1\n","Outer Epoch 1, Training loss 1.9300193905830383, Val accuracy 0.4250999987125397\n","Outer Epoch 2, Training loss 1.55693861246109, Val accuracy 0.461899995803833\n","Outer Epoch 3, Training loss 1.4203070282936097, Val accuracy 0.5023999810218811\n","Outer Epoch 4, Training loss 1.337641751766205, Val accuracy 0.5314000248908997\n","Outer Epoch 5, Training loss 1.2576587915420532, Val accuracy 0.5608000159263611\n","Outer Epoch 6, Training loss 1.2193223476409911, Val accuracy 0.5917999744415283\n","Outer Epoch 7, Training loss 1.2036351680755615, Val accuracy 0.5934000015258789\n","Outer Epoch 8, Training loss 1.1454653143882751, Val accuracy 0.6080999970436096\n","Outer Epoch 9, Training loss 1.2665251255035401, Val accuracy 0.5752000212669373\n","Outer Epoch 10, Training loss 1.0798550724983216, Val accuracy 0.6168000102043152\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","epoch = 1\n","path = data_path + \"epoch_\" + str(epoch)\n","obj = pd.read_pickle(path)\n","print(obj.keys())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IkrX1-y8oABE","executionInfo":{"status":"ok","timestamp":1696992505070,"user_tz":420,"elapsed":574,"user":{"displayName":"Phuong Ha Nguyen","userId":"01448229299682072761"}},"outputId":"d1d8326c-52d1-4a2f-d0ab-d31f8204867d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["dict_keys(['epoch', 'model_state', 'optimizer_state'])\n"]}]}]}