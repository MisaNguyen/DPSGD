{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1067,
     "status": "ok",
     "timestamp": 1697923788781,
     "user": {
      "displayName": "Phuong Ha Nguyen",
      "userId": "01448229299682072761"
     },
     "user_tz": 420
    },
    "id": "r7Rl1QFk5icv",
    "outputId": "81c45361-e288-4ac4-ee3b-4d5cb15b77b9"
   },
   "outputs": [],
   "source": [
    "# First we need to mount the Google drive\n",
    "import os\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/Hadrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 231,
     "status": "ok",
     "timestamp": 1697938301445,
     "user": {
      "displayName": "Phuong Ha Nguyen",
      "userId": "01448229299682072761"
     },
     "user_tz": 420
    },
    "id": "9r5K3CJU236g",
    "outputId": "57086c3a-a789-4268-c3cd-e349e870c3f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 1\n",
      "config: {'outer_n_epochs': 10, 'outer_batch_size': 64, 'lr_outer_initial': 0.5, 'inner_n_epochs': 2, 'inner_batch_size': 64, 'lr_inner_initial': 0.01, 'decay': 0.9, 'sigma': 0.0, 'const_C': 10000}\n",
      "key: outer_n_epochs, value: 10\n",
      "key: outer_batch_size, value: 64\n",
      "key: lr_outer_initial, value: 0.5\n",
      "key: inner_n_epochs, value: 2\n",
      "key: inner_batch_size, value: 64\n",
      "key: lr_inner_initial, value: 0.01\n",
      "key: decay, value: 0.9\n",
      "key: sigma, value: 0.0\n",
      "key: const_C, value: 10000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "configs = dict({\n",
    "\"1\": {\"outer_n_epochs\": 10, \"outer_batch_size\": 64, \"lr_outer_initial\": 0.5,\n",
    "      \"inner_n_epochs\": 2, \"inner_batch_size\": 64, \"lr_inner_initial\": 0.01, \"decay\": 0.9,\n",
    "      \"sigma\": 0.0, \"const_C\": 10000}\n",
    "})\n",
    "\n",
    "\n",
    "for index, config in configs.items():\n",
    "  print(f\"index: {index}\")\n",
    "  print(f\"config: {config}\")\n",
    "  for key,value in config.items():\n",
    "    print(f\"key: {key}, value: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "q8wxEMaM3C59"
   },
   "outputs": [],
   "source": [
    "#!mkdir /content/Hadrive/MyDrive/Test1\n",
    "#!mkdir /content/Hadrive/MyDrive/Test1/Tutorial1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 209,
     "status": "ok",
     "timestamp": 1697938311308,
     "user": {
      "displayName": "Phuong Ha Nguyen",
      "userId": "01448229299682072761"
     },
     "user_tz": 420
    },
    "id": "vkpkT6pfieuj"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.func import functional_call, vmap, grad\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3650,
     "status": "ok",
     "timestamp": 1697938318351,
     "user": {
      "displayName": "Phuong Ha Nguyen",
      "userId": "01448229299682072761"
     },
     "user_tz": 420
    },
    "id": "j0_-UJk0izg6",
    "outputId": "52a28aa7-31f3-4436-bbe5-e14d98a8383b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lencifar10: 50000\n",
      "lencifar10_val: 10000\n"
     ]
    }
   ],
   "source": [
    "data_path = '../../../data'\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4915, 0.4823, 0.4468), (0.2470, 0.2435, 0.2616))\n",
    "])\n",
    "\n",
    "# cifar10 = datasets.CIFAR10(data_path, train=True, download=True, transform=transform)\n",
    "# cifar10_val = datasets.CIFAR10(data_path, train=False, download=True, transform=transform)\n",
    "\n",
    "nbsamples = 100\n",
    "\n",
    "#https://stackoverflow.com/questions/47432168/taking-subsets-of-a-pytorch-dataset\n",
    "cifar10_org = datasets.CIFAR10(data_path, train=True, download=False, transform=transform)\n",
    "#cifar10 = torch.utils.data.Subset(cifar10_org, list(range(0, nbsamples)))\n",
    "cifar10 = cifar10_org\n",
    "\n",
    "cifar10_val_org = datasets.CIFAR10(data_path, train=False, download=False, transform=transform)\n",
    "#cifar10_val = torch.utils.data.Subset(cifar10_val_org, list(range(0, nbsamples)))\n",
    "cifar10_val = cifar10_val_org\n",
    "\n",
    "print(f\"lencifar10: {len(cifar10)}\")\n",
    "print(f\"lencifar10_val: {len(cifar10_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "executionInfo": {
     "elapsed": 193,
     "status": "ok",
     "timestamp": 1697938328256,
     "user": {
      "displayName": "Phuong Ha Nguyen",
      "userId": "01448229299682072761"
     },
     "user_tz": 420
    },
    "id": "iy4nacdKjQGu"
   },
   "outputs": [],
   "source": [
    "# model\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(8 * 8 * 8, 32)\n",
    "        # bài toán phân loại 10 lớp nên output ra 10 nodes\n",
    "        self.fc2 = nn.Linear(32, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        # flatten về dạng vector để cho vào neural network\n",
    "        out = out.view(-1, 8 * 8 * 8)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "executionInfo": {
     "elapsed": 237,
     "status": "ok",
     "timestamp": 1697938330553,
     "user": {
      "displayName": "Phuong Ha Nguyen",
      "userId": "01448229299682072761"
     },
     "user_tz": 420
    },
    "id": "0Pn9A9tujUnW"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import copy\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "from collections import OrderedDict\n",
    "from collections import defaultdict\n",
    "from torch.func import functional_call, vmap, grad\n",
    "\n",
    "def generate_private_grad(model,loss_fn,samples,targets,\n",
    "                           optimizer,inner_n_epochs,inner_batch_size,sigma,const_C,val_loader):\n",
    "    '''\n",
    "        We generate private grad given a batch of samples (samples,targets) in batchclipping mode for classical mini-batch SGD\n",
    "    '''\n",
    "\n",
    "    #prepare a new dataloader based on given mini-batch\n",
    "    mini_dataset = TensorDataset(samples,targets)\n",
    "    mini_dataloader = DataLoader(mini_dataset,inner_batch_size,shuffle=True)\n",
    "\n",
    "    #save the starting model state for compute the sum of gradients in final step\n",
    "    model_state_start = model.state_dict()\n",
    "    # model_tmp = copy.deepcopy(model)\n",
    "    model_tmp = Net()\n",
    "    # input(model_tmp)\n",
    "    # optimizer_tmp = type(optimizer)(model.parameters(), lr=optimizer.defaults['lr'])\n",
    "    # optimizer_tmp = type(optimizer)(model_tmp.parameters(), lr=0.1)\n",
    "    # optimizer_tmp.load_state_dict(optimizer.state_dict())\n",
    "    optimizer_tmp = optim.SGD(model_tmp.parameters(), lr=0.01)\n",
    "\n",
    "    #training the model with given sub-dataset\n",
    "    for epoch in range(1, inner_n_epochs + 1):\n",
    "      for inputs,labels in mini_dataloader:\n",
    "        #copute the gradient of the whole batch\n",
    "        outputs = model_tmp(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        optimizer_tmp.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_tmp.step()\n",
    "\n",
    "    \n",
    "      # #print the test accuracy\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            images, labelsx = data\n",
    "            outputsx = model_tmp(images)\n",
    "            _, predicted = torch.max(outputsx, 1)\n",
    "            c = (predicted == labelsx).squeeze()\n",
    "            correct += c.sum()\n",
    "\n",
    "    if epoch == 1 or epoch % 1 == 0:\n",
    "        print('Inner Epoch {}, Val accuracy {}'.format(epoch, correct / len(cifar10_val)))\n",
    "\n",
    "    #extract the sum of gradients, i.e., sum_grads = model.state_dict_last - model.state_dict_start\n",
    "    # sum_grads contains tensor\n",
    "    model_state_last = model_tmp.state_dict()\n",
    "\n",
    "    sum_grads = OrderedDict()\n",
    "    for layer in model_state_start.keys():\n",
    "         sum_grads[layer] = model_state_last[layer] - model_state_start[layer]\n",
    "\n",
    "\n",
    "    #generate private grad per layer\n",
    "    mean = 0\n",
    "    std = sigma*const_C\n",
    "    norm_type = 2.0\n",
    "    #clipping the gradient\n",
    "    #https://discuss.pytorch.org/t/how-to-clip-grad-norm-grads-from-torch-autograd-grad/137816/2\n",
    "    for layer, grad in sum_grads.items():\n",
    "        #clip the gradients\n",
    "        max_norm = const_C #clipping constant C\n",
    "        total_norm = torch.norm(grad.detach(), norm_type)\n",
    "        clip_coef = max_norm / (total_norm + 1e-6)\n",
    "        clip_coef_clamped = torch.clamp(clip_coef, max=1.0)\n",
    "        grad.detach().mul_(clip_coef_clamped)\n",
    "        #generate the noise and add it to the clipped grads\n",
    "        #generate the noise ~ N(0,(C\\sigma)^2I)\n",
    "        #std -- is C\\sigma as explain this in wikipage https://en.wikipedia.org/wiki/Normal_distribution N(mu,\\sigma^2) and sigma is std\n",
    "        noise = torch.normal(mean=mean, std=std, size=grad.shape)\n",
    "        #generate private gradient per layer\n",
    "        grad = grad + noise\n",
    "    \n",
    "    #reset the model\n",
    "    model.load_state_dict(model_state_start)\n",
    "    #update the model.param.grad with noisy grads\n",
    "    for layer, param in model.named_parameters():\n",
    "        param.grad = sum_grads[layer]\n",
    "\n",
    "    return 0\n",
    "\n",
    "\n",
    "def training_loop(outer_n_epochs, optimizer, model, loss_fn, inner_n_epochs, inner_batch_size, lr_outer, sigma, const_C, train_loader, val_loader, data_path):\n",
    "    '''\n",
    "        Outer phrase: model = model - lr_outer*private_grad\n",
    "        Inner phrase: compute private_grad using batch_clipping and running classical SGD\n",
    "    '''\n",
    "    #Outer phrase\n",
    "    for epoch in range(1, outer_n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "\n",
    "        #extract mini_batch from train_loader and input it to inner phrase\n",
    "        for imgs, labels in train_loader:\n",
    "\n",
    "          outputs = model(imgs)\n",
    "          loss = loss_fn(outputs, labels)\n",
    "          loss_train += loss.item()\n",
    "\n",
    "          optimizer.zero_grad()\n",
    "          '''\n",
    "            generate_private_grad(model,loss_fn,imgs,labels,inner_n_epochs,inner_batch_size,sigma,const_C,val_loader)\n",
    "              1. Compute the grad for whole batch of samples\n",
    "              2. Clip the gradient of the batch of samples\n",
    "              3. Add noise to the clipped grad of the whole batch of samples\n",
    "              4. Update the model.grad. This helps optimizer.step works as normal.\n",
    "          '''\n",
    "          loss.backward()\n",
    "        #   generate_private_grad(model,loss_fn,imgs,labels,\n",
    "                        #    optimizer,inner_n_epochs,inner_batch_size,sigma,const_C,val_loader)\n",
    "\n",
    "          #update the model\n",
    "          optimizer.step()\n",
    "        #   for param in model.parameters():\n",
    "        #       param.data = param.data - lr_outer*param.grad\n",
    "\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for data in val_loader:\n",
    "                images, labels = data\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                c = (predicted == labels).squeeze()\n",
    "                correct += c.sum()\n",
    "        if epoch == 1 or epoch % 1 == 0:\n",
    "            print('Outer Epoch {}, Training loss {}, Val accuracy {}'.format(\n",
    "                epoch,\n",
    "                loss_train / len(train_loader),\n",
    "                correct / len(cifar10_val)))\n",
    "\n",
    "        # before_lr = optimizer.param_groups[0][\"lr\"]\n",
    "        # scheduler.step()\n",
    "        # after_lr = optimizer.param_groups[0][\"lr\"]\n",
    "        # print(\"Epoch %d: SGD lr %.4f -> %.4f\" % (epoch, before_lr, after_lr))\n",
    "\n",
    "\n",
    "        #save the model config\n",
    "        model_state = model.state_dict()\n",
    "        optimizer_state = optimizer.state_dict()\n",
    "        # scheduler_state = scheduler.state_dict()\n",
    "        dict_state = dict()\n",
    "        dict_state[\"epoch\"] = epoch\n",
    "        dict_state[\"sigma\"] = sigma\n",
    "        dict_state[\"const_C\"] = const_C\n",
    "        dict_state[\"model_state\"] = model_state\n",
    "        dict_state[\"optimizer_state\"] = optimizer_state\n",
    "        # dict_state[\"scheduler_state\"] = scheduler_state\n",
    "        dict_state[\"train_loss\"] = loss_train / len(train_loader)\n",
    "        dict_state[\"val_acc\"] = correct / len(cifar10_val)\n",
    "\n",
    "        try:\n",
    "            geeky_file = open(data_path + \"epoch_\" + str(epoch), 'wb')\n",
    "            pickle.dump(dict_state, geeky_file)\n",
    "            geeky_file.close()\n",
    "\n",
    "        except:\n",
    "            print(\"Something went wrong\")\n",
    "\n",
    "        #print(f\"scheduler state: {scheduler_state}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 959342,
     "status": "ok",
     "timestamp": 1697939299434,
     "user": {
      "displayName": "Phuong Ha Nguyen",
      "userId": "01448229299682072761"
     },
     "user_tz": 420
    },
    "id": "auhHQgx8jcbV",
    "outputId": "b8bf0151-dcff-4210-d929-3e5ab71ad30b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 1\n",
      "Outer Epoch 1, Training loss 2.071533844446587, Val accuracy 0.34299999475479126\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 47\u001b[0m\n\u001b[0;32m     42\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(cifar10, batch_size\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mouter_batch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m],shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     43\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(cifar10_val, batch_size\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mouter_batch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m],shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 47\u001b[0m \u001b[43mtraining_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mouter_n_epochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mouter_n_epochs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43minner_n_epochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minner_n_epochs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43minner_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minner_batch_size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr_outer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr_outer_initial\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43msigma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msigma\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconst_C\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconst_C\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata_path_index\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[32], line 104\u001b[0m, in \u001b[0;36mtraining_loop\u001b[1;34m(outer_n_epochs, optimizer, model, loss_fn, inner_n_epochs, inner_batch_size, lr_outer, sigma, const_C, train_loader, val_loader, data_path)\u001b[0m\n\u001b[0;32m    101\u001b[0m loss_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;66;03m#extract mini_batch from train_loader and input it to inner phrase\u001b[39;00m\n\u001b[1;32m--> 104\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[43m  \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[43m  \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nntoa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:627\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    626\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m--> 627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecord_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_profile_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sampler_iter\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[0;32m    629\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;49;00m\n\u001b[0;32m    630\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-arg]\u001b[39;49;00m\n",
      "File \u001b[1;32mc:\\Users\\nntoa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\profiler.py:622\u001b[0m, in \u001b[0;36mrecord_function.__exit__\u001b[1;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[0;32m    620\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting():\n\u001b[0;32m    621\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mDisableTorchFunctionSubclass():\n\u001b[1;32m--> 622\u001b[0m         \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_record_function_exit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_RecordFunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    623\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    624\u001b[0m     torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39m_record_function_exit(record)\n",
      "File \u001b[1;32mc:\\Users\\nntoa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_ops.py:594\u001b[0m, in \u001b[0;36mOpOverload.__call__\u001b[1;34m(self_, *args, **kwargs)\u001b[0m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(self_, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[0;32m    592\u001b[0m     \u001b[38;5;66;03m# use `self_` to avoid naming collide with aten ops arguments that\u001b[39;00m\n\u001b[0;32m    593\u001b[0m     \u001b[38;5;66;03m# are named \"self\". This way, all the aten ops can be called by kwargs.\u001b[39;00m\n\u001b[1;32m--> 594\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mself_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "    = This is the implementation of the idea generalization of batch clipping.\n",
    "    = In this setup has two phrases\n",
    "        Outer phrase: model = model - lr_outer*private_grad\n",
    "        Inner phrase: compute private_grad using batch_clipping and running classical SGD\n",
    "\n",
    "      Pseudo_code:\n",
    "          #Outer_phrase\n",
    "          for epoch in range(1,nb_outer_epoches+1,1):\n",
    "              A. for mini_batch in data_loader:\n",
    "                  #Inner_phrase:\n",
    "                  a. generate_private_gradient:\n",
    "                    1. model_state_begin = model\n",
    "                    2. for epoch' in range(1,nb_inner_epoches+1,1):\n",
    "                          for inner_mini_batch in DataLoader(mini_batch):\n",
    "                              = compute gradient of inner_mini_batch given model\n",
    "                              = update the model with computed gradient from inner_mini_batch\n",
    "\n",
    "                    3. model_state_last = model\n",
    "                    4. compute the sum of all gradients = model_state_last - model_state_begin\n",
    "                    5. compute clipped naive layerwise gradients = clipping(model_state_last - model_state_begin)\n",
    "                    6. add noise to clipped naive layerwise to create private_gradient\n",
    "                  b. Update the model, i.e., model = model - lr_outer*private_gradient\n",
    "              B. Update lr_inner\n",
    "\n",
    "\n",
    "    = Note: there are two learning rate schemes lr_inner and lr_outer\n",
    "            Optimizer uses lr_inner in Inner_phrase\n",
    "            Outer phrase updates its own learning rate lr_outer by itself. We may set lr_outer = 1/2.\n",
    "'''\n",
    "\n",
    "for index, config in configs.items():\n",
    "  print(f\"index: {index}\")\n",
    "  data_path_index = data_path + \"config_\" + str(index) + \"_\"\n",
    "  model = Net()\n",
    "  loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "  #inner setup\n",
    "  optimizer = optim.SGD(model.parameters(), lr=config[\"lr_inner_initial\"])\n",
    "  scheduler = lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n",
    "\n",
    "  train_loader = torch.utils.data.DataLoader(cifar10, batch_size=config[\"outer_batch_size\"],shuffle=True)\n",
    "  val_loader = torch.utils.data.DataLoader(cifar10_val, batch_size=config[\"outer_batch_size\"],shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "  training_loop(\n",
    "      outer_n_epochs = config[\"outer_n_epochs\"],\n",
    "      optimizer = optimizer,\n",
    "      model = model,\n",
    "      loss_fn = loss_fn,\n",
    "      inner_n_epochs = config[\"inner_n_epochs\"],\n",
    "      inner_batch_size = config[\"inner_batch_size\"],\n",
    "      lr_outer = config[\"lr_outer_initial\"],\n",
    "      sigma = config[\"sigma\"],\n",
    "      const_C = config[\"const_C\"],\n",
    "      train_loader = train_loader,\n",
    "      val_loader = val_loader,\n",
    "      data_path = data_path_index\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 574,
     "status": "ok",
     "timestamp": 1696992505070,
     "user": {
      "displayName": "Phuong Ha Nguyen",
      "userId": "01448229299682072761"
     },
     "user_tz": 420
    },
    "id": "IkrX1-y8oABE",
    "outputId": "d1d8326c-52d1-4a2f-d0ab-d31f8204867d"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../../dataepoch_1'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m      3\u001b[0m path \u001b[38;5;241m=\u001b[39m data_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(epoch)\n\u001b[1;32m----> 4\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_pickle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(obj\u001b[38;5;241m.\u001b[39mkeys())\n",
      "File \u001b[1;32mc:\\Users\\nntoa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\pickle.py:185\u001b[0m, in \u001b[0;36mread_pickle\u001b[1;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;124;03mLoad pickled pandas object (or any object) from file.\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;124;03m4    4    9\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    184\u001b[0m excs_to_catch \u001b[38;5;241m=\u001b[39m (\u001b[38;5;167;01mAttributeError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m, \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m)\n\u001b[1;32m--> 185\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[0;32m    192\u001b[0m     \u001b[38;5;66;03m# 1) try standard library Pickle\u001b[39;00m\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;66;03m# 2) try pickle_compat (older pandas version) to handle subclass changes\u001b[39;00m\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;66;03m# 3) try pickle_compat with latin-1 encoding upon a UnicodeDecodeError\u001b[39;00m\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    197\u001b[0m         \u001b[38;5;66;03m# TypeError for Cython complaints about object.__new__ vs Tick.__new__\u001b[39;00m\n\u001b[0;32m    198\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\nntoa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    883\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[0;32m    885\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../../dataepoch_1'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "epoch = 1\n",
    "path = data_path + \"epoch_\" + str(epoch)\n",
    "obj = pd.read_pickle(path)\n",
    "print(obj.keys())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM7IXanhQI8Ro9MvZKyHi/z",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
