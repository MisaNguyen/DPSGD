{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN6qUN4POQRiD2RAhN3IaeW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"r7Rl1QFk5icv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697746163600,"user_tz":420,"elapsed":18932,"user":{"displayName":"Phuong Ha Nguyen","userId":"01448229299682072761"}},"outputId":"c7be9855-0b6e-4798-970b-14086c0c00ac"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/Hadrive\n"]}],"source":["# First we need to mount the Google drive\n","import os\n","from google.colab import drive\n","drive.mount('/content/Hadrive')"]},{"cell_type":"code","source":["\n","configs = dict({\n","\"1\": { \"lr_initial\": 0.1, \"decay\": 0.9, \"sigma\": 0.00000001, \"const_C\": 1000}\n","})\n","\n","\n","for index, config in configs.items():\n","  print(f\"index: {index}\")\n","  print(f\"config: {config}\")\n","  for key,value in config.items():\n","    print(f\"key: {key}, value: {value}\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9r5K3CJU236g","executionInfo":{"status":"ok","timestamp":1697746165520,"user_tz":420,"elapsed":179,"user":{"displayName":"Phuong Ha Nguyen","userId":"01448229299682072761"}},"outputId":"93f34f04-5fbd-4577-e6f1-8f74e732969f"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["index: 1\n","config: {'lr_initial': 0.1, 'decay': 0.9, 'sigma': 1e-08, 'const_C': 1000}\n","key: lr_initial, value: 0.1\n","key: decay, value: 0.9\n","key: sigma, value: 1e-08\n","key: const_C, value: 1000\n"]}]},{"cell_type":"code","source":["#!mkdir /content/Hadrive/MyDrive/Test1\n","#!mkdir /content/Hadrive/MyDrive/Test1/Tutorial1/"],"metadata":{"id":"q8wxEMaM3C59"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torch.optim as optim\n","import torch.optim.lr_scheduler as lr_scheduler\n","import torchvision\n","import torchvision.transforms as transforms\n","from torchvision import datasets\n","from torchvision import transforms\n","from torch.func import functional_call, vmap, grad\n","\n","\n","import pickle\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from collections import OrderedDict\n","from collections import defaultdict\n"],"metadata":{"id":"vkpkT6pfieuj","executionInfo":{"status":"ok","timestamp":1697746175344,"user_tz":420,"elapsed":5571,"user":{"displayName":"Phuong Ha Nguyen","userId":"01448229299682072761"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["data_path = '/content/Hadrive/MyDrive/Test1/Tutorial1/'\n","\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.4915, 0.4823, 0.4468), (0.2470, 0.2435, 0.2616))\n","])\n","\n","# cifar10 = datasets.CIFAR10(data_path, train=True, download=True, transform=transform)\n","# cifar10_val = datasets.CIFAR10(data_path, train=False, download=True, transform=transform)\n","\n","nbsamples = 100\n","\n","#https://stackoverflow.com/questions/47432168/taking-subsets-of-a-pytorch-dataset\n","cifar10_org = datasets.CIFAR10(data_path, train=True, download=False, transform=transform)\n","#cifar10 = torch.utils.data.Subset(cifar10_org, list(range(0, nbsamples)))\n","cifar10 = cifar10_org\n","\n","cifar10_val_org = datasets.CIFAR10(data_path, train=False, download=False, transform=transform)\n","#cifar10_val = torch.utils.data.Subset(cifar10_val_org, list(range(0, nbsamples)))\n","cifar10_val = cifar10_val_org\n","\n","print(f\"lencifar10: {len(cifar10)}\")\n","print(f\"lencifar10_val: {len(cifar10_val)}\")"],"metadata":{"id":"j0_-UJk0izg6","executionInfo":{"status":"ok","timestamp":1697746184281,"user_tz":420,"elapsed":6465,"user":{"displayName":"Phuong Ha Nguyen","userId":"01448229299682072761"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"27255996-a01c-4df9-fd6d-081c0b3730fc"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["lencifar10: 50000\n","lencifar10_val: 10000\n"]}]},{"cell_type":"code","source":["train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64,\n","                                           shuffle=True)\n","val_loader = torch.utils.data.DataLoader(cifar10_val, batch_size=64,\n","                                           shuffle=True)"],"metadata":{"id":"i4W2GOsdjK72","executionInfo":{"status":"ok","timestamp":1697746187650,"user_tz":420,"elapsed":112,"user":{"displayName":"Phuong Ha Nguyen","userId":"01448229299682072761"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# model\n","class Net(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n","        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n","        self.fc1 = nn.Linear(8 * 8 * 8, 32)\n","        # bài toán phân loại 10 lớp nên output ra 10 nodes\n","        self.fc2 = nn.Linear(32, 10)\n","\n","    def forward(self, x):\n","        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n","        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n","        # flatten về dạng vector để cho vào neural network\n","        out = out.view(-1, 8 * 8 * 8)\n","        out = torch.tanh(self.fc1(out))\n","        out = self.fc2(out)\n","        return out\n"],"metadata":{"id":"iy4nacdKjQGu","executionInfo":{"status":"ok","timestamp":1697746190519,"user_tz":420,"elapsed":127,"user":{"displayName":"Phuong Ha Nguyen","userId":"01448229299682072761"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["import pickle\n","\n","import torch\n","import torchvision\n","import torch.nn as nn\n","import numpy as np\n","import torchvision.transforms as transforms\n","from collections import OrderedDict\n","from collections import defaultdict\n","from torch.func import functional_call, vmap, grad\n","\n","def generate_private_grad(model,loss_fn,samples,targets,sigma,const_C):\n","    '''\n","        We generate private grad given a batch of samples (samples,targets) in batchclipping mode\n","        The implementation flow is as follows:\n","            1. samples x0, x1, ..., x(L-1)\n","            2. compute avg of gradient g = sum(g0, ..., g(L-1))/L\n","            3. clipped gradient gc\n","            4. clipped noisy gradient (gc*L + noise)/L\n","\n","        Finally, we normalize the private gradient and update the model.grad. This step allows optimizer update the model\n","    '''\n","\n","    #copute the gradient of the whole batch\n","    outputs = model(samples)\n","    loss = loss_fn(outputs, targets)\n","    model.zero_grad()\n","    loss.backward()\n","\n","    #generate private grad per layer\n","    mean = 0\n","    std = sigma*const_C\n","    batch_size = len(samples)\n","    norm_type = 2.0\n","    #clipping the gradient\n","    #https://discuss.pytorch.org/t/how-to-clip-grad-norm-grads-from-torch-autograd-grad/137816/2\n","    for param in model.parameters():\n","        #clip the gradients\n","        max_norm = const_C #clipping constant C\n","        grad = param.grad\n","        total_norm = torch.norm(grad.detach(), norm_type)\n","        clip_coef = max_norm / (total_norm + 1e-6)\n","        clip_coef_clamped = torch.clamp(clip_coef, max=1.0)\n","        param.grad.detach().mul_(clip_coef_clamped)\n","        #generate the noise and add it to the clipped grads\n","        grad = param.grad\n","        #generate the noise ~ N(0,(C\\sigma)^2I)\n","        #std -- is C\\sigma as explain this in wikipage https://en.wikipedia.org/wiki/Normal_distribution N(mu,\\sigma^2) and sigma is std\n","        noise = torch.normal(mean=mean, std=std, size=grad.shape)\n","        #generate private gradient per layer\n","        param.grad = (grad*batch_size + noise)/batch_size\n","\n","    return 0\n","\n","def training_loop(n_epochs, optimizer, model, loss_fn, sigma, const_C, train_loader, val_loader, data_path):\n","    for epoch in range(1, n_epochs + 1):\n","        loss_train = 0.0\n","\n","        for imgs, labels in train_loader:\n","\n","          outputs = model(imgs)\n","          loss = loss_fn(outputs, labels)\n","          loss_train += loss.item()\n","\n","          optimizer.zero_grad()\n","          '''\n","            generate_private_grad(model,loss_fn,imgs,labels,sigma,const_C)\n","              1. Compute the grad for whole batch of samples\n","              2. Clip the gradient of the batch of samples\n","              3. Add noise to the clipped grad of the whole batch of samples\n","              4. Update the model.grad. This helps optimizer.step works as normal.\n","          '''\n","          #loss.backward()\n","          generate_private_grad(model,loss_fn,imgs,labels,sigma,const_C)\n","          optimizer.step()\n","\n","        correct = 0\n","        with torch.no_grad():\n","            for data in val_loader:\n","                images, labels = data\n","                outputs = model(images)\n","                _, predicted = torch.max(outputs, 1)\n","                c = (predicted == labels).squeeze()\n","                correct += c.sum()\n","        if epoch == 1 or epoch % 1 == 0:\n","            print('Epoch {}, Training loss {}, Val accuracy {}'.format(\n","                epoch,\n","                loss_train / len(train_loader),\n","                correct / len(cifar10_val)))\n","\n","        before_lr = optimizer.param_groups[0][\"lr\"]\n","        scheduler.step()\n","        after_lr = optimizer.param_groups[0][\"lr\"]\n","        print(\"Epoch %d: SGD lr %.4f -> %.4f\" % (epoch, before_lr, after_lr))\n","\n","\n","        #save the model config\n","        model_state = model.state_dict()\n","        optimizer_state = optimizer.state_dict()\n","        scheduler_state = scheduler.state_dict()\n","        dict_state = dict()\n","        dict_state[\"epoch\"] = epoch\n","        dict_state[\"sigma\"] = sigma\n","        dict_state[\"const_C\"] = const_C\n","        dict_state[\"model_state\"] = model_state\n","        dict_state[\"optimizer_state\"] = optimizer_state\n","        dict_state[\"scheduler_state\"] = scheduler_state\n","        dict_state[\"train_loss\"] = loss_train / len(train_loader)\n","        dict_state[\"val_acc\"] = correct / len(cifar10_val)\n","\n","        try:\n","            geeky_file = open(data_path + \"epoch_\" + str(epoch), 'wb')\n","            pickle.dump(dict_state, geeky_file)\n","            geeky_file.close()\n","\n","        except:\n","            print(\"Something went wrong\")\n","\n","        #print(f\"scheduler state: {scheduler_state}\")"],"metadata":{"id":"0Pn9A9tujUnW","executionInfo":{"status":"ok","timestamp":1697747766409,"user_tz":420,"elapsed":163,"user":{"displayName":"Phuong Ha Nguyen","userId":"01448229299682072761"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["for index, config in configs.items():\n","  print(f\"index: {index}\")\n","  data_path_index = data_path + \"config_\" + str(index) + \"_\"\n","  model = Net()\n","  optimizer = optim.SGD(model.parameters(), lr=config[\"lr_initial\"])\n","  loss_fn = nn.CrossEntropyLoss()\n","  '''\n","    LinearLR =>> new LR = initial LR - nb_epochs*(start_factor-end_factor)/total_iters\n","    example, initialLR = 0.1, start = 1.0, end_factor = 0.5, total_iters = 20\n","    (start_factor-end_factor)/total_iters = 0.025.\n","    ===> epoch 1: 0.1 - 1*0.025 = 0.0975\n","    ===> epoch 2: 0.1 - 2*0.025 = 0.0950....\n","  '''\n","  #scheduler = lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.5, total_iters=20)\n","  '''\n","   StepLR =>>> new LR = old LR * gamma\n","  '''\n","  scheduler = lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n","  '''\n","    LambdaLR: new LR = initialLR * f(epoch)\n","    For example: f(epoch) = 1/t\n","  '''\n","  # lambda1 = lambda epoch: 1/(epoch+1)\n","  # scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=[lambda1])\n","\n","  training_loop(\n","      n_epochs = 20,\n","      optimizer = optimizer,\n","      model = model,\n","      sigma = config[\"sigma\"],\n","      const_C = config[\"const_C\"],\n","      loss_fn = loss_fn,\n","      train_loader = train_loader,\n","      val_loader = val_loader,\n","      data_path = data_path_index\n","  )\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"auhHQgx8jcbV","executionInfo":{"status":"ok","timestamp":1697748955432,"user_tz":420,"elapsed":1183546,"user":{"displayName":"Phuong Ha Nguyen","userId":"01448229299682072761"}},"outputId":"6dedcc44-c8c3-4ade-9504-b5923446ae58"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["index: 1\n","Epoch 1, Training loss 1.58858425202577, Val accuracy 0.4422000050544739\n","Epoch 1: SGD lr 0.1000 -> 0.0900\n","Epoch 2, Training loss 1.2638742430588168, Val accuracy 0.5180000066757202\n","Epoch 2: SGD lr 0.0900 -> 0.0810\n","Epoch 3, Training loss 1.1426090981496875, Val accuracy 0.5924000144004822\n","Epoch 3: SGD lr 0.0810 -> 0.0729\n","Epoch 4, Training loss 1.0709242207162522, Val accuracy 0.5735999941825867\n","Epoch 4: SGD lr 0.0729 -> 0.0656\n","Epoch 5, Training loss 1.0213102834761296, Val accuracy 0.5906999707221985\n","Epoch 5: SGD lr 0.0656 -> 0.0590\n","Epoch 6, Training loss 0.9827878783121133, Val accuracy 0.611299991607666\n","Epoch 6: SGD lr 0.0590 -> 0.0531\n","Epoch 7, Training loss 0.9500015035004872, Val accuracy 0.5997999906539917\n","Epoch 7: SGD lr 0.0531 -> 0.0478\n","Epoch 8, Training loss 0.9198796804756155, Val accuracy 0.6351000070571899\n","Epoch 8: SGD lr 0.0478 -> 0.0430\n","Epoch 9, Training loss 0.8974608985511848, Val accuracy 0.6352999806404114\n","Epoch 9: SGD lr 0.0430 -> 0.0387\n","Epoch 10, Training loss 0.8771719949324722, Val accuracy 0.6248000264167786\n","Epoch 10: SGD lr 0.0387 -> 0.0349\n","Epoch 11, Training loss 0.8590067210404769, Val accuracy 0.6272000074386597\n","Epoch 11: SGD lr 0.0349 -> 0.0314\n","Epoch 12, Training loss 0.8413902799339246, Val accuracy 0.6394000053405762\n","Epoch 12: SGD lr 0.0314 -> 0.0282\n","Epoch 13, Training loss 0.8251896070702301, Val accuracy 0.6502000093460083\n","Epoch 13: SGD lr 0.0282 -> 0.0254\n","Epoch 14, Training loss 0.8122293892342721, Val accuracy 0.6377000212669373\n","Epoch 14: SGD lr 0.0254 -> 0.0229\n","Epoch 15, Training loss 0.8029090533476047, Val accuracy 0.6297000050544739\n","Epoch 15: SGD lr 0.0229 -> 0.0206\n","Epoch 16, Training loss 0.7903541157693814, Val accuracy 0.6516000032424927\n","Epoch 16: SGD lr 0.0206 -> 0.0185\n","Epoch 17, Training loss 0.78057680082748, Val accuracy 0.6499000191688538\n","Epoch 17: SGD lr 0.0185 -> 0.0167\n","Epoch 18, Training loss 0.7724230284123774, Val accuracy 0.6351000070571899\n","Epoch 18: SGD lr 0.0167 -> 0.0150\n","Epoch 19, Training loss 0.7636162604364898, Val accuracy 0.6556000113487244\n","Epoch 19: SGD lr 0.0150 -> 0.0135\n","Epoch 20, Training loss 0.7576990112319322, Val accuracy 0.6578999757766724\n","Epoch 20: SGD lr 0.0135 -> 0.0122\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","epoch = 1\n","path = data_path + \"epoch_\" + str(epoch)\n","obj = pd.read_pickle(path)\n","print(obj.keys())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IkrX1-y8oABE","executionInfo":{"status":"ok","timestamp":1696992505070,"user_tz":420,"elapsed":574,"user":{"displayName":"Phuong Ha Nguyen","userId":"01448229299682072761"}},"outputId":"d1d8326c-52d1-4a2f-d0ab-d31f8204867d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["dict_keys(['epoch', 'model_state', 'optimizer_state'])\n"]}]}]}