{"cells":[{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18932,"status":"ok","timestamp":1697746163600,"user":{"displayName":"Phuong Ha Nguyen","userId":"01448229299682072761"},"user_tz":420},"id":"r7Rl1QFk5icv","outputId":"c7be9855-0b6e-4798-970b-14086c0c00ac"},"outputs":[],"source":["# First we need to mount the Google drive\n","import os\n","# from google.colab import drive\n","# drive.mount('/content/Hadrive')"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":179,"status":"ok","timestamp":1697746165520,"user":{"displayName":"Phuong Ha Nguyen","userId":"01448229299682072761"},"user_tz":420},"id":"9r5K3CJU236g","outputId":"93f34f04-5fbd-4577-e6f1-8f74e732969f"},"outputs":[{"name":"stdout","output_type":"stream","text":["index: 1\n","config: {'lr_initial': 0.1, 'decay': 0.9, 'sigma': 1e-08, 'const_C': 1000}\n","key: lr_initial, value: 0.1\n","key: decay, value: 0.9\n","key: sigma, value: 1e-08\n","key: const_C, value: 1000\n"]}],"source":["\n","configs = dict({\n","\"1\": { \"lr_initial\": 0.1, \"decay\": 0.9, \"sigma\": 0.00000001, \"const_C\": 1000}\n","})\n","\n","\n","for index, config in configs.items():\n","  print(f\"index: {index}\")\n","  print(f\"config: {config}\")\n","  for key,value in config.items():\n","    print(f\"key: {key}, value: {value}\")\n","\n"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"q8wxEMaM3C59"},"outputs":[],"source":["#!mkdir /content/Hadrive/MyDrive/Test1\n","#!mkdir /content/Hadrive/MyDrive/Test1/Tutorial1/"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":5571,"status":"ok","timestamp":1697746175344,"user":{"displayName":"Phuong Ha Nguyen","userId":"01448229299682072761"},"user_tz":420},"id":"vkpkT6pfieuj"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torch.optim as optim\n","import torch.optim.lr_scheduler as lr_scheduler\n","import torchvision\n","import torchvision.transforms as transforms\n","from torchvision import datasets\n","from torchvision import transforms\n","from torch.func import functional_call, vmap, grad\n","\n","\n","import pickle\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from collections import OrderedDict\n","from collections import defaultdict\n"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6465,"status":"ok","timestamp":1697746184281,"user":{"displayName":"Phuong Ha Nguyen","userId":"01448229299682072761"},"user_tz":420},"id":"j0_-UJk0izg6","outputId":"27255996-a01c-4df9-fd6d-081c0b3730fc"},"outputs":[{"name":"stdout","output_type":"stream","text":["lencifar10: 50000\n","lencifar10_val: 10000\n"]}],"source":["data_path = '../../../data'\n","\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.4915, 0.4823, 0.4468), (0.2470, 0.2435, 0.2616))\n","])\n","\n","# cifar10 = datasets.CIFAR10(data_path, train=True, download=True, transform=transform)\n","# cifar10_val = datasets.CIFAR10(data_path, train=False, download=True, transform=transform)\n","\n","nbsamples = 100\n","\n","#https://stackoverflow.com/questions/47432168/taking-subsets-of-a-pytorch-dataset\n","cifar10_org = datasets.CIFAR10(data_path, train=True, download=False, transform=transform)\n","#cifar10 = torch.utils.data.Subset(cifar10_org, list(range(0, nbsamples)))\n","cifar10 = cifar10_org\n","\n","cifar10_val_org = datasets.CIFAR10(data_path, train=False, download=False, transform=transform)\n","#cifar10_val = torch.utils.data.Subset(cifar10_val_org, list(range(0, nbsamples)))\n","cifar10_val = cifar10_val_org\n","\n","print(f\"lencifar10: {len(cifar10)}\")\n","print(f\"lencifar10_val: {len(cifar10_val)}\")"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":112,"status":"ok","timestamp":1697746187650,"user":{"displayName":"Phuong Ha Nguyen","userId":"01448229299682072761"},"user_tz":420},"id":"i4W2GOsdjK72"},"outputs":[{"name":"stdout","output_type":"stream","text":["10000\n"]}],"source":["train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64,\n","                                           shuffle=True)\n","val_loader = torch.utils.data.DataLoader(cifar10_val, batch_size=64,\n","                                           shuffle=True)\n","print(len(val_loader.dataset))"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":127,"status":"ok","timestamp":1697746190519,"user":{"displayName":"Phuong Ha Nguyen","userId":"01448229299682072761"},"user_tz":420},"id":"iy4nacdKjQGu"},"outputs":[],"source":["# model\n","class Net(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n","        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n","        self.fc1 = nn.Linear(8 * 8 * 8, 32)\n","        # bài toán phân loại 10 lớp nên output ra 10 nodes\n","        self.fc2 = nn.Linear(32, 10)\n","\n","    def forward(self, x):\n","        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n","        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n","        # flatten về dạng vector để cho vào neural network\n","        out = out.view(-1, 8 * 8 * 8)\n","        out = torch.tanh(self.fc1(out))\n","        out = self.fc2(out)\n","        return out\n"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":163,"status":"ok","timestamp":1697747766409,"user":{"displayName":"Phuong Ha Nguyen","userId":"01448229299682072761"},"user_tz":420},"id":"0Pn9A9tujUnW"},"outputs":[],"source":["import pickle\n","\n","import torch\n","import torchvision\n","import torch.nn as nn\n","import numpy as np\n","import torchvision.transforms as transforms\n","from collections import OrderedDict\n","from collections import defaultdict\n","from torch.func import functional_call, vmap, grad\n","\n","def generate_private_grad(model,loss_fn,samples,targets,sigma,const_C):\n","    '''\n","        We generate private grad given a batch of samples (samples,targets) in batchclipping mode\n","        The implementation flow is as follows:\n","            1. samples x0, x1, ..., x(L-1)\n","            2. compute avg of gradient g = sum(g0, ..., g(L-1))/L\n","            3. clipped gradient gc\n","            4. clipped noisy gradient (gc*L + noise)/L\n","\n","        Finally, we normalize the private gradient and update the model.grad. This step allows optimizer update the model\n","    '''\n","\n","    #copute the gradient of the whole batch\n","    outputs = model(samples)\n","    loss = loss_fn(outputs, targets)\n","    model.zero_grad()\n","    loss.backward()\n","\n","    #generate private grad per layer\n","    mean = 0\n","    std = sigma*const_C\n","    batch_size = len(samples)\n","    norm_type = 2.0\n","    #clipping the gradient\n","    #https://discuss.pytorch.org/t/how-to-clip-grad-norm-grads-from-torch-autograd-grad/137816/2\n","    for param in model.parameters():\n","        #clip the gradients\n","        max_norm = const_C #clipping constant C\n","        grad = param.grad\n","        total_norm = torch.norm(grad.detach(), norm_type)\n","        clip_coef = max_norm / (total_norm + 1e-6)\n","        clip_coef_clamped = torch.clamp(clip_coef, max=1.0)\n","        param.grad.detach().mul_(clip_coef_clamped)\n","        #generate the noise and add it to the clipped grads\n","        grad = param.grad\n","        #generate the noise ~ N(0,(C\\sigma)^2I)\n","        #std -- is C\\sigma as explain this in wikipage https://en.wikipedia.org/wiki/Normal_distribution N(mu,\\sigma^2) and sigma is std\n","        noise = torch.normal(mean=mean, std=std, size=grad.shape)\n","        #generate private gradient per layer\n","        param.grad = (grad*batch_size + noise)/batch_size\n","\n","    return 0\n","\n","def training_loop(n_epochs, optimizer, model, loss_fn, sigma, const_C, train_loader, val_loader, data_path):\n","    for epoch in range(1, n_epochs + 1):\n","        loss_train = 0.0\n","\n","        for imgs, labels in train_loader:\n","\n","          outputs = model(imgs)\n","          loss = loss_fn(outputs, labels)\n","          loss_train += loss.item()\n","\n","          optimizer.zero_grad()\n","          '''\n","            generate_private_grad(model,loss_fn,imgs,labels,sigma,const_C)\n","              1. Compute the grad for whole batch of samples\n","              2. Clip the gradient of the batch of samples\n","              3. Add noise to the clipped grad of the whole batch of samples\n","              4. Update the model.grad. This helps optimizer.step works as normal.\n","          '''\n","          #loss.backward()\n","          generate_private_grad(model,loss_fn,imgs,labels,sigma,const_C)\n","          optimizer.step()\n","\n","        correct = 0\n","        with torch.no_grad():\n","            for data in val_loader:\n","                images, labels = data\n","                outputs = model(images)\n","                _, predicted = torch.max(outputs, 1)\n","                c = (predicted == labels).squeeze()\n","                correct += c.sum()\n","        if epoch == 1 or epoch % 1 == 0:\n","            print('Epoch {}, Training loss {}, Val accuracy {}'.format(\n","                epoch,\n","                loss_train / len(train_loader),\n","                correct / len(cifar10_val)))\n","\n","        before_lr = optimizer.param_groups[0][\"lr\"]\n","        scheduler.step()\n","        after_lr = optimizer.param_groups[0][\"lr\"]\n","        print(\"Epoch %d: SGD lr %.4f -> %.4f\" % (epoch, before_lr, after_lr))\n","\n","\n","        #save the model config\n","        model_state = model.state_dict()\n","        optimizer_state = optimizer.state_dict()\n","        scheduler_state = scheduler.state_dict()\n","        dict_state = dict()\n","        dict_state[\"epoch\"] = epoch\n","        dict_state[\"sigma\"] = sigma\n","        dict_state[\"const_C\"] = const_C\n","        dict_state[\"model_state\"] = model_state\n","        dict_state[\"optimizer_state\"] = optimizer_state\n","        dict_state[\"scheduler_state\"] = scheduler_state\n","        dict_state[\"train_loss\"] = loss_train / len(train_loader)\n","        dict_state[\"val_acc\"] = correct / len(cifar10_val)\n","\n","        try:\n","            geeky_file = open(data_path + \"epoch_\" + str(epoch), 'wb')\n","            pickle.dump(dict_state, geeky_file)\n","            geeky_file.close()\n","\n","        except:\n","            print(\"Something went wrong\")\n","\n","        #print(f\"scheduler state: {scheduler_state}\")"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1183546,"status":"ok","timestamp":1697748955432,"user":{"displayName":"Phuong Ha Nguyen","userId":"01448229299682072761"},"user_tz":420},"id":"auhHQgx8jcbV","outputId":"6dedcc44-c8c3-4ade-9504-b5923446ae58"},"outputs":[{"name":"stdout","output_type":"stream","text":["index: 1\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[19], line 26\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;124;03m  LambdaLR: new LR = initialLR * f(epoch)\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;124;03m  For example: f(epoch) = 1/t\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# lambda1 = lambda epoch: 1/(epoch+1)\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=[lambda1])\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[43mtraining_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43msigma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msigma\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconst_C\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconst_C\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata_path_index\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[1;32mIn[18], line 59\u001b[0m, in \u001b[0;36mtraining_loop\u001b[1;34m(n_epochs, optimizer, model, loss_fn, sigma, const_C, train_loader, val_loader, data_path)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, n_epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     57\u001b[0m     loss_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m---> 59\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[43m      \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\nntoa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[1;32mc:\\Users\\nntoa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[1;32mc:\\Users\\nntoa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[1;32mc:\\Users\\nntoa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\datasets\\cifar.py:119\u001b[0m, in \u001b[0;36mCIFAR10.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    116\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 119\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    122\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n","File \u001b[1;32mc:\\Users\\nntoa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n","File \u001b[1;32mc:\\Users\\nntoa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\nntoa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\nntoa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:277\u001b[0m, in \u001b[0;36mNormalize.forward\u001b[1;34m(self, tensor)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m    270\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;124;03m        tensor (Tensor): Tensor image to be normalized.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;124;03m        Tensor: Normalized Tensor image.\u001b[39;00m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\nntoa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\transforms\\functional.py:350\u001b[0m, in \u001b[0;36mnormalize\u001b[1;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensor, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg should be Tensor Image. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensor)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 350\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_t\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\nntoa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\transforms\\_functional_tensor.py:920\u001b[0m, in \u001b[0;36mnormalize\u001b[1;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[0;32m    918\u001b[0m mean \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor(mean, dtype\u001b[38;5;241m=\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mtensor\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    919\u001b[0m std \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor(std, dtype\u001b[38;5;241m=\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mtensor\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m--> 920\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43m(\u001b[49m\u001b[43mstd\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43many\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    921\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstd evaluated to zero after conversion to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, leading to division by zero.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    922\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mean\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["for index, config in configs.items():\n","  print(f\"index: {index}\")\n","  data_path_index = data_path + \"config_\" + str(index) + \"_\"\n","  model = Net()\n","  optimizer = optim.SGD(model.parameters(), lr=config[\"lr_initial\"])\n","  loss_fn = nn.CrossEntropyLoss()\n","  '''\n","    LinearLR =>> new LR = initial LR - nb_epochs*(start_factor-end_factor)/total_iters\n","    example, initialLR = 0.1, start = 1.0, end_factor = 0.5, total_iters = 20\n","    (start_factor-end_factor)/total_iters = 0.025.\n","    ===> epoch 1: 0.1 - 1*0.025 = 0.0975\n","    ===> epoch 2: 0.1 - 2*0.025 = 0.0950....\n","  '''\n","  #scheduler = lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.5, total_iters=20)\n","  '''\n","   StepLR =>>> new LR = old LR * gamma\n","  '''\n","  scheduler = lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n","  '''\n","    LambdaLR: new LR = initialLR * f(epoch)\n","    For example: f(epoch) = 1/t\n","  '''\n","  # lambda1 = lambda epoch: 1/(epoch+1)\n","  # scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=[lambda1])\n","\n","  training_loop(\n","      n_epochs = 20,\n","      optimizer = optimizer,\n","      model = model,\n","      sigma = config[\"sigma\"],\n","      const_C = config[\"const_C\"],\n","      loss_fn = loss_fn,\n","      train_loader = train_loader,\n","      val_loader = val_loader,\n","      data_path = data_path_index\n","  )\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":574,"status":"ok","timestamp":1696992505070,"user":{"displayName":"Phuong Ha Nguyen","userId":"01448229299682072761"},"user_tz":420},"id":"IkrX1-y8oABE","outputId":"d1d8326c-52d1-4a2f-d0ab-d31f8204867d"},"outputs":[{"name":"stdout","output_type":"stream","text":["dict_keys(['epoch', 'model_state', 'optimizer_state'])\n"]}],"source":["import pandas as pd\n","epoch = 1\n","path = data_path + \"epoch_\" + str(epoch)\n","obj = pd.read_pickle(path)\n","print(obj.keys())"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyN6qUN4POQRiD2RAhN3IaeW","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"}},"nbformat":4,"nbformat_minor":0}
