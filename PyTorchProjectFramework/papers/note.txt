= Individual clipping still implies that the average of the aggregate update U is at most C.
Hence, the actual update U can have norm sC where s is the sample size.
Hence, there is must more freedom and clean accuracy may improve a lot.

= Also, my argument indeed implies the factor 2 penalty of we clip the final U.

The above suggests that individual clipping is way better and if Toan can rerun the experiments,
then this would shed a lot of light on this. And we can compare accuracy with clipping U as Nhuong did
(but now correcting the factor 2 in the DP argument) and find out how they compare.

=> Nhuong did clip the whole batch gradient norm with C
and added the noise N(0,C^2 * sigma^2) into the sum of gradient => I dont think it is correct to use Nhuong result


For the clipping paper we have the experiments:

= Keep de DP accountant with the hyper parameter sigma

=> Done

= Do the DP accountant with the actual sigma_i calculated as sigma/min{norm,C}

=> See below

= Do the above experiment and have a decreasing sequence of clipping constants C_i: Can we do this?
The above experiment may suggest such a sequence and if this also has good DP,
then this leads to future work where we want to incorporate this into our ICML theory
(where we a-priori optimize hyperparameters).

=> I can adjust both sigma and C in each iteration so it is do-able.

= All the above can be done with individual clipping as well as sample clipping.

I like to add another thought:

= The client already know wglobal_{i-1} and wglobal_i and can compute the difference,
this can be used to give an estimate of the to be computed U. Let this estimate be E.

= When we clip v representing an individual gradient or the aggregated gradient U,
we simple clip the vector v-E. This allows us to gain better clean accuracy, but how much?

*** Urgent

PS We will soon enter the ICML rebuttal phase -- what are the current results for the DP accountant --
for the actual sigma_i=sigma/min{C,norm_i} ?

=> Norm_i  = max_gradient_norm between each individual gradients?
=> Should I change C_i = min{C_{i-1},norm_i}?

For T meeting our lower bound, and T being a factor 4 (or whatever this was) smaller?

T meeting lower bound (main body) => eps does not change ?

TODO: Fix error: loss = NaN after some iterations
- Find a setting which both benefit SGD and DPSGD in terms of learning rate
    => Trying out other deep learning models
    => Adjust current model structure
    => Change data preprocessing (** the way we normalize data affects how we choose the noise (C,sigma))

TODO: Implement/re-contruct Federated Learning Framework
    => Read Nhuong's code for reference (remember to check flaws)
    => Look for available open-source FL frameworks
    =>
