= Individual clipping still implies that the average of the aggregate update U is at most C.
Hence, the actual update U can have norm sC where s is the sample size.
Hence, there is must more freedom and clean accuracy may improve a lot.

= Also, my argument indeed implies the factor 2 penalty of we clip the final U.

The above suggests that individual clipping is way better and if Toan can rerun the experiments,
then this would shed a lot of light on this. And we can compare accuracy with clipping U as Nhuong did
(but now correcting the factor 2 in the DP argument) and find out how they compare.

For the clipping paper we have the experiments:

= Keep de DP accountant with the hyper parameter sigma

= Do the DP accountant with the actual sigma_i calculated as sigma/min{norm,C}

= Do the above experiment and have a decreasing sequence of clipping constants C_i: Can we do this?
The above experiment may suggest such a sequence and if this also has good DP,
then this leads to future work where we want to incorporate this into our ICML theory
(where we a-priori optimize hyperparameters).

= All the above can be done with individual clipping as well as sample clipping.

I like to add another thought:

= The client already know wglobal_{i-1} and wglobal_i and can compute the difference,
this can be used to give an estimate of the to be computed U. Let this estimate be E.

= When we clip v representing an individual gradient or the aggregated gradient U,
we simple clip the vector v-E. This allows us to gain better clean accuracy, but how much?

*** Urgent

PS We will soon enter the ICML rebuttal phase -- what are the current results for the DP accountant --
for the actual sigma_i=sigma/min{C,norm_i} ?
For T meeting our lower bound, and T being a factor 4 (or whatever this was) smaller?

